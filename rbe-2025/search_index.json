[["index.html", "R: Learning by Example Preliminaries License Getting R and Rstudio Keeping Track of Your Work Overview of Rstudio Using R Function Arguments Importing Data Examining Data Missing Values Filtering with Logical Expressions and Sorting Workhorse Functions Exercises", " R: Learning by Example David A. Armstrong II and William Poirier 2025-05-29 Preliminaries R By Example is intended to walk users through transitioning to R from other software. In particular, it supports the course of the same name that I have taught through the ICPSR Summer Program since 2015. License This book, in its entirety, is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. In the ensuing pages, we will walk through lots of statistical models with only the the slightest attention paid to the underlying statistical theory. As such, this is not so much a book (or a course) about statistics as it is a book about learning how to run and evaluate models you already know in R. To following along in the book, you should have the most recent versions of R and RStudio. Rather than slides, I have decided to distribute this book that has more prose in than slides would permit. The idea is to provide something that will serve as a slightly more comprehensive reference as you start to employ R in your own analyses. There is an ever increasing number of R books out there. The ones I particularly like are: John Fox and Sanford Weisberg’s An R Companion to Applied Regression - the first edition of this book is how I learned R in 2002. Robert Kabacoff’s R in Action - this is a good “from first principles” book about R that I routinely recommend to people. Chester Ismay and Albert Kim’s Statistical Inference via Data Science - has some good introductory chapters that would be particularly useful for this audience. Getting R and Rstudio R is an object-oriented statistical programming environment. It remains largely command-line driven. There are a couple of attempts at generating point-and-click GUIs for R, but these are almost necessarily limited in scope and tend to be geared toward undergraduate research methods students - both RCommander and Jamovi are good examples. R is open-source (i.e., free) and downloadable from CRAN. Click the link for your operating system. In Windows, click on the link for base and then the link for “Download R for Windows”. Once it is done, double-click on the resulting file and that will guide you through the installation process. There are some decisions to be made, but if you’re unsure, following the defaults is generally not a bad idea. We will not interact with R through its own GUI, but through RStudio, so the particularities of the setup will be unimportant in all but the most unusual cases. For Mac users, click on the link for “Download R for Mac” on the CRAN home page and then click the “R-.pkg” link. This book is compiled with R version 4.4.2. You should also download Rstudio, an Integrated Development Environment (IDE) for R. This application sits on top of your existing R installation (i.e., it also requires you to install R separately) to provide some nice text editing functions along with some other usefull features. I’ve spent a considerable amount of time using other competing IDEs - WinEDT (a long time ago), TextMate, Atom, Sublime and Microsoft’s VS Code. There were things about all of them that I really liked, but ultimately for someone whose workflow is almost entirely in R, Markdown and LaTeX, there is little reason to move to something else. RStudio is also increasingly becoming a suitable IDE for other languages,too, e.g., Python. In RStudio, you can change themes (color schemes), fonts and other aspects of the appearance. You can also use (and optionally set) shortcut keys for your own favorite operations, too. Some that I use regularly are: ctrl +p finds the matching bracket ctrl + shift + e expands the selection to the matching bracket I also mapped the “quick add next” operation, which allows you to highlight a single instance of a string and then highlight the next one with the click of a keystroke. Then, you have multiple cursors at each instance that you can use to make multiple changes at once. Keeping Track of Your Work In general, I’m a big fan of using RMarkdown documents to keep track of your work. They provide a great format for including both prose (that can explain to your colleagues, readers and even your future self what you did) and R code. It also could then form the basis for a paper or book you could write using RStudio. I would encourage you to write in RMarkdown files that parallel the chapters of the book, so you can keep track of what you are doing to complete the exercises. Overview of Rstudio When first opening Rstudio, you are met with a somewhat complicated array of windows and buttons. Here are the four main area of interest. First, on the upper left corner, we can find the main area of work. This is where you can create, open and write into files. To see it, you first need to open a file. In this book, we are mostly interested in .R files, but note that Rstudio supports multiple languages like, Rmd, LaTeX and even Python. Think of this window as your Word editor, it is where you write your code so that it can be saved for later. Right bellow the file window is the console. Each time you execute a command from your main code in the second quadrant, it is printed in the console. While you can write commands directly into the console, there is no easy way to save the order of operation you executed from it. It is more useful to think of the console as a place where you can test commands and check on the result of your analysis since it will not be saved into your .R file. Moving on to the right side of the screen, we can view two sets of panels that give information about your R session. On top, we have three window toggles. The environment is where all the objects that you create are stored might that be data frames, functions, vectors, matrices or lists. It helps you keep track of the objects in your environment such that you don’t overwrite any by mistake. It also gives dimentional information on said objects, like the number of rows of a data frame for example. Next, is the history window in which you can view the whole order of operations executed since Rstudio was opened. Finally the connection window is where you can connect your environment to an external database like SQL. On the bottom right corner we have another set of windows. The files window allows you to navigate your computer’s directories an open files, althought we recommend doing so from the .R file. The plots window will show a preview of the graphs you produce and will allow you to navigate the history of their production. The packages window provides information about the available packages that you have installed via the function install.packages() and the ones that are loaded via the function library(). The help window will open when you execute the ?function() command and provide information about how to use the function. Finally, the viewer window allows you to view local web content from a static HTML file or a locally ran web application. knitr::include_graphics(&quot;images/rstudio1.png&quot;) Using R Like Stata and SAS, R has an active user-developer community. This is attractive as the types of models and situations R can deal with is always expanding. Unlike Stata, in R, you have to load the packages you need before you’re able to access the commands within those packages. All openly distributed packages are available from the Comprehensive R Archive Network, though some of them come with the Base version of R. To see what packages you have available, type library() or click on the “Packages” tab in the files panel. There are two related functions that you will need to obtain new packages for R. install.packages() will download the relevant source code from R and install it on your machine. This step only has to be done once until you upgrade to a new minor (or major) version of R. For example, if you upgrade from 3.5.0 to 3.5.1, all of the packages you downloaded will still be available. In this step, a dialog box will ask you to choose a CRAN mirror - this is one of many sites that maintain complete archives of all of R’s user-developed packages. Usually, the advice is to pick one close to you (or the cloud option). library() will make the commands in the packages you downloaded available to you in the current R session (a new session starts each time R is started and continues until that instance of R is terminated). As suggested this has to be done (when you want to use functions other than those loaded automatically) each time you start R. There is an option to have R load packages automatically on startup by modifying the .RProfile file (more on that later). You can accomplish the same thing through the “packages” tab in the files panel. Each package that has been installed has a checkbox next to it. You can load the package by checking the checkbox. There is a search bar to help you locate your package without endless scrolling. There is also an “install” button in the upper right-hand corner of the packages tab. Clicking that will open an install packages dialog where you can type in the name of the package you want to install. The “object-oriented” nature of R means that you’re generally saving the results of commands into objects that you can access whenever you want and manipulate with other commands. R is a case-sensitive environment, so be careful how you name and access objects in the space and be careful how you call functions lm() \\(\\neq\\) LM(). There are a few tips that don’t really belong anywhere, but are nonetheless important, so I’ll just mention them here and you can refer back when they become relevant. In RStudio, if you position your cursor in a line you want to execute (or block text you want to execute), then hit ctrl+enter on a PC or command+enter on the mac, the functions will be automatically executed. You can return to the command you previously entered in the R console by hitting the “up arrow” (similar to “Page Up” in Stata). You can find out what directory R is in by typing getwd(). You can set the working directory of R by typing setwd(path) where path is the full path to the directory you want to use. The directories must be separated by forward slashes / and the entire string must be in quotes (either double or single). For example: setwd(\"C:/users/david/desktop\"). You can also do this through the “Session” dropdown menu where you can select “Set Working Directory” as an option. To see the values in any object, just type that object’s name into the command window and hit enter (or look in the object browser in RStudio). Assigning Output to Objects In this section, we will spend a bit of time on the very basics of coding in R. Some of this may seem tedious, but this is a good way of getting to understand how R works and we won’t spend too long here. R can be used as a big calculator. By typing 2+2 into R, you will get the following output: 2 + 2 ## [1] 4 After my input of 2+2, R has provided the output of 4, the evaluation of that mathematical expression. R just prints this output to the console. Doing it this way, the output is not saved per se. Notice that you do not have to ask R to “display” the results of operations where in Stata you would have to type display 2+2 to get the same result. Often times, we want to save the output so we can look at it later. The assignment character in R is &lt;- (the less-than sign directly followed by the minus sign). You may hear me say “X gets 10,” in R, this would translate to X &lt;- 10 X ## [1] 10 You can also use the = as the assignment character. When I started using R, people wren’t doing this, so I haven’t changed over yet, but the following is an equivalent way of specifying the above statement: X &lt;- 10 X ## [1] 10 As with any convention that doesn’t matter much, there are dogmatic adherents on either side of the debate. Some argue that the code is easier to read using the arrow. Others argue that using a single keystroke to produce the assignment character is more efficient. In truth, both are probably right. Your choice is really a matter of taste. For this book, we’ve tried to keep to the tidyverse style guide (here) in which they recommend the use of the &lt;- operator. We strongly suggest that once you choose to do something in a certain way, you keep to it. This goes with naming objects and commenting your code as well, the key is consistency. The goal is, after all, for future you or others to look at your code and understand what is going on. To assign the output of an evaluated function to an object, just put the object on the left-hand side of the arrow and the function on the right-hand side. X &lt;- 4 + 4 Now the object X contains the evaluation of the expression 4+4 or 8. We see the contents of X simply by typing its name at the command prompt and hitting enter. In the above command, we’re assigning the output (or result) of the command 4+4 to X. X ## [1] 8 There are a few things worth noting here: The object does not always save the call - the code that produced the output. Models tend to save this kind of information, but simpler mathematical calculations do not. There is no “undo” button and no warning that something is about to be overwritten. The onus is on you to make sure that you keep track of the names of objects you want to keep. Usually, this is not problematic as if you are keeping good track of what you’re doing, you’ll be able to re-generate any previous result easily. Vectors and Matrices We can make a vector of numbers by combining a bunch of numbers together using the c() function. This collects the numbers together in a single object. Y &lt;- c(1, 2, 3, 4) Y ## [1] 1 2 3 4 We can also do math on vectors. For example, we can add a scalar and it will add the scalar to each element of the vector: Y + 3 ## [1] 4 5 6 7 The matrix() function allows us to make matrices. mat1 &lt;- matrix(c(1, 2, 3, 4), nrow = 2, ncol = 2) mat1 ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 The default orientation of matrices is in column-major format, meaning that each column of the matrix is filled in until all of the numbers in the vector have been used. You could also fill in by rows by using the byrow=TRUE argument to the matrix() function. mat2 &lt;- matrix(c(1, 2, 3, 4), nrow = 2, ncol = 2, byrow = TRUE) mat2 ## [,1] [,2] ## [1,] 1 2 ## [2,] 3 4 Function Arguments As we will see throughout this book, each R function can take a number of arguments. These arguments and what they do are detailed in the function’s help file. You can access a function’s help file by typing help matrix or ?matrix; or, you can click on the “Help” tab in the files pane on the left. You can then use the search bar in the upper-right hand corner of the panel to search for a function’s help file. Arguments can take lots of forms, but the most common are: formula - a model specification that takes the form: outcome ~ covariate1 + covariate2 for additive functions and outcome ~ covariate1*covariate2 for conditional or multiplicative functions. logical - either TRUE or FALSE (must be in all caps), BTW - TRUE has a numerical value of 1 and FALSE has a numerical value of 0. string - A character string, must be in quotes (single or double quotes are fine, so long as they match). data - Generally a data frame or something that can be coerced to a data frame. vector or list - allow multiple values to be passed to a single argument. For example, looking at the help file for the matrix() function, we see that the first argument data is a vector, nrow and ncol are scalars (single numbers) and byrow is a logical value. Importing Data Before we move on to more complicated operations and more intricacies of dealing with data, the one thing everyone wants to know is - “How do I get my data into R?” As it turns out, the answer is - “quite easily.” There are a number of packages in R that can help read in from and write out to other statistical environments. I personally like the rio package. This package is actually a wrapper to lots of different packages for importing and exporting data in other formats. It automatically identifies the data type from its extension and uses the correct importer for the data. Let’s look at a couple of examples. The dataset we’ll be using here has three variables - x1, (a numeric variable), x2 (a labeled numeric variable [0=none, 1=some]) and x3 a string variable (“no” and “yes”). I’ve called this dataset r_example.sav (SPSS) and r_example.dta (Stata). R has lots of different data structures available (e.g., arrays, lists, ect…). The one that we are going to be concerned with right now is the data frame; the R terminology for a dataset. A data frame can have different types of variables in it (i.e., character and numeric). It is rectangular (i.e., all rows have the same number of columns and all columns have the same number of rows). There are some more distinctions that make the data frame special, but we’ll talk about those later. ## load rio package - only need to do this once per R session library(rio) ## load data, note the relative path to the dataset from the current directory spss.dat &lt;- import(&quot;data/r_example.sav&quot;) ## print the contents spss.dat ## x1 x2 x3 ## 1 1 0 yes ## 2 2 0 no ## 3 3 1 no ## 4 4 0 yes ## 5 3 0 no ## 6 4 0 yes ## 7 1 1 yes ## 8 2 1 yes ## 9 5 1 no ## 10 6 0 no There is also an example Stata dataset, which you could read in as follows: stata.dat &lt;- import(&quot;data/r_example.dta&quot;) Data Types in R This is a convenient time to talk about different types of data in R. There are basically three different types of variables - numeric variables, factors and character strings. Numeric variables would be something like GDP/capita, age or income (in $). Generally, these variables do not contain labels because they have many unique values. Dummy variables are also numeric with values 0 and 1. R will only do mathematical operations on numeric variables (e.g., mean, variance, etc…). Factors are variables like social class or party for which you voted. When you think about how to include variables in a model, factors are variables that you would include by making a set of category dummy variables. Factors in R look like numeric variables with value labels in either Stata or SPSS. That is to say that there is a numbering scheme where each unique label value gets a unique number (all non-labeled values are coded as missing). Unlike in those other programs, R will not let you perform mathematical operations on factors. Character strings are simply text. There is no numbering scheme with corresponding labels, the value in each cell is simply that cell’s text, not a number with a corresponding label like in a factor. Using the rio package, it reads numeric variables with labels as numbers, but it attaches an attribute to the variable called labels which can be used to turn the variable into a factor. Note the difference in the output below between x2 and x3 - x2 is numeric with a labels attribute and x3 is a character string (denoted with chr). str(spss.dat) ## &#39;data.frame&#39;: 10 obs. of 3 variables: ## $ x1: num 1 2 3 4 3 4 1 2 5 6 ## ..- attr(*, &quot;label&quot;)= chr &quot;x1&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F8.2&quot; ## $ x2: num 0 0 1 0 0 0 1 1 1 0 ## ..- attr(*, &quot;label&quot;)= chr &quot;x2&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F8.2&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:2] 0 1 ## .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;none&quot; &quot;some&quot; ## $ x3: chr &quot;yes&quot; &quot;no&quot; &quot;no&quot; &quot;yes&quot; ... ## ..- attr(*, &quot;label&quot;)= chr &quot;x3&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;A3&quot; ## ..- attr(*, &quot;display_width&quot;)= int 11 To turn x2 into a factor, we could use the function factorize() that’s in the rio package. spss.dat$x2_fac &lt;- rio::factorize(spss.dat$x2) str(spss.dat) ## &#39;data.frame&#39;: 10 obs. of 4 variables: ## $ x1 : num 1 2 3 4 3 4 1 2 5 6 ## ..- attr(*, &quot;label&quot;)= chr &quot;x1&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F8.2&quot; ## $ x2 : num 0 0 1 0 0 0 1 1 1 0 ## ..- attr(*, &quot;label&quot;)= chr &quot;x2&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F8.2&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:2] 0 1 ## .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;none&quot; &quot;some&quot; ## $ x3 : chr &quot;yes&quot; &quot;no&quot; &quot;no&quot; &quot;yes&quot; ... ## ..- attr(*, &quot;label&quot;)= chr &quot;x3&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;A3&quot; ## ..- attr(*, &quot;display_width&quot;)= int 11 ## $ x2_fac: Factor w/ 2 levels &quot;none&quot;,&quot;some&quot;: 1 1 2 1 1 1 2 2 2 1 ## ..- attr(*, &quot;label&quot;)= chr &quot;x2&quot; Examining Data There are a few different methods for examining the properties of your data. The first will tell you what type of data are in your data frame and gives a sense of what some representative values are. The str command shows the structure of your dataset along with any attributes of the variables that would be otherwise hidden from view. str(spss.dat) ## &#39;data.frame&#39;: 10 obs. of 4 variables: ## $ x1 : num 1 2 3 4 3 4 1 2 5 6 ## ..- attr(*, &quot;label&quot;)= chr &quot;x1&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F8.2&quot; ## $ x2 : num 0 0 1 0 0 0 1 1 1 0 ## ..- attr(*, &quot;label&quot;)= chr &quot;x2&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F8.2&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:2] 0 1 ## .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;none&quot; &quot;some&quot; ## $ x3 : chr &quot;yes&quot; &quot;no&quot; &quot;no&quot; &quot;yes&quot; ... ## ..- attr(*, &quot;label&quot;)= chr &quot;x3&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;A3&quot; ## ..- attr(*, &quot;display_width&quot;)= int 11 ## $ x2_fac: Factor w/ 2 levels &quot;none&quot;,&quot;some&quot;: 1 1 2 1 1 1 2 2 2 1 ## ..- attr(*, &quot;label&quot;)= chr &quot;x2&quot; The second method is a more substantive summary. The skimr package has a function called skim_without_charts() that provides a nice summary depending on the variable type. library(skimr) skim_without_charts(spss.dat) Table 0.1: Data summary Name spss.dat Number of rows 10 Number of columns 4 _______________________ Column type frequency: character 1 factor 1 numeric 2 ________________________ Group variables None Variable type: character skim_variable n_missing complete_rate min max empty n_unique whitespace x3 0 1 2 3 0 2 0 Variable type: factor skim_variable n_missing complete_rate ordered n_unique top_counts x2_fac 0 1 FALSE 2 non: 6, som: 4 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 x1 0 1 3.1 1.66 1 2 3 4 6 x2 0 1 0.4 0.52 0 0 0 1 1 Missing Values In R, missing data are indicated with NA (similar to the ., or .a, .b, etc…, in Stata). The dataset r_example_miss.dta, looks like this in Stata: . list +-----------------+ | x1 x2 x3 | |-----------------| 1. | 1 none yes | 2. | 2 none no | 3. | . some no | 4. | 4 . yes | 5. | 3 none no | |-----------------| 6. | 4 none yes | 7. | 1 some yes | 8. | 2 some yes | 9. | 5 some no | 10. | 6 none no | +-----------------+ Notice that it looks like values are missing on all three variables. Let’s read the data into R and see what happens. stata2.dat &lt;- import(&quot;data/r_example_miss.dta&quot;) stata2.dat$x2_fac &lt;- rio::factorize(stata2.dat$x2) stata2.dat ## x1 x2 x3 x2_fac ## 1 1 0 yes none ## 2 2 0 no none ## 3 NA 1 no some ## 4 4 NA yes &lt;NA&gt; ## 5 3 0 no none ## 6 4 0 yes none ## 7 1 1 yes some ## 8 2 1 yes some ## 9 5 1 no some ## 10 6 0 no none Notice that the missing elements are NA. There are a few different methods for dealing with missing values, though they produce the same statistical result, they have different post-estimation behavior. These are specified through the na.action argument to modeling commands and you can see how these work by using the help functions: ?na.action. In lots of the things we do, we will have to give the argument na.rm=TRUE to remove the missing data from the calculation (i.e., listwise delete). Filtering with Logical Expressions and Sorting A logical expression is one that evaluates to either TRUE (the condition is met) or FALSE (the condition is not met). There are a few operators you need to know (which are the same as the operators in Stata or SPSS). EQUALITY == (two equal signs) is the symbol for logical equality. A == B evaluates to TRUE if A is equivalent to B and evaluates to FALSE otherwise. INEQUALITY != is the command for inequality. A != B evaluates to TRUE when A is not equivalent to B. AND &amp; is the conjunction operator. A &amp; B would evaluate to TRUE if both A and B were met. It would evaluate to FALSE if either A and/or B were not met. OR | (the pipe character) is the logical or operator. A | B would evaluate to TRUE if either A and/or B is met and would evaluate to FALSE only if neither A nor B were met. NOT ! (the exclamation point) is the character for logical negation. !(A &amp; B) is the mirror image of (A &amp; B) such that the latter evaluates to TRUE when the former evaluates to FALSE. When using these with variables, the conditions for factors and character strings should be specified with characters. With numeric variables, the conditions should be specified using numbers. A few examples will help to illuminate things here. stata.dat$x3 == &quot;yes&quot; ## [1] TRUE FALSE FALSE TRUE FALSE TRUE TRUE TRUE FALSE FALSE stata.dat$x2_fac == &quot;none&quot; ## logical(0) stata.dat$x2 == 1 ## [1] FALSE FALSE TRUE FALSE FALSE FALSE TRUE TRUE TRUE FALSE stata.dat$x1 == 2 ## [1] FALSE TRUE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE Workhorse Functions Here are sets of useful and widely used functions that will get you running quickly. First, let’s look at some basic statistical functions. The min() and max() functions output the lowest and highest value of a vector. min(stata.dat$x1) ## [1] 1 max(stata.dat$x1) ## [1] 6 The which.min() and which.max() functions output the position of the first minimum or maximum value in the vector. which.min(stata.dat$x1) ## [1] 1 which.max(stata.dat$x1) ## [1] 10 The median, mean, standard deviation and quantiles of a vector can as easily be obtained. median(stata.dat$x1) ## [1] 3 mean(stata.dat$x1) ## [1] 3.1 sd(stata.dat$x1) ## [1] 1.66333 quantile(stata.dat$x1) ## 0% 25% 50% 75% 100% ## 1 2 3 4 6 The summary() function outputs a lot of this information in one call (min, max, first and third quantiles, median and mean). summary(stata.dat$x1) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.0 2.0 3.0 3.1 4.0 6.0 Next, we have functions that will allow you to better understand your data. When used on a data.frame, head() and tail() will give an overview of the first and last 6 rows of a dataset respectively. head(stata.dat) ## x1 x2 x3 ## 1 1 0 yes ## 2 2 0 no ## 3 3 1 no ## 4 4 0 yes ## 5 3 0 no ## 6 4 0 yes tail(stata.dat) ## x1 x2 x3 ## 5 3 0 no ## 6 4 0 yes ## 7 1 1 yes ## 8 2 1 yes ## 9 5 1 no ## 10 6 0 no This becomes more useful when the number of rows of the data increases substantially. Speaking of which, the nrow() and ncol() functions outputs the number of rows and columns of a data.frame. nrow(stata.dat) ## [1] 10 ncol(stata.dat) ## [1] 3 The vector counterpart of this is the length() function which outputs the number of elements in a vector. Used on a data.frame, it outputs the number of columns. length(stata.dat$x1) ## [1] 10 length(stata.dat) ## [1] 3 When it comes to cleaning data and manipulating it, it’s often times useful to quickly be able to know the names of the columns of a data.frame or the categories included in one of these columns. The function names() outputs, get ready, the column names of a data.frame or any named object. names(stata.dat) ## [1] &quot;x1&quot; &quot;x2&quot; &quot;x3&quot; The table() function outputs both the unique categories in a vector and their frequency in a single call. This is paramount in checking your work, making sure that your new dummy for education is well coded, for example. table(stata.dat$x3) ## ## no yes ## 5 5 stata.dat$x3_dummy &lt;- ifelse(stata.dat$x3 == &quot;yes&quot;, 1, 0) table(stata.dat$x3_dummy) ## ## 0 1 ## 5 5 I just introduced above a new set of functions which deal with logical statements. Indeed, the ifelse() function takes as input a condition, the desired output when true, and the desired output when false. Hence, the above statement reads: when x3 equals ‘’yes’’, output a 1, otherwise output a 0. Note that this function can be nested into itself such that multiple conditions can be added. For example: table(stata.dat$x1) ## ## 1 2 3 4 5 6 ## 2 2 2 2 1 1 stata.dat$x1_fruits &lt;- ifelse(stata.dat$x1 &lt; 3, &quot;Banana&quot;, ifelse(stata.dat$x1 %in% c(3, 4), &quot;Melon&quot;, ifelse(stata.dat$x1 &gt;= 5, &quot;Rambutan&quot;, NA))) table(stata.dat$x1_fruits) ## ## Banana Melon Rambutan ## 4 4 2 The which() function also takes as input a logical statement, but outputs the position of the set of elements for which the condition is true. which(stata.dat$x1==2) ## [1] 2 8 The next set of functions deal with object classes. We’ve seen that functions can take different types of objects (formulas, strings, logical expressions, etc.), but we might also want to deal with object classes. In its simplest form, a class can be understood as a way of characterizing the information included in an object. Let’s say we look at the x1_fruits variable created earlier. The class() function will output the class of the input vector. class(stata.dat$x1_fruits) ## [1] &quot;character&quot; So, x1_fruits is a character vector, meaning that it contains a letter like information. The four types of classes you are bound to face while working in R are: 1) character, 2) numeric, 3) logical, and 4) factor. Note that, while looking at a variable, the presence of numbers isn’t sufficient to conclude with certainty that R will treat the variable as containing numbers. Indeed, sometimes variables are treated as characters because of the way they were input in the data. Hence, one needs to know how to make R understand the variable correctly. The functions as.character(), as.numeric(), and as.logical() serve this purpose. class(stata.dat$x2) ## [1] &quot;numeric&quot; stata.dat$x2_logical &lt;- as.logical(stata.dat$x2) table(stata.dat$x2) ## ## 0 1 ## 6 4 table(stata.dat$x2_logical) ## ## FALSE TRUE ## 6 4 Notice how a variable that was a bunch of zeros and ones got transformed into FALSE and TRUE values. This worked because R knows how to understand zeros and ones in terms of logical statements. The same is not true if we wanted to transform fruits into numbers. class(stata.dat$x1_fruits) ## [1] &quot;character&quot; as.numeric(stata.dat$x1_fruits) ## [1] NA NA NA NA NA NA NA NA NA NA Hence, transforming number characters into numbers make sense, transforming logical values into numbers does too, and transforming numbers into characters is no problem. Don’t try to memorize these, after some trial and error you’ll get a feel for it. Lastly, factors allow us to apply an ordering to classes that don’t inherently have one, like characters. For example, let’s say we wanted to induce a preference ordering on the fruit variable. The factor() function takes the root variable and a levels argument that expects a vector consisted of every category included in the root variable in the required order. So, if we prefer bananas over rambutans and rambutans over melons: table(stata.dat$x1_fruits) ## ## Banana Melon Rambutan ## 4 4 2 stata.dat$x1_fruits_factor &lt;- factor(stata.dat$x1_fruits,levels=c(&quot;Banana&quot;,&quot;Rambutan&quot;,&quot;Melon&quot;)) table(stata.dat$x1_fruits_factor) ## ## Banana Rambutan Melon ## 4 2 4 Notice how, by default, the table() function outputted the categories in alphabetical order. Once we transformed it into a factor with different levels, though, the output of table() reflects the new ordering. Checking the class of an object can also be formulated in a logical statement using is.character(), is.numeric(), is.logical(), is.factor(), is.na(), etc. is.numeric(stata.dat$x1_fruits) ## [1] FALSE Finally, some useful functions to generate data. Let’s say you want to create a vector with 25 ones. You can either use the c() function and write 25 ones yourself, or use rep(). What is neat with this function is that you can either repeat an object \\(n\\) times or each element of an object \\(n\\) times. vect25 &lt;- rep(1,25) vect25 ## [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 vectEach4 &lt;- rep(1:4,each=4) vectEach4 ## [1] 1 1 1 1 2 2 2 2 3 3 3 3 4 4 4 4 Similarly, seq() allows the creation of sequences, like a vector ranging from 0 to 99 by intervals of 3. vect99 &lt;- seq(0,99,3) If you want to simulate data coming from a known distribution, you can usually find a function for it within base R. The rnorm() for normal distributions, rbinom() for binomial distributions, rpois() for Poisson distributions, etc. vectNormal &lt;- rnorm(1000,mean=3,sd=2.7) mean(vectNormal) ## [1] 3.010215 sd(vectNormal) ## [1] 2.716447 Exercises Each chapter of this book will be accompanied by a set of exercises. Some will cover exact application of what is covered in each chapters, and some will go beyond what is explicitly stated in order to both challenge and teach the reader how to navigate R. We strongly advise opening a new exerciseFile.R for each chapter as to both better return your solutions and avoid conflicts between objects and packages. The Tidyverse is package of packages that contains multiple functions that ease data manipulation and visualization. Install and load the Tidyverse. Creating objects is an important part of the R workflow. Some of the packages from the Tidyverse ship with small datasets, such as dplyr which makes available the starwars dataset even if you don’t see it inside your environment window. This contains information about the characters from the Star Wars universe. Create a data frame that contains the data from the dataset starwars. Let’s explore the data: 3.1 What is the class of the variable species? 3.2 What is the mean and median heigth of Star Wars characters? (Hint: you might want to check the help files with ? for the mean() and median() functions.) 3.3 How many eye colors can be found in the eye_color variable? 3.4 The birth_year variable indicates the year of birth relative the the battle of Yavin (BBY). Hence, a birth_year of 8 means that the character was born 8 years before the BBY. Who is the oldest character from the dataset? 3.5 Who is the oldest human from the dataset? Tatooine plays a big role in the movies, create a variable that is TRUE when the character’s homeworld is Tatooine and FALSE otherwise. What type is the resulting variable? Using the variable we just created, form a subset of the data that only contains droids from Tatooine and remove the NA values. Let’s now try our hands at a real-world dataset. Import the wvs6 dataset into your environment. A tricky data type in R is the factor. The wvs6 dataset contains multiple variables which are coded as factors. Print out all the possible categories of the educ variable. 7.1 What type do they appear to be? 7.2 What happens if you transform the variable into a numerical one? (Hint: you need the as.numeric() function.) 7.3 What if you do the same for the country variable? 7.4 Using the droplevels() and the factor() functions, create a new variable in which you remove the 0 frequency levels and inverse the ordering of the soc_class variable. This should result into a variable with Lower class as the first level and Upper class as the last. 7.5 From the variable you created at 7.4, rename the levels of the factor for “LC”, “WC”, “LMC”, “UMC”, and “UC”. The sacsecval, and resemaval respectively correspond to a measure of overall secular values and emancipative values. Create an additive index that combines both variables. Make sure that the values from the resulting variable remains between 0 and 1. In the wvs dataset, what is the country with the highest average age of respondents? (Hint: doing this using base R is quite tedious, we encourage you to use the dplyr functions from the Tidyverse.) Let’s try our hand at some for loops. Create a loop that prints one start (*), then two, then 3, all the way to a specified n. For additional fun, try and make it so once it reaches n the printing goes back to 1, but the indexing needs to be i in 1:((2*n)-1). "],["datasets.html", "Datasets colo colo_covid gss (Canada) gsss16 (USA) Davenport-Soule-Armstrong Data France 2004 ces19 Snijders and Bosker Multileve School Data World Values Survey (multilevel) Repression Data Oil and Democracy Dataset PoliticalDemocracy World Values Survey (measurement)", " Datasets I wanted to have a palce to which you can refer to see some details about the datasets that we’re using here. colo This data comes from a few different sources. hospitals and icu_beds are from KHN. tpop-urban_rural come from the US Census. phys_health-inactivity come from County Health Rankings and Roadmaps. wkly_wage comes from the BLS. Here is a brief description of the variables. colo_covid The colo_covid dataset comes from the NY Times running total of Covid-19 cases and deaths. You can obtain it here. The case and death counts are the current values, not the daily values. The variables are: gss (Canada) The gss2016can dataset (which is in the file data/gss20162can.rda) is the 2016 Canadian General Social Survey. The table for the full dataset is below gsss16 (USA) The gss16 data frame (which is in the data/gss16.rda file) comes from the 2016 US General Social Survey. The table for the entire dataset is below. Davenport-Soule-Armstrong Data These data are replication data from an article I wrote with Christian Davenport and Sarah Soule. There are lots of variables in the dataset, but these are the ones we will use. For more information on the data collection project, see here. France 2004 These data are from a survey concerning peoples’ vote choices in the 2004 French election. The variables in the dataset are as follows: ces19 These data come from the Canadian Election Study’s 2019 survey. Snijders and Bosker Multileve School Data These are data used by Snijders and Bosker for most of the examples in their book - Multilevel Analysis. You can find more about the data here. Here are the variables in the dataset. World Values Survey (multilevel) These data are used in the multilevel models chapter. They come from the World Values Survey, wave 6 (2010-2014). I randomly sampled 50 observations from each country for computational efficiency. The variables are as follows: Repression Data These are data that I used in the article “Measuring the Democracy Repression Nexus” in Electoral Studies in 2009. They come from various different sources. The descriptions of the variables are as follows: Oil and Democracy Dataset These are data that we use in the TSCS/Panel chapte. They come from: Ross, Michel (2001) “Does Oil Hinder Democracy?” World Politics 53(3): 325-361. There are lots of variables in the dataset, but here are the ones we’re primarily using. PoliticalDemocracy These are data that we use in the measurement chapter. They come from Bollen, K. A. (1979). “Political democracy and the timing of development.” American Sociological Review, 44, 572-587. We are getting this particular data frame from the lavaan package. The variables in the dataset are. World Values Survey (measurement) "],["viz.html", "Chapter 1 Visualizing 1.1 Wrangling data 1.2 GGplot 1.3 Recap Exercises", " Chapter 1 Visualizing The goal of this case study is to start making graphs that help us understand our data better. We will be using the ggplot2 package and perhaps some add-ons for making graphs. This is but one of a few different graphical systems in R. The main ones are: base - the base graphics available in R. They are great for quick plots, but for publication-quality plots, we can do better. lattice - based on the Trellis graphics system developed at Bell Labs in the 1990s to implement many of the theoretical ideas discussed by William Cleveland. In R, it is based on the grid package, though we usually don’t interact directly with the functions in grid. ggplot2 - is also based on grid, but is intended to implement Wilkinson’s vision in The Grammar of Graphics of model-based graphical displays. Our first exercise would be to generate some visual insights on the COVID-19 pandemic in Colorado. We’re going to walk through all of the steps that we would use from reading in the data, to managing it to making plots. The first thing we want to do is read in some baseline demographic data. I’ve already got these coded in R. load(&quot;data/colo.rda&quot;) library(summarytools) We can see what variables are there: No Variable Label Stats / Values Freqs (% of Valid) Graph Valid Missing 1 fips [character] FIPS code 1. 08001 2. 08003 3. 08005 4. 08007 5. 08009 6. 08011 7. 08013 8. 08014 9. 08015 10. 08017 [ 54 others ] 1 ( 1.6%) 1 ( 1.6%) 1 ( 1.6%) 1 ( 1.6%) 1 ( 1.6%) 1 ( 1.6%) 1 ( 1.6%) 1 ( 1.6%) 1 ( 1.6%) 1 ( 1.6%) 54 (84.4%) IIIIIIIIIIIIIIII 64 (100.0%) 0 (0.0%) 2 county [character] County name 1. Adams 2. Alamosa 3. Arapahoe 4. Archuleta 5. Baca 6. Bent 7. Boulder 8. Broomfield 9. Chaffee 10. Cheyenne [ 54 others ] 1 ( 1.6%) 1 ( 1.6%) 1 ( 1.6%) 1 ( 1.6%) 1 ( 1.6%) 1 ( 1.6%) 1 ( 1.6%) 1 ( 1.6%) 1 ( 1.6%) 1 ( 1.6%) 54 (84.4%) IIIIIIIIIIIIIIII 64 (100.0%) 0 (0.0%) 3 st [character] State abbreviation 1. CO 64 (100.0%) IIIIIIIIIIIIIIIIIIII 64 (100.0%) 0 (0.0%) 4 hospitals [integer] # Hospitals in the county Mean (sd) : 1.2 (1.3) min &lt; med &lt; max: 0 &lt; 1 &lt; 6 IQR (CV) : 1.2 (1) 0 : 17 (26.6%) 1 : 31 (48.4%) 2 : 7 (10.9%) 3 : 5 ( 7.8%) 4 : 1 ( 1.6%) 5 : 2 ( 3.1%) 6 : 1 ( 1.6%) IIIII IIIIIIIII II I 64 (100.0%) 0 (0.0%) 5 icu_beds [integer] # ICU beds in the county Mean (sd) : 25 (74.1) min &lt; med &lt; max: 0 &lt; 0 &lt; 506 IQR (CV) : 6.5 (3) 20 distinct values : : : : : 64 (100.0%) 0 (0.0%) 6 tpop [numeric] Total population (2009 ACS) Mean (sd) : 177986.4 (362030.9) min &lt; med &lt; max: 1524 &lt; 30028 &lt; 1432984 IQR (CV) : 75579.5 (2) 64 distinct values : : : : :     . 64 (100.0%) 0 (0.0%) 7 mpop [numeric] Male Population (2009 ACS) Mean (sd) : 0.5 (0) min &lt; med &lt; max: 0.5 &lt; 0.5 &lt; 0.7 IQR (CV) : 0 (0.1) 64 distinct values   :   :   : : : : : . 64 (100.0%) 0 (0.0%) 8 fpop [numeric] Female Population (2009 ACS) Mean (sd) : 0.5 (0) min &lt; med &lt; max: 0.3 &lt; 0.5 &lt; 0.5 IQR (CV) : 0 (0.1) 64 distinct values         :         :         :         : :       . : : 64 (100.0%) 0 (0.0%) 9 white_male [numeric] White male proportion Mean (sd) : 0.5 (0) min &lt; med &lt; max: 0.4 &lt; 0.5 &lt; 0.6 IQR (CV) : 0 (0.1) 64 distinct values     :     :     :     : .   : : : 64 (100.0%) 0 (0.0%) 10 white_female [numeric] White female proportion Mean (sd) : 0.4 (0) min &lt; med &lt; max: 0.2 &lt; 0.5 &lt; 0.5 IQR (CV) : 0 (0.1) 64 distinct values           :         . :         : :         : :       . : : 64 (100.0%) 0 (0.0%) 11 white_pop [numeric] White population proportion Mean (sd) : 0.9 (0) min &lt; med &lt; max: 0.8 &lt; 0.9 &lt; 1 IQR (CV) : 0 (0) 64 distinct values       :       :       :       : :   . : : : 64 (100.0%) 0 (0.0%) 12 black_male [numeric] Black male proportion Mean (sd) : 0 (0) min &lt; med &lt; max: 0 &lt; 0 &lt; 0.1 IQR (CV) : 0 (1.4) 64 distinct values : : : : : . . 64 (100.0%) 0 (0.0%) 13 black_female [numeric] Black female proportion Mean (sd) : 0 (0) min &lt; med &lt; max: 0 &lt; 0 &lt; 0.1 IQR (CV) : 0 (1.5) 62 distinct values : : : : : . 64 (100.0%) 0 (0.0%) 14 black_pop [numeric] Black population proportion Mean (sd) : 0 (0) min &lt; med &lt; max: 0 &lt; 0 &lt; 0.1 IQR (CV) : 0 (1.2) 64 distinct values : : : : : : 64 (100.0%) 0 (0.0%) 15 hisp_male [numeric] Hispanic male proportion Mean (sd) : 0.1 (0.1) min &lt; med &lt; max: 0 &lt; 0.1 &lt; 0.3 IQR (CV) : 0.1 (0.7) 64 distinct values   :   : . : : :   . . : : : : : 64 (100.0%) 0 (0.0%) 16 hisp_female [numeric] Hispanic female proportion Mean (sd) : 0.1 (0.1) min &lt; med &lt; max: 0 &lt; 0.1 &lt; 0.3 IQR (CV) : 0.1 (0.7) 64 distinct values : : : : : : : : . . : : : : : . 64 (100.0%) 0 (0.0%) 17 hisp_pop [numeric] Hispanic population proportion Mean (sd) : 0.2 (0.1) min &lt; med &lt; max: 0.1 &lt; 0.1 &lt; 0.6 IQR (CV) : 0.2 (0.7) 64 distinct values   :   : : : : : . . : : : : : 64 (100.0%) 0 (0.0%) 18 over60_pop [numeric] Over 60 population proportion Mean (sd) : 0.6 (0) min &lt; med &lt; max: 0.6 &lt; 0.6 &lt; 0.7 IQR (CV) : 0 (0) 64 distinct values       : .     : : :     : : :     : : : . : : : : : : 64 (100.0%) 0 (0.0%) 19 over60_male [numeric] Over 60 male proportion Mean (sd) : 0.3 (0) min &lt; med &lt; max: 0.3 &lt; 0.3 &lt; 0.5 IQR (CV) : 0 (0.1) 64 distinct values   : . : : : : : : : : . 64 (100.0%) 0 (0.0%) 20 over60_female [numeric] Over 60 female proportion Mean (sd) : 0.3 (0) min &lt; med &lt; max: 0.2 &lt; 0.3 &lt; 0.3 IQR (CV) : 0 (0.1) 64 distinct values               :               :               :             : : :           . : : : 64 (100.0%) 0 (0.0%) 21 lt25k [numeric] Proportion making less than $25,000/year Mean (sd) : 0.2 (0.1) min &lt; med &lt; max: 0.1 &lt; 0.3 &lt; 0.5 IQR (CV) : 0.1 (0.4) 64 distinct values     :   :   . :   : : :   : : : : : :   : : : : : : : : : : : : : :   . 64 (100.0%) 0 (0.0%) 22 gt100k [numeric] Proportion making more than $100,000/year Mean (sd) : 0.2 (0.1) min &lt; med &lt; max: 0 &lt; 0.1 &lt; 0.5 IQR (CV) : 0.1 (0.6) 64 distinct values   :   : . : : : . : : : : : : : . : 64 (100.0%) 0 (0.0%) 23 nohs [numeric] Proportion without a high school degree Mean (sd) : 0.1 (0.1) min &lt; med &lt; max: 0 &lt; 0.1 &lt; 0.3 IQR (CV) : 0.1 (0.5) 64 distinct values   .   :   :   : . :   : : : : : : : : : : : 64 (100.0%) 0 (0.0%) 24 BAplus [numeric] Proportion with a BA/S degree or greater Mean (sd) : 0.3 (0.1) min &lt; med &lt; max: 0.1 &lt; 0.2 &lt; 0.6 IQR (CV) : 0.2 (0.4) 64 distinct values : : : : : : : : .   : . : : : : : : : : . . 64 (100.0%) 0 (0.0%) 25 repvote [numeric] Republican share of two-party vote in 2020 Mean (sd) : 0.6 (0.2) min &lt; med &lt; max: 0.2 &lt; 0.6 &lt; 0.9 IQR (CV) : 0.3 (0.3) 64 distinct values     :     .     :   . : .     : : : : : . . : : : : : : : : : : : : 64 (100.0%) 0 (0.0%) 26 urban_rural [factor] Census Urban-rural classification 1. Metro &gt;1M 2. Metro 250k-1M 3. Metro &lt; 250k 4. UP &gt; 20k, adj 5. UP &lt; 20k, non-adj 6. UP 2500-20k, adj 7. UP 2500-20k, non-adj 8. Rural, adj 9. Rural, non-adj 10 (15.6%) 5 ( 7.8%) 2 ( 3.1%) 3 ( 4.7%) 3 ( 4.7%) 6 ( 9.4%) 15 (23.4%) 3 ( 4.7%) 17 (26.6%) III I I IIII IIIII 64 (100.0%) 0 (0.0%) 27 tot_pop [integer] Total Poulation (Colorado Estimate) Mean (sd) : 90062.1 (183363.4) min &lt; med &lt; max: 726 &lt; 15105.5 &lt; 729239 IQR (CV) : 38011.5 (2) 64 distinct values : : : : :     . 64 (100.0%) 0 (0.0%) 28 full_vac_pop [integer] Number of fully vaccinated people Mean (sd) : 46407.9 (98270) min &lt; med &lt; max: 420 &lt; 6631 &lt; 437037 IQR (CV) : 15386.2 (2.1) 64 distinct values : : : : : 64 (100.0%) 0 (0.0%) 29 pct_vac_pop [numeric] Proportion of total population that is fully vaccinated Mean (sd) : 44.6 (13.6) min &lt; med &lt; max: 16.4 &lt; 42.9 &lt; 78.2 IQR (CV) : 19.5 (0.3) 63 distinct values     . :     : : .   . : : : .   : : : : : . : : : : : . 64 (100.0%) 0 (0.0%) 30 eligible_pop [integer] Number of people eligible for the vaccine (12+) Mean (sd) : 77351.3 (156889.4) min &lt; med &lt; max: 661 &lt; 13084 &lt; 632816 IQR (CV) : 33153.2 (2) 64 distinct values : : : : : 64 (100.0%) 0 (0.0%) 31 full_vac_eligible [integer] Number of eligible people vaccinated Mean (sd) : 46407.1 (98268.6) min &lt; med &lt; max: 420 &lt; 6631 &lt; 437030 IQR (CV) : 15386 (2.1) 64 distinct values : : : : : 64 (100.0%) 0 (0.0%) 32 pct_vac_eligible [numeric] Proportion of eligible population that is fully vaccinated Mean (sd) : 51.2 (15.1) min &lt; med &lt; max: 17.9 &lt; 49.5 &lt; 85.9 IQR (CV) : 23 (0.3) 62 distinct values       :     . : :     : : : .     : : : : : . : : : : : : . 64 (100.0%) 0 (0.0%) 33 adult_pop [integer] Adult population (18+) Mean (sd) : 70368.8 (142887.8) min &lt; med &lt; max: 613 &lt; 11890.5 &lt; 590056 IQR (CV) : 30172.8 (2) 64 distinct values : : : : :   .     . 64 (100.0%) 0 (0.0%) 34 full_vac_adult [integer] Number of adults who are fully vaccinated Mean (sd) : 43782 (92506) min &lt; med &lt; max: 412 &lt; 6469.5 &lt; 415992 IQR (CV) : 14743.2 (2.1) 64 distinct values : : : : :     . 64 (100.0%) 0 (0.0%) 35 pct_vac_adult [numeric] Proportion of adults who are fullyvaccinated Mean (sd) : 53.8 (14.9) min &lt; med &lt; max: 18.4 &lt; 53.1 &lt; 88.6 IQR (CV) : 21.6 (0.3) 61 distinct values       : :     . : : .     : : : : .     : : : : : . . : : : : : : 64 (100.0%) 0 (0.0%) 36 over65_pop [integer] Number of people over the age of 65 Mean (sd) : 13230.7 (24794.9) min &lt; med &lt; max: 199 &lt; 2681 &lt; 99332 IQR (CV) : 7059.2 (1.9) 64 distinct values : : : : :   .   . 64 (100.0%) 0 (0.0%) 37 full_vac_over65 [integer] Number of people over 65 who are fully vaccinated Mean (sd) : 10693.3 (20642.6) min &lt; med &lt; max: 142 &lt; 1975.5 &lt; 85755 IQR (CV) : 5349 (1.9) 64 distinct values : : : : :       .     . 64 (100.0%) 0 (0.0%) 38 pct_vac_over65 [numeric] Proportion of those over 65 who are fully vaccinated Mean (sd) : 71.7 (11.9) min &lt; med &lt; max: 34.8 &lt; 70.8 &lt; 93.9 IQR (CV) : 15.4 (0.2) 59 distinct values       :       : .       : : .       : : :     : : : : : 64 (100.0%) 0 (0.0%) 39 phys_health [numeric] Average number of physically unhealthy days out of the last 30 (2016) Mean (sd) : 3.4 (0.5) min &lt; med &lt; max: 2.5 &lt; 3.3 &lt; 4.8 IQR (CV) : 0.7 (0.1) 20 distinct values   : . : : : : : : : : : : . 64 (100.0%) 0 (0.0%) 40 mental_health [numeric] Average number of mentally unhealthy days out of the last 30 (2016) Mean (sd) : 3.5 (0.3) min &lt; med &lt; max: 2.9 &lt; 3.5 &lt; 4.4 IQR (CV) : 0.4 (0.1) 15 distinct values     . :     : :   : : : .   : : : :   . . : : : : : : . 64 (100.0%) 0 (0.0%) 41 smoking_pct [numeric] Percentage of adults who currently smoke (2016) Mean (sd) : 0.1 (0) min &lt; med &lt; max: 0.1 &lt; 0.1 &lt; 0.2 IQR (CV) : 0 (0.1) 12 distinct values   :   : :   : :   : : : : : : :   . 64 (100.0%) 0 (0.0%) 42 inactivity [numeric] Percentage of adults over 20 reporting no leisure-time physical acticity (2016) Mean (sd) : 0.2 (0) min &lt; med &lt; max: 0.1 &lt; 0.2 &lt; 0.3 IQR (CV) : 0.1 (0.2) 16 distinct values       :       :   .   : . : : : : : : : . : : : : : : : . 64 (100.0%) 0 (0.0%) 43 wkly_wage [numeric] Average Weekly Wage Mean (sd) : 894.1 (219.3) min &lt; med &lt; max: 548 &lt; 832 &lt; 1606 IQR (CV) : 216.2 (0.2) 64 distinct values     :   : :   : :   : : . . : : : . . 64 (100.0%) 0 (0.0%) 44 rich_poor [factor] 1. &lt; $750 2. $750 - $1000 3. &lt; $1000 14 (21.9%) 35 (54.7%) 15 (23.4%) IIII IIIIIIIIII IIII 64 (100.0%) 0 (0.0%) We could then look at the structure of the data: str(colo) ## &#39;data.frame&#39;: 64 obs. of 44 variables: ## $ fips : chr &quot;08109&quot; &quot;08115&quot; &quot;08017&quot; &quot;08027&quot; ... ## ..- attr(*, &quot;label&quot;)= chr &quot;FIPS code&quot; ## $ county : chr &quot;Saguache&quot; &quot;Sedgwick&quot; &quot;Cheyenne&quot; &quot;Custer&quot; ... ## ..- attr(*, &quot;label&quot;)= chr &quot;County name&quot; ## $ st : chr &quot;CO&quot; &quot;CO&quot; &quot;CO&quot; &quot;CO&quot; ... ## ..- attr(*, &quot;label&quot;)= chr &quot;State abbreviation&quot; ## $ hospitals : int 0 1 1 0 2 0 1 0 1 1 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;# Hospitals in the county&quot; ## $ icu_beds : int 0 0 0 0 11 0 4 0 6 0 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;# ICU beds in the county&quot; ## $ tpop : num 13686 4606 3752 9908 112620 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Total population (2009 ACS)&quot; ## $ mpop : num 0.501 0.494 0.509 0.516 0.505 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Male Population (2009 ACS)&quot; ## $ fpop : num 0.499 0.506 0.491 0.484 0.495 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Female Population (2009 ACS)&quot; ## $ white_male : num 0.456 0.458 0.481 0.484 0.448 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;White male proportion&quot; ## $ white_female : num 0.458 0.482 0.472 0.461 0.436 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;White female proportion&quot; ## $ white_pop : num 0.914 0.94 0.954 0.945 0.884 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;White population proportion&quot; ## $ black_male : num 0.00658 0.00782 0.0032 0.01171 0.00387 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Black male proportion&quot; ## $ black_female : num 0.00453 0.00174 0.00586 0.00323 0.00224 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Black female proportion&quot; ## $ black_pop : num 0.01111 0.00955 0.00906 0.01494 0.00611 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Black population proportion&quot; ## $ hisp_male : num 0.1723 0.0816 0.0672 0.0317 0.0655 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Hispanic male proportion&quot; ## $ hisp_female : num 0.1868 0.0851 0.0496 0.0234 0.0642 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Hispanic female proportion&quot; ## $ hisp_pop : num 0.3591 0.1667 0.1167 0.0551 0.1297 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Hispanic population proportion&quot; ## $ over60_pop : num 0.623 0.618 0.633 0.601 0.638 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Over 60 population proportion&quot; ## $ over60_male : num 0.311 0.306 0.324 0.311 0.323 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Over 60 male proportion&quot; ## $ over60_female : num 0.312 0.313 0.309 0.29 0.315 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Over 60 female proportion&quot; ## $ lt25k : num 0.403 0.312 0.249 0.291 0.197 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Proportion making less than $25,000/year&quot; ## $ gt100k : num 0.0954 0.0829 0.0968 0.0835 0.2053 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Proportion making more than $100,000/year&quot; ## $ nohs : num 0.2687 0.1865 0.1203 0.0611 0.0655 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Proportion without a high school degree&quot; ## $ BAplus : num 0.19 0.138 0.207 0.296 0.416 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Proportion with a BA/S degree or greater&quot; ## $ repvote : num 0.447 0.792 0.875 0.721 0.448 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Republican share of two-party vote in 2020&quot; ## $ urban_rural : Factor w/ 9 levels &quot;Metro &gt;1M&quot;,&quot;Metro 250k-1M&quot;,..: 9 9 9 8 4 9 7 1 7 7 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Census Urban-rural classification&quot; ## $ tot_pop : int 6824 2229 1825 5059 56272 726 17756 18844 16181 12122 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Total Poulation (Colorado Estimate)&quot; ## $ full_vac_pop : int 2371 897 420 1974 31638 568 10721 7586 6987 3723 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Number of fully vaccinated people&quot; ## $ pct_vac_pop : num 34.7 40.2 23 39 56.2 78.2 60.4 40.3 43.2 30.7 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Proportion of total population that is fully vaccinated&quot; ## $ eligible_pop : int 5852 1948 1533 4612 49629 661 16204 16857 13454 10024 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Number of people eligible for the vaccine (12+)&quot; ## $ full_vac_eligible: int 2371 897 420 1974 31635 568 10719 7586 6987 3723 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Number of eligible people vaccinated&quot; ## $ pct_vac_eligible : num 40.5 46 27.4 42.8 63.7 85.9 66.2 45 51.9 37.1 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Proportion of eligible population that is fully vaccinated&quot; ## $ adult_pop : int 5311 1797 1382 4330 45313 613 15249 15624 12058 8924 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Adult population (18+)&quot; ## $ full_vac_adult : int 2316 893 419 1961 30042 543 10189 7350 6617 3611 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Number of adults who are fully vaccinated&quot; ## $ pct_vac_adult : num 43.6 49.7 30.3 45.3 66.3 88.6 66.8 47 54.9 40.5 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Proportion of adults who are fullyvaccinated&quot; ## $ over65_pop : int 1664 613 381 1648 10210 199 3720 4061 2367 2165 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Number of people over the age of 65&quot; ## $ full_vac_over65 : int 847 474 204 1114 8790 142 1950 2519 1915 1405 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Number of people over 65 who are fully vaccinated&quot; ## $ pct_vac_over65 : num 50.9 77.3 53.5 67.6 86.1 71.4 52.4 62 80.9 64.9 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Proportion of those over 65 who are fully vaccinated&quot; ## $ phys_health : num 4.5 3.3 3.1 3.4 3.1 3.3 3 2.8 4.1 3.7 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Average number of physically unhealthy days out of the last 30 (2016)&quot; ## $ mental_health : num 4.2 3.5 3.4 3.7 3.6 3.4 3.1 3.2 4.1 3.8 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Average number of mentally unhealthy days out of the last 30 (2016)&quot; ## $ smoking_pct : num 0.18 0.15 0.14 0.15 0.13 0.16 0.13 0.15 0.17 0.17 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Percentage of adults who currently smoke (2016)&quot; ## $ inactivity : num 0.17 0.2 0.26 0.2 0.13 0.18 0.1 0.16 0.17 0.22 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Percentage of adults over 20 reporting no leisure-time physical acticity (2016)&quot; ## $ wkly_wage : num 711 758 903 679 848 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Average Weekly Wage&quot; ## $ rich_poor : Factor w/ 3 levels &quot;&lt; $750&quot;,&quot;$750 - $1000&quot;,..: 1 2 2 1 2 1 3 2 2 1 ... Next, we can download the most recent COVID-19 case data: library(rio) f &lt;- &quot;https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv&quot; covid &lt;- import(f) The data we downloaded has all of the counties across the US, so we’ll need to learn about how to subset and filter data. To this, we turn to the tidyverse. 1.1 Wrangling data We’re going to use the dplyr package, which is part of the tidyverse, for wrangling data. There are tons of different ways to filter, subset, select, sample, etc… data, but the tidyverse functions are generally pretty intuitive (after a brief introduction) and simple. We can do lots of things with dplyr, but the most common operations could be boiled down to the following: mutate() adds new variables that are functions of existing variables select() picks variables based on their names. filter() picks cases based on their values. arrange() changes the ordering of the rows. summarise() reduces multiple values down to a single summary. reframe() is a new verb that is like summarise, but doesn’t expect a single observation per group. There is also a group_by() operator that allows the operations to be done for groups of observations rather than the entire data frame. Here is a useful cheat sheet for the dplyr functions. These functions are often combined with the pipe character, %&gt;%. The pipe character operates by taking whatever is on its left side and sending it to the function on its right. The tidyverse functions are all configured so that they expect the first argument to be the piped data. However, you could put the piped output anywhere in the function as the period (.) stands in for the piped output. We’ll see some examples of this later on. A function that uses the pipe character usually starts by piping the original data into one of the dplyr verbs. For example, we could filter the covid data to only contain those from Colorado from the most recent date. To do this, we could 1) group the data by county, 2) arrange by date and 3, take the last observation for each county. library(dplyr) covid &lt;- covid %&gt;% filter(state == &quot;Colorado&quot;) covid_last &lt;- covid %&gt;% group_by(county) %&gt;% arrange(date, .by_group = TRUE) %&gt;% reframe(deaths = last(deaths), cases = last(cases)) 1.1.1 Merging Datasets Next, we could go ahead and put our two datasets together. The left_join() function will keep all of the rows of the first argument and only the matching elements of the second dataset. right_join() does the opposite. There are also full_join() which keeps all elements of both datasets and inner_join() which keeps on the elements present in both datasets. By default, the *_join() functions will join the data based on all common columns across both datasets. You can change this behaviour by using join = join_by(left_var == right_var) where left_var is the name of the variable to join by the dataset on the left side of the join and right_var is the same for the right-side dataset. Just by way of information, there are also some filter-join functions in the dplyr package. semi_join() returns the rows of its first argument that exist in its second argument and anti_join() returns the rows of its first argument that do not exist in its second argument. colo_dat &lt;- colo %&gt;% select(-fips, -st) %&gt;% left_join(covid_last) colo_dat ## county hospitals icu_beds tpop mpop fpop white_male white_female white_pop ## 1 Saguache 0 0 13686 0.5006576 0.4993424 0.4559404 0.4579863 0.9139266 ## 2 Sedgwick 1 0 4606 0.4937039 0.5062961 0.4580981 0.4815458 0.9396439 ## 3 Cheyenne 1 0 3752 0.5085288 0.4914712 0.4813433 0.4722814 0.9536247 ## 4 Custer 0 0 9908 0.5157449 0.4842551 0.4840533 0.4606379 0.9446912 ## 5 La Plata 2 11 112620 0.5045463 0.4954537 0.4476647 0.4362280 0.8838927 ## 6 San Juan 0 0 1524 0.5603675 0.4396325 0.5301837 0.4212598 0.9514436 ## 7 Pitkin 1 4 35900 0.5174930 0.4825070 0.4932033 0.4562674 0.9494708 ## 8 Park 0 0 37112 0.5264066 0.4735934 0.4965510 0.4454624 0.9420134 ## 9 Alamosa 1 6 33366 0.5011689 0.4988311 0.4345741 0.4381107 0.8726848 ## 10 Prowers 1 0 24328 0.5025485 0.4974515 0.4754193 0.4711444 0.9465636 ## 11 Moffat 1 0 26376 0.5128147 0.4871853 0.4848347 0.4606460 0.9454807 ## 12 Jefferson 4 94 1160466 0.4985825 0.5014175 0.4570164 0.4599962 0.9170126 ## 13 Summit 1 8 62014 0.5423292 0.4576708 0.5141420 0.4362241 0.9503660 ## 14 Pueblo 2 32 335058 0.4927863 0.5072137 0.4424249 0.4591623 0.9015872 ## 15 Boulder 5 72 652156 0.5031588 0.4968412 0.4543698 0.4475923 0.9019621 ## 16 Denver 6 227 1432984 0.5018758 0.4981242 0.4081748 0.3997951 0.8079699 ## 17 Montrose 1 8 84428 0.4930829 0.5069171 0.4632113 0.4796039 0.9428152 ## 18 Eagle 1 5 109986 0.5302493 0.4697507 0.4986271 0.4453112 0.9439383 ## 19 San Miguel 0 0 16382 0.5321695 0.4678305 0.5081187 0.4430472 0.9511659 ## 20 Costilla 0 0 7656 0.5177638 0.4822362 0.4566353 0.4250261 0.8816614 ## 21 Broomfield 1 20 138534 0.4972209 0.5027791 0.4384339 0.4416389 0.8800728 ## 22 Chaffee 1 2 40054 0.5283867 0.4716133 0.4901882 0.4492935 0.9394817 ## black_male black_female black_pop hisp_male hisp_female hisp_pop over60_pop over60_male ## 1 0.0065761 0.0045302 0.0111062 0.1722929 0.1867602 0.3590530 0.6232647 0.3108709 ## 2 0.0078159 0.0017369 0.0095528 0.0816327 0.0851064 0.1667390 0.6183811 0.3057225 ## 3 0.0031983 0.0058635 0.0090618 0.0671642 0.0495736 0.1167377 0.6333464 0.3241780 ## 4 0.0117077 0.0032297 0.0149374 0.0316916 0.0234154 0.0551070 0.6012898 0.3110830 ## 5 0.0038714 0.0022376 0.0061090 0.0654591 0.0641982 0.1296573 0.6378880 0.3226304 ## 6 0.0013123 0.0000000 0.0013123 0.0708661 0.0524934 0.1233596 0.6213565 0.3500483 ## 7 0.0075766 0.0028969 0.0104735 0.0495822 0.0517549 0.1013370 0.6335860 0.3271969 ## 8 0.0045807 0.0040957 0.0086764 0.0334124 0.0288855 0.0622979 0.6249745 0.3294877 ## 9 0.0121081 0.0090511 0.0211593 0.2303543 0.2277168 0.4580711 0.6496055 0.3265410 ## 10 0.0054258 0.0034528 0.0088787 0.2033870 0.1830812 0.3864683 0.6395511 0.3230867 ## 11 0.0045496 0.0032605 0.0078101 0.0846982 0.0718077 0.1565059 0.6426835 0.3303122 ## 12 0.0081347 0.0059511 0.0140857 0.0765606 0.0783582 0.1549188 0.6411245 0.3212147 ## 13 0.0086110 0.0040958 0.0127068 0.0773374 0.0684362 0.1457735 0.6462361 0.3510404 ## 14 0.0144035 0.0114070 0.0258105 0.2140883 0.2168162 0.4309045 0.6376744 0.3158653 ## 15 0.0067499 0.0049651 0.0117150 0.0703758 0.0687627 0.1391385 0.6466982 0.3268295 ## 16 0.0494772 0.0487849 0.0982621 0.1502808 0.1463896 0.2966704 0.6537768 0.3296953 ## 17 0.0042166 0.0030559 0.0072725 0.1070261 0.1000379 0.2070640 0.6267402 0.3102388 ## 18 0.0111287 0.0044915 0.0156202 0.1557471 0.1424909 0.2982380 0.6515174 0.3457555 ## 19 0.0054938 0.0031742 0.0086681 0.0554267 0.0515200 0.1069466 0.6430550 0.3415366 ## 20 0.0114943 0.0088819 0.0203762 0.3074713 0.2951933 0.6026646 0.6152519 0.3184087 ## 21 0.0084745 0.0066698 0.0151443 0.0635945 0.0624540 0.1260485 0.6484918 0.3243828 ## 22 0.0158286 0.0015479 0.0173765 0.0636141 0.0376492 0.1012633 0.6210195 0.3309992 ## over60_female lt25k gt100k nohs BAplus repvote urban_rural tot_pop ## 1 0.3123938 0.4033045 0.0953811 0.2687361 0.1895787 0.4473479 Rural, non-adj 6824 ## 2 0.3126585 0.3124336 0.0828905 0.1864989 0.1384439 0.7917317 Rural, non-adj 2229 ## 3 0.3091684 0.2488479 0.0967742 0.1202532 0.2070524 0.8751183 Rural, non-adj 1825 ## 4 0.2902067 0.2910489 0.0834706 0.0611050 0.2963776 0.7211337 Rural, adj 5059 ## 5 0.3152576 0.1970761 0.2053351 0.0654549 0.4158661 0.4477447 UP &gt; 20k, adj 56272 ## 6 0.2713082 0.1392758 0.0724234 0.0448029 0.4516129 0.4479167 Rural, non-adj 726 ## 7 0.3063891 0.1525447 0.3312360 0.0445772 0.5906691 0.2580188 UP 2500-20k, non-adj 17756 ## 8 0.2954868 0.1299184 0.2435255 0.0707616 0.3135566 0.6420050 Metro &gt;1M 18844 ## 9 0.3230644 0.3761405 0.1177483 0.1626225 0.2659823 0.4885325 UP 2500-20k, non-adj 16181 ## 10 0.3164645 0.3589133 0.0656355 0.2092519 0.1704614 0.7485690 UP 2500-20k, non-adj 12122 ## 11 0.3123713 0.2124651 0.1397209 0.1152357 0.1471191 0.8585532 UP 2500-20k, non-adj 13252 ## 12 0.3199098 0.1543146 0.2831228 0.0742428 0.3843765 0.4622031 Metro &gt;1M 583081 ## 13 0.2951957 0.0982958 0.2765267 0.0550049 0.4885633 0.3479566 UP &lt; 20k, non-adj 30983 ## 14 0.3218090 0.3143779 0.1142508 0.1504617 0.2097279 0.5027031 Metro &lt; 250k 168110 ## 15 0.3198687 0.1930457 0.3150113 0.0663074 0.5663567 0.2382778 Metro 250k-1M 327164 ## 16 0.3240815 0.2794546 0.1860275 0.1671625 0.3925907 0.2040418 Metro &gt;1M 729239 ## 17 0.3165014 0.2769821 0.1219949 0.1520444 0.2029647 0.7246070 UP &gt; 20k, adj 42765 ## 18 0.3057619 0.1315326 0.3073141 0.1262167 0.4696625 0.3893629 UP &lt; 20k, non-adj 55070 ## 19 0.3015184 0.1968556 0.2210940 0.0605951 0.4593614 0.2577345 Rural, non-adj 8174 ## 20 0.2968432 0.5222052 0.0581930 0.2619849 0.1707317 0.3432574 Rural, non-adj 3872 ## 21 0.3241089 0.1079362 0.3522239 0.0456233 0.4193482 0.4213444 Metro &gt;1M 70762 ## 22 0.2900203 0.2997847 0.1248654 0.0906938 0.2949720 0.5244674 UP 2500-20k, non-adj 20361 ## full_vac_pop pct_vac_pop eligible_pop full_vac_eligible pct_vac_eligible adult_pop ## 1 2371 34.7 5852 2371 40.5 5311 ## 2 897 40.2 1948 897 46.0 1797 ## 3 420 23.0 1533 420 27.4 1382 ## 4 1974 39.0 4612 1974 42.8 4330 ## 5 31638 56.2 49629 31635 63.7 45313 ## 6 568 78.2 661 568 85.9 613 ## 7 10721 60.4 16204 10719 66.2 15249 ## 8 7586 40.3 16857 7586 45.0 15624 ## 9 6987 43.2 13454 6987 51.9 12058 ## 10 3723 30.7 10024 3723 37.1 8924 ## 11 3968 29.9 11081 3968 35.8 9851 ## 12 348157 59.7 508992 348149 68.4 467388 ## 13 21528 69.5 28057 21527 76.7 26562 ## 14 70396 41.9 144823 70395 48.6 131053 ## 15 212447 64.9 289956 212443 73.3 265779 ## 16 437037 59.9 632816 437030 69.1 590056 ## 17 15778 36.9 37085 15778 42.5 33523 ## 18 35477 64.4 47782 35476 74.2 43715 ## 19 5523 67.6 7120 5523 77.6 6503 ## 20 1935 50.0 3406 1935 56.8 3122 ## 21 46211 65.3 61262 46211 75.4 55296 ## 22 10776 52.9 18234 10775 59.1 17080 ## full_vac_adult pct_vac_adult over65_pop full_vac_over65 pct_vac_over65 phys_health mental_health ## 1 2316 43.6 1664 847 50.9 4.5 4.2 ## 2 893 49.7 613 474 77.3 3.3 3.5 ## 3 419 30.3 381 204 53.5 3.1 3.4 ## 4 1961 45.3 1648 1114 67.6 3.4 3.7 ## 5 30042 66.3 10210 8790 86.1 3.1 3.6 ## 6 543 88.6 199 142 71.4 3.3 3.4 ## 7 10189 66.8 3720 1950 52.4 3.0 3.1 ## 8 7350 47.0 4061 2519 62.0 2.8 3.2 ## 9 6617 54.9 2367 1915 80.9 4.1 4.1 ## 10 3611 40.5 2165 1405 64.9 3.7 3.8 ## 11 3909 39.7 2156 1331 61.7 3.4 3.4 ## 12 328652 70.3 99332 85755 86.3 2.8 3.2 ## 13 20517 77.2 4335 3573 82.4 3.0 3.1 ## 14 67385 51.4 32020 24240 75.7 4.6 4.2 ## 15 198452 74.7 48807 45826 93.9 2.9 3.1 ## 16 415992 70.5 87947 70603 80.3 3.2 3.6 ## 17 15403 45.9 10174 7026 69.1 3.6 3.6 ## 18 33443 76.5 6869 5983 87.1 2.9 3.1 ## 19 5211 80.1 1258 966 76.8 3.0 3.4 ## 20 1870 59.9 1069 744 69.6 4.8 4.4 ## 21 42634 77.1 10213 9559 93.6 2.8 3.3 ## 22 10446 61.2 5391 4206 78.0 3.1 3.6 ## smoking_pct inactivity wkly_wage rich_poor deaths cases ## 1 0.18 0.17 711 &lt; $750 12 1203 ## 2 0.15 0.20 758 $750 - $1000 11 490 ## 3 0.14 0.26 903 $750 - $1000 9 322 ## 4 0.15 0.20 679 &lt; $750 21 656 ## 5 0.13 0.13 848 $750 - $1000 86 13002 ## 6 0.16 0.18 548 &lt; $750 0 203 ## 7 0.13 0.10 1149 &lt; $1000 7 6676 ## 8 0.15 0.16 830 $750 - $1000 20 2913 ## 9 0.17 0.17 783 $750 - $1000 62 4560 ## 10 0.17 0.22 693 &lt; $750 51 2654 ## 11 0.16 0.21 1000 $750 - $1000 52 3062 ## 12 0.13 0.13 1221 &lt; $1000 1356 128067 ## 13 0.15 0.10 834 $750 - $1000 14 10087 ## 14 0.17 0.22 907 $750 - $1000 802 47690 ## 15 0.11 0.10 1418 &lt; $1000 373 68435 ## 16 0.16 0.12 1458 &lt; $1000 1360 177033 ## 17 0.14 0.16 823 $750 - $1000 160 9398 ## 18 0.13 0.11 1015 &lt; $1000 35 16274 ## 19 0.15 0.14 899 $750 - $1000 8 2322 ## 20 0.19 0.24 648 &lt; $750 18 708 ## 21 0.13 0.13 1606 &lt; $1000 116 13997 ## 22 0.13 0.17 845 $750 - $1000 47 4749 ## [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 42 rows ] 1.2 GGplot As mentioned above, we’re using the ggplot package to make plots. There is another nice cheat sheet here. There are a few different aspects of the plot that we can specify. Geometries are ways of mapping the data onto aesthetic elements (values, colour, shape, size, etc…) of the data onto the plotting region. Themes govern the overall look of the plot including color schemes for the background, axis labels, positioning of the legend, etc… Scales govern how the axes behave and the exact ways in which aesthetic elements are represented in the plot. Facets allow multiple-panel plots to be constructed. Let’s say we wanted to make a bar plot such that the height of the bar represented the number of distinct observations we had each day. All of the plots we make will start with the ggplot() function. This initializes the plot with data and often aesthetics. Aesthetic elements that show up in the call to ggplot() are defined for all subsequent geometries. We can also define aesthetic elements to be specific to each geometry if desired. There are times we’ll see later where this might be useful. 1.2.1 Bar Plots In the plot below, we plot urban_rural (the county’s urban-rural status) as the variable on the \\(x\\)-axis. Because we are making a bar graph it automatically counts up the number of observations in each group and plots that on the y-axis. If this is the behaviour we want, we do not need to supply a y aesthetic. Then we add (with a + sign) the bar geometry to our initialized plot and we get the following. library(ggplot2) ggplot(colo_dat, aes(x=urban_rural)) + geom_bar() We can make the plot a bit “prettier” (to my eye at least) by changing the theme and adding a title. ggplot(colo_dat, aes(x=urban_rural)) + geom_bar() + theme_bw() + ggtitle(&quot;COVID-19 Observations in Colorado Counties by Day&quot;) + labs(x=&quot;County Density&quot;, y=&quot;Number of Observations&quot;) We also see that the text labels on the x-axis are overplotting each other. We can solve this in two ways. We could use ccord_flip() at the end to make horizontal rather than vertical bars. Here is what that looks like. ggplot(colo_dat, aes(x=urban_rural)) + geom_bar() + theme_bw() + ggtitle(&quot;COVID-19 Observations in Colorado Counties by Day&quot;) + labs(x=&quot;County Density&quot;, y=&quot;Number of Observations&quot;) + coord_flip() You Try It! Now, I want you to try making a bar plot to see how many observations are in each level of the variable rich_poor (a categorical indicator of average weekly wage). We could also make bar graphs by plotting the mean of some variable by some other variable. For example, we might want to plot the mental health by urban-rural context from the colo data object. Here ggplot(colo_dat, aes(x=urban_rural, y=mental_health)) + stat_summary(geom=&quot;bar&quot;, fun=mean) + theme_bw() + labs(x=&quot;&quot;, y=&quot;Average Mental Health Score&quot;) + coord_flip() Now, one last thing that we might want to do would be to order the bars by height. We could do this with the reorder() function - applied to the x aesthetic in the call to ggplot(). ggplot(colo, aes(x=reorder(urban_rural, mental_health, mean), y=mental_health)) + stat_summary(geom=&quot;bar&quot;, fun=mean) + theme_bw() + labs(x=&quot;&quot;, y=&quot;Average Mental Health Score&quot;) + coord_flip() You Try It! Now you can make a bar plot of average republican vote by urban-rural context. Make sure to sort by height. If we wanted to see the actual data instead, we could use the summarise() function from the dplyr package. colo_dat %&gt;% group_by(urban_rural) %&gt;% summarise(mean_mh = mean(mental_health)) %&gt;% arrange(desc(mean_mh)) ## # A tibble: 9 × 2 ## urban_rural mean_mh ## &lt;fct&gt; &lt;dbl&gt; ## 1 Metro &lt; 250k 4.05 ## 2 UP 2500-20k, adj 3.83 ## 3 Rural, adj 3.77 ## 4 UP &gt; 20k, adj 3.67 ## 5 Rural, non-adj 3.63 ## 6 UP 2500-20k, non-adj 3.54 ## 7 Metro 250k-1M 3.28 ## 8 Metro &gt;1M 3.21 ## 9 UP &lt; 20k, non-adj 3.17 1.2.2 Line Graphs Now that our two datasets are together, we could make some more graphs. We could make line-graphs of cases. First, we could show how the total number of cases looks across counties. ggplot(covid, aes(x=date, y=cases)) + stat_summary(fun=sum, geom=&quot;line&quot;) + theme_bw() + labs(x=&quot;Date&quot;, y=&quot;Total COVID-19 Cases&quot;) Note that cases in the dataset is the number of cases to date - a cumulative sum. If we wanted to figure out how many new cases happened each day, we could subtract the previous day’s total form the current day. We could do this with the lag() function. library(tidyr) covid_filled &lt;- covid %&gt;% complete(county, date) %&gt;% fill(cases, deaths, .direction = &quot;down&quot;) %&gt;% mutate(cases = ifelse(cases &lt; dplyr::lag(cases), dplyr::lag(cases), cases)) %&gt;% filter(date &gt;= lubridate::ymd(&quot;2021-01-01&quot;)) %&gt;% group_by(county) %&gt;% arrange(date, .by_group = TRUE) %&gt;% mutate(new_cases = cases - dplyr::lag(cases)) Then, we could plot the sum of new_cases by date. ggplot(covid_filled, aes(x=date, y=new_cases)) + stat_summary(fun=sum, geom=&quot;line&quot;) + theme_bw() + labs(x=&quot;Date&quot;, y=&quot;New COVID-19 Cases&quot;) You Try It! Now, I want you to try doing the same thing with the deaths variable. We could also put a smooth trend line in the data with the geom_smooth() function. For this to work, though, we would need to actually make the summary data. covid_filled %&gt;% group_by(date) %&gt;% mutate(new_cases = sum(new_cases)) %&gt;% ggplot(aes(x=date, y=new_cases)) + geom_line(col=&quot;gray50&quot;) + geom_smooth(col=&quot;red&quot;, size=1) + theme_bw() + labs(x=&quot;Date&quot;, y=&quot;New COVID-19 Cases&quot;) You Try It! Now, you do the same thing with the deaths variable. What if we wanted to plot the three worst counties in Colorado? Here we would first have to define “worst”. Let’s say for our purposes that we use the total number of cases on the last recorded day. We could then figure out the cases on the last day with the last() operator. We could also figure use the top_n() function to find the top worst_cases &lt;- covid_last %&gt;% ungroup %&gt;% arrange(desc(cases)) %&gt;% slice_head(n=5) covid %&gt;% filter(county %in% worst_cases$county) %&gt;% ggplot(aes(x=date, y=cases, colour=county)) + geom_line() + theme_bw() + theme(legend.position=&quot;top&quot;) + labs(x=&quot;Date&quot;, y=&quot;COVID-19 Cases&quot;, colour=&quot;&quot;) It looks like things are pretty stable until say the middle of 2020 or so, why don’t we start then. The xlim() function allows us to do this. library(lubridate) covid %&gt;% filter(county %in% worst_cases$county) %&gt;% ggplot(aes(x=date, y=cases, colour=county)) + geom_line() + theme_bw() + xlim(ymd(&quot;2020-07-01&quot;), max(covid$date)) + theme(legend.position=&quot;top&quot;) + labs(x=&quot;Date&quot;, y=&quot;COVID-19 Cases&quot;, colour=&quot;&quot;) Or, we could do it with the scale_x_continuous() function, can be used to impose exact limits on the plotting region by setting expand=c(0,0) as below. covid %&gt;% filter(county %in% worst_cases$county) %&gt;% ggplot(aes(x=date, y=cases, colour=county)) + geom_line() + theme_bw() + theme(legend.position=&quot;top&quot;) + scale_x_date(limits=c(ymd(&quot;2020-03-15&quot;), max(covid$date)), expand=c(0,0)) + labs(x=&quot;Date&quot;, y=&quot;COVID-19 Cases&quot;, colour=&quot;&quot;) covid %&gt;% filter(county %in% worst_cases$county) %&gt;% ggplot(aes(x=date, y=cases, colour=county)) + geom_line(show.legend = FALSE) + theme_bw() + scale_y_continuous( sec.axis = sec_axis(~., breaks=worst_cases$cases, labels = c(&quot;Denver&quot;, &quot;Arapahoe&quot;, &quot;Adams&quot;, &quot;El Paso&quot;, &quot;Jefferson&quot;))) + theme(legend.position=&quot;top&quot;) + scale_x_date(limits=c(ymd(&quot;2020-03-15&quot;), max(covid$date)), expand=c(0,0)) + labs(x=&quot;Date&quot;, y=&quot;COVID-19 Cases&quot;, colour=&quot;&quot;) You Try It! You make the same kind of graph, but get the 4 worst counties with respect to the deaths variable. 1.2.2.1 Controlling Colour We can change the colours of lines with the scale_colour_manual() function. The first argument is values which has to be a vector of colours. There is also an optional labels argument that would allow you to override the default labels attached to the colours. If you’re looking for resources for choosing colours, here are a couple that might be useful Colorbrewer is a website that is specifically designed for mapping colors, but the palettes are general enough to be used for anything. All of the palettes are implemented in the RColorBrewer package. Paletton is not a fancy exercise bike, but a website that helps you pick colour palettes. If you find one you like, there is a “Tables/Export” button right below the colour swatches. If you click that, it will tell you the hex codes for the colours that you can use in R. Looking at the ColorBrewer website, we let’s say we wanted to use the “Set-2” palette. We could do that as follows: library(RColorBrewer) covid %&gt;% filter(county %in% worst_cases$county) %&gt;% ggplot(aes(x=date, y=cases, colour=county)) + geom_line(show.legend = FALSE) + theme_bw() + scale_y_continuous( sec.axis = sec_axis(~., breaks=worst_cases$cases, labels = c(&quot;Denver&quot;, &quot;Arapahoe&quot;, &quot;Adams&quot;, &quot;El Paso&quot;, &quot;Jefferson&quot;))) + scale_colour_manual(values=brewer.pal(5, &quot;Set2&quot;)) + theme(legend.position=&quot;top&quot;) + scale_x_date(limits=c(ymd(&quot;2020-03-15&quot;),max(covid$date)), expand=c(0,0)) + labs(x=&quot;Date&quot;, y=&quot;COVID-19 Cases&quot;, colour=&quot;&quot;) You Try It! Implement a new colour scheme in your plot of the worst places in Colorado with respect to deaths. We could move back and plot all of the lines on the same graph. Doing this, we probably don’t want to colour them differently. We would rather have them look all the same to identify the overall patterns. We could also put in a line for the mean with the stat_summary() function and we could even change the scale of \\(y\\) to be log10 instead of in the level. Note that the group aesthetic is defined for the line geometry, but not for the whole plot. To define it for the whole would calculate the mean of cases by day within each county. Putting the group aesthetic in the line geometry means that it only affects the drawing of these lines and not the line drawn by stat_summary(). ggplot(covid, aes(x=date, y=cases)) + geom_line(aes(group=county), col=&quot;gray65&quot;, size=.5) + stat_summary(fun=mean, col=&quot;red&quot;, geom=&quot;line&quot;) + theme_bw() + scale_x_date(limits=c(ymd(&quot;2020-03-15&quot;), max(covid$date)), expand=c(.01,.01)) + scale_y_log10() + labs(x=&quot;Date&quot;, y=&quot;COVID-19 Cases&quot;, colour=&quot;&quot;) You Try It! Now, you do the same as above, but for deaths. 1.2.3 Histograms We can move to talk about histograms. We could look at the distribution of republican vote (repvote) in Colorado. ggplot(colo_dat, aes(x=repvote)) + geom_histogram() + theme_bw() + labs(x=&quot;COVID-19 Cases&quot;) Note, that there are lots of bins, probably too many. We could instead use fewer, say 7. ggplot(colo_dat, aes(x=repvote)) + geom_histogram(bins=7) + theme_bw() + labs(x=&quot;COVID-19 Cases&quot;) You Try It! Find a variable or two in the data and make a histogram of their distributions. Note that the bins=# argument to the histogram geometry will produce a histogram with approximately # bins. To make things a bit more interesting, let’s make a new variable that codes whether a majority of the two-party vote went to the republican or note. There is a recode() function in the dplyr package, but it’s not actually the best one. I have always like the one in the car package better. 1.2.3.1 Package masking This brings up an important point to think about. When we load two packages that have functions of the same name in them, the more recent one loaded masks the more distant one loaded. In this case, we have been using the dplyr functions quite extensively and we just want to use one function from the car package. Rather than using library(car) and then specifying the recode function and hoping that car was loaded after dplyr, we can be explicit about where we want R to find the function. We can cal car::recode() instead. This tells R to get the recode() function from the car package. The two colons :: mean get a function that is exported from the namespace. Let’s talk about these two terms for a second. When people build packages for R, it creates a namespace that contains all of the functions that exist in the package. This is, for example, so functios inside the car package can easily access other functions inside the car package. Some, and sometimes all, of the functions that people write in a package are exported - meaning that they are intended to be called directly by end users. Some functions, however, may not be exported - specifically those that are intended to be called by other functions within the package, but not by the end users directly. The two colons can retrieve functions from that namespace that are exported. Three colons ::: can retrieve any function from the namespace, whether or not it was exported by the end user. These can be useful if you want to make sure you’re always using the right version of a function that you know often gets masked. As more and more packages are written for R, this is more and more likely to happen. There are a few that happen regularly for things that I do. The aforementioned recode() function is one (both in car and dplyr), but so is the select() function, which is in both the MASS package and dplyr. 1.2.3.2 Recoding Variables We will recode the values of repvote such that values less than or equal to 50 are coded as “Democratic Majority” and values greater than 50 is coded as “Republican Majority”. If we had any values that were exactly \\(50%\\), we might want to include a category or ties, but we don’t have that here. The first argument to the recode() function is a vector of values to be recoded. The second is a set of recode instructions with the following properties: The entire set of recode instructions must be inside a single set of quotations (either single quotes or double quotes). Each individual recode instruction is separated from the next by a semicolon (;). The recode instructions take the form of old-values = new-value, where old-values can be a scalar (e.g., 2=1), vector (e.g., c(1,2,3)=0) or range (e.g., 1:4 =0). The range operator (:) works differently in the recode function than it does in the rest of R. In the rest of R, it creates sequences of integer values between the number on the left of the colon to the number on the right: 1:5 ## [1] 1 2 3 4 5 However in the recode() function, it means all values between the number on its left and the number on its right, inclusive of the bounds. For ranges, there are a couple of “helper” values: lo and hi fill in for the lowest and highest values in the variables without having to identify the numbers directly. The result can be returned as a factor (if you supply the argument as.factor=TRUE) and if you do that, you can also set the ordering of the levels with the levels argument. Here, the levels have to be spelled exactly as they are in the recode instructions. As discussed in the Data Types in R section, the first value of a factor will serve as the reference category in models. Below, we put all of these pieces together. colo_dat &lt;- colo_dat %&gt;% mutate(maj_party = car::recode(repvote, &quot;lo:.5=&#39;Democratic&#39;; .5:hi=&#39;Republican&#39;&quot;, as.factor=TRUE, levels=c(&quot;Democratic&quot;, &quot;Republican&quot;))) Note that above, .5 is the upper bound of the first recode statement and the lower bound of the second. We don’t actually have any values of exactly 0.5, but if we did, those values would go into the Democratic catetory. It’s the first recode statement involving a number that is operative. Now, we could go back to make our histogram of the log of cases conditioning on majority vote. There are generally two methods of plotting multiple groups - superposition and justaposition. We’ve already seen superposition at work in the graphs above with multiple lines. In superposed graphs, multiple groups are plotted in the same plotting region (we might call it a “panel”). We saw this above when we had different coloured lines where each group is represented as a different colour. The aesthetic elements size, shape, linetype, colour - all represent methods for identifying superposed series. In juxtaposed graphs, each group is plotted in its own panel. Colours are not needed because each panel contains only a single series and the group is identified in the strip above the panel. This is done with either the facet_wrap() or facet_grid() function in R. First, let’s do the juxtaposed plot of mental health. ggplot(colo_dat, aes(x=mental_health)) + geom_histogram(bins=7) + theme_bw() + theme(aspect.ratio=1) + facet_wrap(~maj_party) + labs(x=&quot;Mental Health Score&quot;) If we wanted to superpose the cases, we could do that by specifying the fill aesthetic. In the code below, we set the alpha parameter in the histogram geometry, which allows the colours to be semi-transparent. The alpha parameter ranges from 0 (completely transparent) to 1 (completely opaque). ggplot(colo_dat, aes(x=mental_health, fill=maj_party)) + geom_histogram(bins=7, alpha=.3) + theme_bw() + labs(x=&quot;Mental Health&quot;) The default position of the histograms above is to stack them. That is, for each bin, the democratic values are stacked on top of the republican values. If you would rather have them each plotted starting at zero, you could use position=\"identity\" as an argument to the histogram geometry. ggplot(colo_dat, aes(x=mental_health, fill=maj_party)) + geom_histogram(bins=7, alpha=.3, position=&quot;identity&quot;) + theme_bw() + labs(x=&quot;Mental Health&quot;) You Try It! Make a two-category variable from the mental_health variable such that values less than or equal to 3.5 are in the “low” category and those greater than 3.5 are in the “high” category. Make a histogram of the inactivity variable by the new mental_health categorical variable that you made. 1.2.4 Scatterplots Finally, for now, we’ll investigate scatterplots. We can make a scatterplot between the average weekly wage (wkly_wage) and the proportion of people with a BA or higher (BAplus). ggplot(colo_dat, aes(x=BAplus, y=wkly_wage)) + geom_point() + theme_bw() + labs(x=&quot;% with BA Degree or Higher&quot;, y=&quot;Average Weekly Wage&quot;) In the figure above the plotting symbol is the default filled circle. This can be changed with either pch or shape. The options are: knitr::include_graphics(&quot;images/pch.png&quot;) The filled symbols are fine for small datasets, but open symbols are better for larger datasets as they allow you to more easily visualize overplotting of symbols (i.e., it’s easier to see the density of data). Another option would be to set the alpha parameter to a small value - this would allow you to visualize overplotting as well. You Try It! Now, you make a scatterplot of the weekly wage against a different variable. Bonus points for adding a smooth trend line to the graph. Make the symbols different based on the mental_health categorical variable you made before. If you wanted text instead of points, you could add a text geometry instead of the point geometry. ggplot(colo_dat, aes(x=BAplus, y=wkly_wage)) + geom_text(aes(label=county)) + theme_bw() + labs(x=&quot;% with BA Degree or Higher&quot;, y=&quot;Average Weekly Wage&quot;) If you’re printing this for the web or you want to explore the plot yourself, you could use the plotly package, which makes interactive graphics. There is a ggplotly function that turns ggplots into interactive plotly plots. The function below will buld a plot such that when you hover over each point, it gives the county and the values of the two variables in the scatterplot. library(plotly) g1 &lt;- colo_dat %&gt;% mutate(label = paste(&quot;\\nCounty: &quot;, county, &quot;\\nBA or Greater: &quot;, round(BAplus, 2)*100, &quot;\\nWeekly Wage: &quot;, wkly_wage, sep=&quot;&quot;)) %&gt;% ggplot(aes(x=BAplus, y=wkly_wage, text=label)) + geom_point() + theme_bw() + labs(x=&quot;% with BA Degree or Higher&quot;, y=&quot;Average Weekly Wage&quot;) ggplotly(g1, tooltip = &quot;text&quot;) You Try It! Take one of the visualizations you made above and make it into a plotly plot. 1.2.5 Multiple Series As a wrap-up, let’s see how we would plot both deaths and cases in the same plot. We saw how to do this earlier when we wanted to plot multiple groups for a single variable. The idea here would be the same, we simply need to transform our dataset into one where the groups represent different variables. That is, we need to reshape it from wide to long format. To do this, we can use the pivot_longer() function from the tidyr package. tmp &lt;- covid %&gt;% select(county, date, deaths, cases) %&gt;% mutate(cases = cases/100) %&gt;% group_by(date) %&gt;% summarise(across(c(&quot;cases&quot;, &quot;deaths&quot;), sum)) %&gt;% pivot_longer(cols=c(&quot;cases&quot;, &quot;deaths&quot;), names_to=&quot;var&quot;, values_to=&quot;val&quot;) tmp %&gt;% mutate(var = factor(var, levels=c(&quot;cases&quot;, &quot;deaths&quot;), labels=c(&quot;Cases (Hundreds)&quot;, &quot;Deaths&quot;))) %&gt;% ggplot(aes(x=date, y=val, colour=var)) + geom_line() + theme_bw() + theme(legend.position=&quot;top&quot;) + labs(x=&quot;Date&quot;, y=&quot;Count&quot;, colour=&quot;Series&quot;) 1.2.5.1 With Great Power Comes Great Responsibility The ggplot environment makes it really easy to do lots of cool things. Just remember the important equation \\(\\text{Cool} + \\text{Cool} + \\text{Cool} \\neq \\text{Super Cool}\\). Here’s an example. Before we move on, we’re going to create one more categorical variable, this time out of the urban_rural variable, which is already categorical. However, we want to group all of the areas identified as “Metro” together, the areas identified as “UP” together and the areas identified as “Rural” together. We could do this with a traditional recode statement, but I thought this would be a good place to talk about how R can deal with strings. There is a really useful package called stringr which has tons of functions for string manipulation. The one we’re going to use is called str_extract() which pulls some sub-string out of a set of characters. It’s like a search and recovery mission for parts of strings. Below, we’re going to search for Metro or UP or Rural and then if it finds any of those it will extract just those bits. library(stringr) colo_dat &lt;- colo_dat %&gt;% mutate(mur = str_extract(urban_rural, &quot;Metro|UP|Rural&quot;), mur = factor(mur, levels=c(&quot;Rural&quot;, &quot;UP&quot;, &quot;Metro&quot;))) Below, we make a scatterplot of weekly wage by proportion with a BA or greater we encode colour as republican vote and size as the log of total population. ggplot(colo_dat, aes(x=BAplus, y=wkly_wage, colour=repvote, size=log(tpop))) + geom_point() + geom_smooth(se=TRUE, alpha=.25, show.legend=FALSE) + scale_colour_viridis_c() + facet_wrap(~mur) + theme_bw() + labs(x = &quot;Proportion with BA Degree or Higher&quot;, y=&quot;Weekly Wage)&quot;, colour=&quot;% Republican&quot;, size=&quot;Total Population (log)&quot;) As the figure above clearly demonstrates, we can encode lots of information into geometric elements. The cognitive task of decoding all of that information is quite high. Generally, we’re not great at distinguishing different sizes of things, though with only 4 sizes, it might not be too bad. We’re also not good at making fine distinctions in color hue or saturation. Just make sure that what you’re presenting conveys the point you want to convey in the simplest possible way. 1.3 Recap OK, so back to the main question - what do we learn about COVID-19 in Colorado? Well, given all of the tools that we’ve developed and what we know about COVID-19 infections, I think we could put together a useful visual. I want to show deaths and cases (though in different panels). I want to group by the mur variable. Let’s do it … covid %&gt;% left_join(colo_dat %&gt;% select(county, tpop, mur)) %&gt;% group_by(date, mur) %&gt;% summarise(cases = median(cases/(tpop/100000)), deaths = median(deaths/(tpop/100000))) %&gt;% select(date, mur, cases, deaths) %&gt;% pivot_longer(cols=c(&quot;cases&quot;, &quot;deaths&quot;), names_to=&quot;var&quot;, values_to=&quot;val&quot;) %&gt;% na.omit() %&gt;% ggplot(aes(x=date, y=val, colour=mur)) + geom_line(size=.25) + facet_wrap(~var, scales=&quot;free_y&quot;, ncol=1) + theme_bw() + theme(legend.position=&quot;top&quot;) + scale_colour_viridis_d() + labs(x=&quot;Date&quot;, y=&quot;Count/100k&quot;, colour=&quot;&quot;) I used medians here instead of means because the means seemed to be pulled by some outliers.The insights we gain here are: In terms of cases/capita, all three types of areas seem to be doing quite similarly in general. Cases picked up in metro areas first, then urban areas then rural areas, on average. It looks like the lag is about a week moving from larger to smaller areas. Deaths are better in rural areas than other places and better in non-metro places with substantial urban populations than in metro areas. In terms of the death rate, \\(\\frac{\\text{Deaths}}{\\text{Cases}}\\), things are much worse in metro areas than in the other two types of places. You Try It! I have compiled a similar dataset (with most of the same variables, though not the public health ones beyond the COVID stuff) for Minnesota. Using the same process above, how do the insights from Minnesota differ from those for Colorado. Exercises The Canadian Election Study (CES) offers data on Canadian’s political behaviour and attitudes. Load the data from the 2019 study and remove the NA values of the party identification (pid) variable and the political cynicism (cynicism) variable. The cynicism variable is a Likert scale type variable coded from -1 to 1. Create a factor variable that contains the categorical values (Strongly disagree, Disagree, Neutral, Agree, Strongly agree) using the recode() function from the car package. You might have to round up the values of cynicism beforehand. We are interested in the level of cynicism of Canadians, particularly if there exist a difference across partisans. Create a bar graph that displays the mean level of cynicism from the partisans of each party. For added difficulty, try and do it in one call. As you can see, it is difficult to read a bar graph that has negative and positive values. Transform the cynicism variable such that it has values between 0 and 1, 0.5 being the neutral position. Rerun the graph. Add a horizontal line at the 0.5 mark using geom_hline(). Now, let’s add \\(95\\%\\) confidence intervals around the means. The geom_errorbar() takes the lower and upper bounds of the confidence interval as inputs. Great, we now have all the information we wanted to show our readers. But it’s not the prettiest graph out there. Add titles to the x- and y-axis, change the colour of the bars, and change the theme. However, displaying a legend doesn’t make much sense here, so make sure to add the argument show.legend = F to both geoms in the graph. (Hint: labs(), scale_fill_brewer() and theme() are useful functions here). Maybe the difference isn’t from party lines, but really from differences in age. Add facets in accordance with the agegrp variable. (Hint: you need to work on the computation of the means and confidence intervals before adding facet_wrap(~agegrp) in the graph). The Liberal and Conservative parties in Canada are natural opponents. Hence, we would expect partisans from one party to really dislike the leader of the other party. The variables leader_lib and leader_con are feeling thermometers for each one of the party’s leaders. Create a scatter plot that shows this relationship making it as publishable as possible. "],["describing-relationships-in-the-general-social-survey.html", "Chapter 2 Describing Relationships in the General Social Survey 2.1 The Canadian GSS. 2.2 Looking at the data. 2.3 Descriptive Output in knitr Exercises", " Chapter 2 Describing Relationships in the General Social Survey Sometimes we want numerical rather than visual depictions of relationships and the simplest form those take is descriptive statistics. Below, I discuss some of the tools we can use to summarize and describe (mostly bivariate) relationships in data. 2.1 The Canadian GSS. We are going to use the Canadian General Social Survey (Cycle 30: Canadians at Work and Home). The codebook would be too big to display in the file, but you can download it here. In particular, we are going to look at relationships between self-reported mental health (SRH_115) and some other variables. First, we’ll have to load the data. library(rio) load(&quot;data/gss2016can.rda&quot;) res &lt;- gss2016can[,grep(&quot;^RES&quot;, names(gss2016can))] res &lt;- res[,-9] a &lt;- psych::alpha(scale(res)) gss2016can$resilience &lt;- -a$scores attr(gss2016can$resilience, &quot;label&quot;) &lt;- &quot;Resilience Score made from a scale of RES_X items&quot; 2.2 Looking at the data. After loding the data, we could make a frequency distribution of the variable of interest. There are loads of ways to do this. We’ll use the one in my package DAMisc first. There are lots of variables that have labels, so rather than using the factorize() function on each one, we’ll just do it to the whole dataset. gss2016canf &lt;- factorize(gss2016can) Now, we’ve got two copies of the data - one with factors (gss2016canf) and one with all numeric data (gss2016can). This may come in handy a bit later. library(DAMisc) xt(gss2016canf, &quot;SRH_115&quot;) ## $tab ## $tab[[1]] ## Var1 Freq ## Excellent 24% (4,728) ## Very good 37% (7,319) ## Good 29% (5,746) ## Fair 7% (1,393) ## Poor 2% (336) ## Total 100% (19,522) ## ## ## $chisq ## NULL ## ## $stats ## list() ## ## attr(,&quot;class&quot;) ## [1] &quot;xt&quot; You Try It! We’re going to use the 2016 US General Social Survey for exercises in this section. You can load the data with: load(&quot;data/gss16.rda&quot;) Look at the distribution of two variables we’re going to consider - aidhouse (the government should provide housing to the poor) and partyid (partisan identification). - For now, just look at the univariate distributions. Note that most people estimated their mental health to be “Excellent” or “Very Good”. Just as before, we could make a bar plot of this as well: library(ggplot2) ggplot(gss2016canf, aes(x=SRH_115)) + geom_bar() + theme_bw() + labs(x=&quot;Self-reported Mental Health&quot;, y=&quot;Count&quot;) Note in the plot above that the missing value (NA) is represented. If we don’t want that to be the case, we simply filter them out: library(dplyr) gss2016canf %&gt;% filter(!is.na(SRH_115)) %&gt;% ggplot(aes(x=SRH_115)) + geom_bar() + theme_bw() + labs(x=&quot;Self-reported Mental Health&quot;, y=&quot;Count&quot;) There is a variable called resilience in the data that is a scale of a bunch of questions about personal resilience. They are most of the RES_X questions in the codebook, except for RES_09, which was not interestingly correlated with the other variables. We could look to see how that variable changes as a function of self-reported mental health. One way of doing this would be to summarise resilience by SRH_115. We can do this with the sumStats() function in the DAMisc package: You Try It! Make a bar plot of aidhouse and of partyid, each independently. sumStats(gss2016canf, &quot;resilience&quot;, byvar=&quot;SRH_115&quot;) ## # A tibble: 6 × 12 ## variable SRH_115 mean sd iqr min q25 q50 q75 max n nNA ## &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 resilience Excellent 0.344 0.530 0.778 -3.44 -0.0132 0.387 0.764 1.11 4728 8 ## 2 resilience Very good 0.0735 0.537 0.770 -3.73 -0.278 0.0976 0.492 1.20 7319 9 ## 3 resilience Good -0.189 0.614 0.792 -3.73 -0.558 -0.158 0.234 1.12 5746 10 ## 4 resilience Fair -0.535 0.674 0.940 -3.35 -0.977 -0.543 -0.0371 1.03 1393 2 ## 5 resilience Poor -1.03 0.869 1.03 -3.73 -1.55 -1.01 -0.519 1.03 336 2 ## 6 resilience &lt;NA&gt; -0.410 0.984 1.03 -3.00 -0.883 -0.152 0.148 1.03 87 56 This gives many of the “usual suspects” in terms of summary statistics. If you wanted to make the same sort of summary, but visually (say means and \\(95\\%\\) confidence intervals), you could do that with ggplot(): gss2016canf %&gt;% filter(!is.na(SRH_115)) %&gt;% ggplot(aes(x=SRH_115, y=resilience)) + stat_summary(geom=&quot;errorbar&quot;, fun.data=mean_cl_normal, width=0, conf.int=0.95) + stat_summary(geom=&quot;point&quot;, fun=mean, size=.75) + theme_bw() + labs(x=&quot;Self-reported Mental Health&quot;, y=&quot;Resilience&quot;) The stat_summary() function calculates the confidence bounds and means and returns them to the appropriate aesthetics (y.min and y.max, in this case). In this case, because none of the confidence intervals overlap that resilience is statistically different across all categories. You Try It! Make a similar graph for realinc as a function of aidhouse from the US GSS data. 2.2.1 Cross-tabulations One of the main tools we use for descriptive analysis is the cross-tabulation (aka joint frequency distribution, contingency table). There are loads of ways to do these in R. Which one you use really depends on what exactly you want to see and the flexibility in the output format. The CrossTable() function in the gmodels package provides lots of information and is quite customizable in what it presents. Here’s an example of self-reported mental health (SRH_115) on self-reported physical health (SRH_110). library(gmodels) CrossTable(x = gss2016canf$SRH_115, gss2016canf$SRH_110) ## ## ## Cell Contents ## |-------------------------| ## | N | ## | Chi-square contribution | ## | N / Row Total | ## | N / Col Total | ## | N / Table Total | ## |-------------------------| ## ## ## Total Observations in Table: 19506 ## ## ## | gss2016canf$SRH_110 ## gss2016canf$SRH_115 | Excellent | Very good | Good | Fair | Poor | Row Total | ## --------------------|-----------|-----------|-----------|-----------|-----------|-----------| ## Excellent | 1845 | 1588 | 942 | 277 | 73 | 4725 | ## | 2161.995 | 7.879 | 310.573 | 140.310 | 39.298 | | ## | 0.390 | 0.336 | 0.199 | 0.059 | 0.015 | 0.242 | ## | 0.682 | 0.226 | 0.137 | 0.121 | 0.118 | | ## | 0.095 | 0.081 | 0.048 | 0.014 | 0.004 | | ## --------------------|-----------|-----------|-----------|-----------|-----------|-----------| ## Very good | 593 | 4187 | 1951 | 481 | 101 | 7313 | ## | 174.634 | 910.892 | 148.777 | 167.828 | 73.722 | | ## | 0.081 | 0.573 | 0.267 | 0.066 | 0.014 | 0.375 | ## | 0.219 | 0.595 | 0.285 | 0.209 | 0.163 | | ## | 0.030 | 0.215 | 0.100 | 0.025 | 0.005 | | ## --------------------|-----------|-----------|-----------|-----------|-----------|-----------| ## Good | 217 | 1019 | 3439 | 856 | 210 | 5741 | ## | 421.009 | 533.809 | 1002.572 | 47.897 | 4.344 | | ## | 0.038 | 0.177 | 0.599 | 0.149 | 0.037 | 0.294 | ## | 0.080 | 0.145 | 0.502 | 0.373 | 0.340 | | ## | 0.011 | 0.052 | 0.176 | 0.044 | 0.011 | | ## --------------------|-----------|-----------|-----------|-----------|-----------|-----------| ## Fair | 38 | 213 | 442 | 572 | 126 | 1391 | ## | 124.315 | 166.052 | 4.462 | 1017.235 | 152.312 | | ## | 0.027 | 0.153 | 0.318 | 0.411 | 0.091 | 0.071 | ## | 0.014 | 0.030 | 0.064 | 0.249 | 0.204 | | ## | 0.002 | 0.011 | 0.023 | 0.029 | 0.006 | | ## --------------------|-----------|-----------|-----------|-----------|-----------|-----------| ## Poor | 11 | 27 | 79 | 111 | 108 | 336 | ## | 27.175 | 73.181 | 12.915 | 128.964 | 890.336 | | ## | 0.033 | 0.080 | 0.235 | 0.330 | 0.321 | 0.017 | ## | 0.004 | 0.004 | 0.012 | 0.048 | 0.175 | | ## | 0.001 | 0.001 | 0.004 | 0.006 | 0.006 | | ## --------------------|-----------|-----------|-----------|-----------|-----------|-----------| ## Column Total | 2704 | 7034 | 6853 | 2297 | 618 | 19506 | ## | 0.139 | 0.361 | 0.351 | 0.118 | 0.032 | | ## --------------------|-----------|-----------|-----------|-----------|-----------|-----------| ## ## Note that some of these things we may not want to see. We could turn off the row proportions prop.r=FALSE and the cell proportions prop.t=FALSE and we could turn off the \\(\\chi^2\\) contribution prop.chisq=FALSE. We could also turn on the \\(\\chi^2\\) statistic with CrossTable(x = gss2016canf$SRH_115, gss2016canf$SRH_110, prop.r=FALSE, prop.t=FALSE, prop.chisq=FALSE, chisq = TRUE) ## ## ## Cell Contents ## |-------------------------| ## | N | ## | N / Col Total | ## |-------------------------| ## ## ## Total Observations in Table: 19506 ## ## ## | gss2016canf$SRH_110 ## gss2016canf$SRH_115 | Excellent | Very good | Good | Fair | Poor | Row Total | ## --------------------|-----------|-----------|-----------|-----------|-----------|-----------| ## Excellent | 1845 | 1588 | 942 | 277 | 73 | 4725 | ## | 0.682 | 0.226 | 0.137 | 0.121 | 0.118 | | ## --------------------|-----------|-----------|-----------|-----------|-----------|-----------| ## Very good | 593 | 4187 | 1951 | 481 | 101 | 7313 | ## | 0.219 | 0.595 | 0.285 | 0.209 | 0.163 | | ## --------------------|-----------|-----------|-----------|-----------|-----------|-----------| ## Good | 217 | 1019 | 3439 | 856 | 210 | 5741 | ## | 0.080 | 0.145 | 0.502 | 0.373 | 0.340 | | ## --------------------|-----------|-----------|-----------|-----------|-----------|-----------| ## Fair | 38 | 213 | 442 | 572 | 126 | 1391 | ## | 0.014 | 0.030 | 0.064 | 0.249 | 0.204 | | ## --------------------|-----------|-----------|-----------|-----------|-----------|-----------| ## Poor | 11 | 27 | 79 | 111 | 108 | 336 | ## | 0.004 | 0.004 | 0.012 | 0.048 | 0.175 | | ## --------------------|-----------|-----------|-----------|-----------|-----------|-----------| ## Column Total | 2704 | 7034 | 6853 | 2297 | 618 | 19506 | ## | 0.139 | 0.361 | 0.351 | 0.118 | 0.032 | | ## --------------------|-----------|-----------|-----------|-----------|-----------|-----------| ## ## ## Statistics for All Table Factors ## ## ## Pearson&#39;s Chi-squared test ## ------------------------------------------------------------ ## Chi^2 = 8742.488 d.f. = 16 p = 0 ## ## ## The one problem with CrossTable() is the that output isn’t amenable to being exported. It only shows up as it is on the screen. There isn’t a single table that you could export to a format that would be good for publication. You could also use the xt() function from the DAMisc package. Note that this function also returns several measures of association - Cramér’s V, Kruskal-Goodman \\(\\gamma\\) and Kendall’s \\(\\tau_b\\). The function uses a permutation test to generate \\(p\\)-values for the statistics. xt(gss2016canf, &quot;SRH_115&quot;, &quot;SRH_110&quot;) ## $tab ## $tab[[1]] ## SRH_115/SRH_110 Excellent Very good Good Fair Poor Total ## Excellent 68% (1,845) 23% (1,588) 14% (942) 12% (277) 12% (73) 24% (4,725) ## Very good 22% (593) 60% (4,187) 28% (1,951) 21% (481) 16% (101) 37% (7,313) ## Good 8% (217) 14% (1,019) 50% (3,439) 37% (856) 34% (210) 29% (5,741) ## Fair 1% (38) 3% (213) 6% (442) 25% (572) 20% (126) 7% (1,391) ## Poor 0% (11) 0% (27) 1% (79) 5% (111) 17% (108) 2% (336) ## Total 100% (2,704) 100% (7,034) 100% (6,853) 100% (2,297) 100% (618) 100% (19,506) ## ## ## $chisq ## $chisq[[1]] ## ## Pearson&#39;s Chi-squared test ## ## data: tmptab ## X-squared = 8742.5, df = 16, p-value &lt; 2.2e-16 ## ## ## ## $stats ## $stats[[1]] ## statistic p-value ## Chi-squared 8742.4880786 0 ## Cramers V 0.3347368 0 ## Lambda 0.2644140 0 ## ## ## attr(,&quot;class&quot;) ## [1] &quot;xt&quot; You Try It! Make a cross-tabulation of aidhouse and partyid. - What is the relationship like? 2.2.2 Permutation Tests in R. Permutation tests are great when you don’t know the sampling distribution of a test statistic or you want to calculate a \\(p\\)-value when the distributional assumptions required to derive the sampling distribution are dubious. This will allow us the opportunity to talk about loops a bit. The main idea is that we want to impose the null hypothesis relationship and then see how the statistic we’re calculating varies in repeated sampling. In all of these statistics, the null hypothesis is that there is no relationship. So, how do we impose a condition of no relationship? Well, one way would be to take one of the variables in the relationship and randomly re-arrange the rows. The random re-arrangement induces no relationship (though the statistic will not be exactly zero). We could do this random re-arrangement many times to build a sampling distribution under the null hypothesis. Then, we could see how the statistic we calculated compares to the distribution we just calculated. First, we can figure out how to calculate the statistic once. tmp &lt;- gss2016canf %&gt;% select(SRH_115, SRH_110) tmp$SRH_110 &lt;- sample(tmp$SRH_110, nrow(tmp), replace=FALSE) tab &lt;- with(tmp, table(SRH_115, SRH_110)) stat &lt;- tau.b(tab) stat ## [1] 0.001399321 Now we can do it lots of times. We’ll use 1000, but if you were doing this “in real life”, you would probably want 2000 or 2500 iterations to build the distribution. The general advice on the number of samples is that you need more the further out in the tail of the distribution the quantity is you are trying to estimate. If we wanted to estimate the \\(95^{th}\\) percentile, we would need fewer than for the \\(99.9^{th}\\) percentile. To do this, we can use a for() loop. Note that the first argument to for() is the name of the counter then we use in and identify the range of the index. Here we go from 1 to 1000 by ones. In each iteration, we perform the calculation in the loop. Before we run the loop, we initialize a value we call stat that will hold the results. There are two schools of thought here. The first is that we could initialize it with NULL and it will grow bigger with every result. This works well if the number of results is relatively small and the results themselves are small (e.g., scalars instead of lists). If you have tons of results or they are big, it is considered better practice to initialize the appropriate size object and fill in an element with each iteration. Finally, we’re setting the random number generating seed to make sure that we can reproduce the same result next time. set.seed(432143) stat &lt;- NULL for(i in 1:1000){ tmp$SRH_110 &lt;- sample(tmp$SRH_110, nrow(tmp), replace=FALSE) tab &lt;- with(tmp, table(SRH_115, SRH_110)) stat &lt;- c(stat, tau.b(tab)) } If we wanted to initialize the appropriate size object, we could do as follows: set.seed(432143) stat &lt;- vector(mode=&quot;numeric&quot;, length=1000) for(i in 1:1000){ tmp$SRH_110 &lt;- sample(tmp$SRH_110, nrow(tmp), replace=FALSE) tab &lt;- with(tmp, table(SRH_115, SRH_110)) stat[i] &lt;- tau.b(tab) } We could then look at the histogram of the results: ggplot(mapping=aes(x=stat)) + geom_histogram() + theme_bw() + labs(x=&quot;Tau-b Statistic Null Distribution&quot;) We could then calculate a \\(p\\)-value by figuring out what proportion of the null distribution is to the right of our test statistic. tab &lt;- with(gss2016canf, table(SRH_115, SRH_110)) tb &lt;- tau.b(tab) ## Calculate the p-value mean(stat &gt; tb) ## [1] 0 2.2.3 Back to the Data If we wanted to make a plot of the cross-tabulation, we could do that with the geom_tile() function. gss2016canf %&gt;% group_by(SRH_115, SRH_110) %&gt;% summarise(n=n()) %&gt;% na.omit %&gt;% ggplot(aes(y=SRH_115, x=SRH_110, fill=n)) + geom_tile() + scale_fill_viridis_c() + theme_bw() + labs(x=&quot;Self-reported Physical Health&quot;, y=&quot;Self-reported Mental Health&quot;, fill=&quot;Count&quot;) + ggtitle(&quot;Joint Frequency Distribution&quot;) Or, we could plot the standardized residuals from the cross-tabulation. This can be done with the chisq.test() function executed on the cross-tabulation generated by the table() function. tab &lt;- with(gss2016canf, table(SRH_115, SRH_110)) x &lt;- chisq.test(tab) df &lt;- as.data.frame(x$stdres) # Make all non-significant values set to 0 df$Freq[which(df$Freq &lt; 2 &amp; df$Freq &gt; -2)] &lt;- 0 ggplot(df, aes(x=SRH_110, y=SRH_115, fill=Freq)) + geom_tile() + scale_fill_viridis_c() + theme_bw() + labs(x=&quot;Self-reported Physical Health&quot;, y=&quot;Self-reported Mental Health&quot;, fill=&quot;Count&quot;) + ggtitle(&quot;Standardized Residuals from Chi-Squared Test&quot;) Another visualization we can make of a cross-tabulation is a mosaic plot. In a mosaic plot, the colum widths are proportional to the number of observations in the column variable and the heights of the bars are proportional to the column proportions in the row variable. Here’s an example using the ggmosaic() function in the ggmosaic package. First, we need to install the ggmosaic package from GitHub using the remotes package (which you may need to install first, see the Using R section for a reminder of how installing packages works). remotes::install_github(&quot;haleyjeppson/ggmosaic&quot;) library(ggmosaic) gss2016canf %&gt;% select(SRH_110, SRH_115) %&gt;% na.omit() %&gt;% ggplot() + geom_mosaic(aes(x=product(SRH_110), fill=SRH_115)) + labs(x=&quot;Self-reported Physical Health&quot;, y=&quot;Self-reported Mental Health&quot;, fill = &quot;Mental Health&quot;) You Try It! Use the three alternative visualizations from above with aidhouse and partyid - heat map, standardized residuals and mosaic plot. - Which one do you think is most useful? 2.3 Descriptive Output in knitr As we think about using descriptive statistics, we should also think about how to best use them in a reproducible research framework, like knitr. So, let’s talk through the few different methods we talked about above. First, let’s talk about summary statistics. The main reason we might want to output a bunch of summary statistics is for an appendix where we give the summary statistics of all relevant variables in a paper. There are a few packages that can help with this. The summarytools package has the dfSummary() function that can be used as follows: library(summarytools) tmp &lt;- gss2016canf %&gt;% select(SRH_110, SRH_115, resilience) dfSummary(tmp, graph.col=FALSE, valid.col=FALSE, varnumbers=FALSE) Data Frame Summary tmp Dimensions: 19609 x 3 Duplicates: 6898 Variable Label Stats / Values Freqs (% of Valid) Missing SRH_110 [factor] Self rated health in general 1. Excellent 2. Very good 3. Good 4. Fair 5. Poor 2708 (13.9%) 7043 (36.0%) 6866 (35.1%) 2302 (11.8%) 619 ( 3.2%) 71 (0.4%) SRH_115 [factor] Self rated mental health in general 1. Excellent 2. Very good 3. Good 4. Fair 5. Poor 4728 (24.2%) 7319 (37.5%) 5746 (29.4%) 1393 ( 7.1%) 336 ( 1.7%) 87 (0.4%) resilience [numeric] Resilience Score made from a scale of RES_X items Mean (sd) : 0 (0.6) min &lt; med &lt; max: -3.7 &lt; 0 &lt; 1.2 IQR (CV) : 0.9 (-557.9) 7892 distinct values 87 (0.4%) This is fine, but there are other options that are available, too. We could use the sumStats() function from DAMisc for the numeric variables: s &lt;- sumStats(gss2016canf, c(&quot;SRH_110&quot;, &quot;SRH_115&quot;, &quot;resilience&quot;), convertFactors = TRUE) s ## # A tibble: 3 × 11 ## variable mean sd iqr min q25 q50 q75 max n nNA ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 SRH_110 2.54 0.975 1 1 2 3 3 5 19609 71 ## 2 SRH_115 2.25 0.957 1 1 2 2 3 5 19609 87 ## 3 resilience -0.00115 0.643 0.895 -3.73 -0.401 -0.000398 0.494 1.20 19609 87 The function above turns all of the factors into numbers for the numeric summary. You could then use kable() from the knitr package to output the table to markdown or xtable() from the package of the same name to send it to LaTeX. knitr::kable(s[,-1]) mean sd iqr min q25 q50 q75 max n nNA 2.5435050 0.9752404 1.000000 1.000000 2.0000000 3.0000000 3.0000000 5.000000 19609 71 2.2464911 0.9571498 1.000000 1.000000 2.0000000 2.0000000 3.0000000 5.000000 19609 87 -0.0011526 0.6430187 0.895259 -3.726695 -0.4008406 -0.0003976 0.4944184 1.200536 19609 87 You Try It! Start a new RMarkdown document and put in your exercise answers. Have the output look like you would want it to look if you were showing it to someone else. Exercises Load the GSS data for the USA in 2016. From it, select the advfront (the federal government should support scientific research) and astrosci (is astrology scientific or not) variables, and drop the NA values. Knowing that advfront is coded 1 for Strongly agree positions and 4 for Strongly disagree, and that astrosci is coded 1 for Very scientific, 2 for Sort of scientific and 3 for Not at all scientific, recode the variables with their textual values. We’ll call the new variables fedSuppScience and astroIsScience. Make sure that these new variables are factors. Using the xt function from the DAMisc package, print a summary of the distribution of each category of the novel fedSuppScience and astroIsScience variables. Create visualizations for the distributions of the two variables. Make sure to name the axis properly and to use another theme than the basic ggplot one. We might be interested in the relationship between believing that science should be supported by the federal government and thinking astrology is science. Hence, summarize fedSuppScience by astroIsScience using the sumStats function and create a cross-tabulation using the xt function. It doesn’t look like there is a strong relationship between the two variables. Make sure by first visualizing the joint frequency distribution using geom_tile and then visualizing the standardized residuals. Using function kable from knitr, create a latex table of the variables’ cross-table. In section 2.2.2 of the chapter, we showed how to rearrange rows of data frame in order to impose a null hypothesis of no relationship. For this exercise, use the base R sample function and the tau.b function from DAMisc to produce one Kendall’s Tau-b from the resulting distribution of fedSuppScience and astroIsScience. (Hint: Create a new data frame containing only the required variables. This will prevent you from having to rerun everything above in case you make a mistake.) Now, using the code you wrote in exercise 8, simulate 2000 random distribution, storing in a vector the resulting statistic of each one. Use set.seed(122) in order to obtain the same results as the answer key. Present your results in the form of a histogram. Compute the p-value from the resulting distribution of Tau-b statistics. (Hint: you need two things for this, the distribution generated in exercise 9 and the original sample’s cross-table.) "],["linear-models.html", "Chapter 3 Linear Models 3.1 Effects Plots 3.2 Diagnostic Tools: Linearity 3.3 Diagnostic Tools: Heteroskedasticity 3.4 Diagnostics: Outliers and Influential Data 3.5 Diagnostics for Normality 3.6 Interactions Exercises", " Chapter 3 Linear Models As we all know, linear models are the workhorse of statistical modeling. These are ubiquitous across disciplines whether in the OLS regression framework or in a classical ANOVA framework. Here, we’re going to focus on the OLS regression framework to highlight lots of what you can do with linear models in R. We will work with the COVID-19 data from Colorado that we worked with before. colo_dat &lt;- colo_dat %&gt;% mutate(cases_pc = cases*10000/tpop) There are a few things worth noting here. First, as discussed previously, the main argument here is a formula where the outcome variable is on the left-hand side of the tilde (~) and the explanatory variables are on the right-hand side of the tilde. They can be separated by plus signs if an additive model is desired or with asterisks if the effect of the variables is conditional on each other. We will see an example of this below. mod &lt;- lm(log(cases_pc) ~ white_pop, data=colo_dat) summary(mod) ## ## Call: ## lm(formula = log(cases_pc) ~ white_pop, data = colo_dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.61171 -0.15814 -0.01486 0.14214 0.69646 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.9881 0.6995 12.848 &lt; 2e-16 *** ## white_pop -2.0956 0.7596 -2.759 0.00762 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2528 on 62 degrees of freedom ## Multiple R-squared: 0.1093, Adjusted R-squared: 0.09496 ## F-statistic: 7.61 on 1 and 62 DF, p-value: 0.007618 The stargazer package allows us to export model results in a way that is nicely presented in an RMarkdown document, a LaTeX document or to word through html. library(stargazer) stargazer(mod, type=&quot;html&quot;, style=&quot;ajps&quot;, covariate.labels=c( &quot;White Population (%)&quot;, &quot;Intercept&quot;), star.cutoffs=.05, star.char = &quot;`*`&quot;, notes=&quot;`*` p &lt; .05&quot;, notes.append=FALSE) log(cases_pc) White Population (%) -2.096* (0.760) Intercept 8.988* (0.700) N 64 R-squared 0.109 Adj. R-squared 0.095 Residual Std. Error 0.253 (df = 62) F Statistic 7.610* (df = 1; 62) * p &lt; .05 To include in a word document, you could do the following: cat( stargazer(mod, type=&quot;html&quot;, style=&quot;ajps&quot;, covariate.labels=c( &quot;White Population (%)&quot;, &quot;Intercept&quot;), star.cutoffs=.05, star.char = &quot;`*`&quot;, notes=&quot;`*` p &lt; .05&quot;, notes.append=FALSE), file=&quot;table.html&quot;) Then you could choose Insert \\(\\rightarrow\\) File… and browse to the file table.html that you just created. This will bring the table into your Word document nicely formatted. Alternatively, you could write in RMarkdown and knit to a Word document. Now, back to the model. In the above, we know that for every 1 unit change in white_pop (the proportion of the population that is white), that the log of cases per 10,000 people goes down by 2 units. This is a reasonably big change on the log scale, but a one-unit change is also huge. It represents changing from a county with no white people to a county with only white people. In reality, the range of the white_pop variable is: range(colo_dat$white_pop) ## [1] 0.7672597 0.9664671 Given that it varies in a reasonably small range, it might be worth trying to plot out the variable’s effect. There are a couple of different ways to do this. One is with the effects package that produces lattice plots of the effects. The other is with the ggeffects package, which makes it easy to make ggplot2 plots of the effect. Let’s start with the effects package. You Try It! Using the GSS 2016 data that we used in the previous chapter, estimate a regression of aid_scale on age, sei10, the log of realinc, partyid and sex. - Make a nice looking table of the results. 3.1 Effects Plots The Effect() function from the effects package allows us to plot the effect of a variable in a model. For example, library(effects) e &lt;- Effect(&quot;white_pop&quot;, mod, xlevels=list(white_pop = 50)) Note that the summary object has 50 values from the smallest to largest values of white_pop along with their predictions and \\(95\\%\\) confidence intervals. We can make a plot of those values with the plot() function: plot(e) In general, the effect package will unwind any functions or transformations of the independent variables, but not the dependent variable. So, we would have to do that ourselves. Doing this kind of a transformation is a bit difficult without some more intervention, so why don’t we move to the ggeffects package. We’ll make the same plot as above. You Try It! Use the effects package to make a plot of the effect of realinc on aid_scale. library(ggeffects) library(ggplot2) preds &lt;- ggpredict(mod, terms = &quot;white_pop [all]&quot;) ## Model has log-transformed response. Back-transforming predictions to original response ## scale. Standard errors are still on the transformed scale. ggplot(preds, aes(x=x, y=predicted)) + geom_ribbon(aes(ymin=conf.low, ymax=conf.high), alpha=.25) + geom_line() + theme_bw() + labs(x=&quot;Proportion of the Population that is White&quot;, y=&quot;Predicted COVID-19 Cases/10k&quot;) The terms= argument identifies the terms to move around. Everything else will be held constant at representative values. In particular white_pop [all] means move the white_pop variable and use all of its unique values to generate predictions. The output from ggpredict always names the first variable in terms= - x, the predictions predicted and the confidence bounds conf.low and conf.high. You may also note that a warning got printed indicating that the model had a log-transformed response and that the predictions and confidence intervals were back-transformed to the original scale of the \\(y\\)-variable (i.e., cases/10k rather than log(cases/10k)). The standard errors remain on the scale defined by the model rather than the original response scale. The confidence intervals are transformed using an end-point transformation which means that they are arrived at, in this case, on the log scale and then simply transformed by applying exp() to the bounds and fit. You Try It! Use the ggeffects package to replicate the graph you just made with the effects package. To see how it works, let’s add a categorical variable - the three category metro, urban, rural variable should work. We’ll also add in the republican majority variable, too. library(stringr) colo_dat &lt;- colo_dat %&gt;% mutate(mur = str_extract(urban_rural, &quot;Metro|UP|Rural&quot;), mur = factor(mur, levels=c(&quot;Rural&quot;, &quot;UP&quot;, &quot;Metro&quot;)), rep_maj = factor(repvote &gt; .5, levels=c(FALSE,TRUE), labels=c(&quot;No&quot;, &quot;Yes&quot;))) Now, we can estimate the model: mod2 &lt;- lm(log(cases_pc) ~ mur + rep_maj + white_pop, data=colo_dat) summary(mod2) ## ## Call: ## lm(formula = log(cases_pc) ~ mur + rep_maj + white_pop, data = colo_dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.58724 -0.16880 -0.02825 0.13434 0.67306 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9.31144 0.74099 12.566 &lt; 2e-16 *** ## murUP 0.09553 0.07365 1.297 0.19965 ## murMetro -0.08477 0.08809 -0.962 0.33984 ## rep_majYes 0.01376 0.06741 0.204 0.83892 ## white_pop -2.47620 0.79404 -3.118 0.00281 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2476 on 59 degrees of freedom ## Multiple R-squared: 0.187, Adjusted R-squared: 0.1319 ## F-statistic: 3.393 on 4 and 59 DF, p-value: 0.0145 Here, we see a slightly bigger effect of white_pop. If we wanted to test the significance of the term rather than the coefficient (i.e., all of the mur coefficients jointly equal to zero rather than independent tests), we could use that Anova() function from the car package. library(car) Anova(mod2) ## Anova Table (Type II tests) ## ## Response: log(cases_pc) ## Sum Sq Df F value Pr(&gt;F) ## mur 0.3273 2 2.6696 0.07764 . ## rep_maj 0.0026 1 0.0417 0.83892 ## white_pop 0.5962 1 9.7249 0.00281 ** ## Residuals 3.6171 59 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 By default, you get a type II test, though you could get a type III test, by specifying type=\"III\". To remind, the difference comes from how higher order terms are handled. Let’s imagine we’ve got the following model: \\[y = b_0 + b_1x + b_2z + b_3xz + b_4w + e\\] In this case, we have a higher order term (the interaction between \\(x\\) and \\(z\\)). A type II test would execute the following four tests: Is \\(x\\) significant controlling for \\(z\\) and \\(w\\), but not \\(xz\\). Is \\(z\\) significant controlling for \\(x\\) and \\(w\\), but not \\(xz\\). Is \\(xz\\) significant controlling for \\(x\\), \\(z\\) and \\(w\\). Is \\(w\\) significant controlling for \\(x\\), \\(z\\) and \\(xz\\). A type III test would test each term controlling for all other terms, including higher order terms. This is more like what you would get from the regression output. Arguably, type II tests are more interesting because they don’t presume the existence of the higher order term in testing lower-order terms, thus making them appropriate for evaluating the significance of lower order terms independently of the higher order term. 3.2 Diagnostic Tools: Linearity There are lots of diagnostic tools for the linear model. Perhaps one of the most useful is the component + residual plot. This can be produced with the crPlot() function in the car package. If we’ve got the following model: \\[\\log(\\text{Cases}) = b_0 + b_1\\text{Urban} + b_2\\text{Metro} + b_3\\text{Republican Majority} + b_4\\text{White} + e\\] then the component plus residual plot for white_pop would have the variable itself on the \\(x\\)-axis and on the \\(y\\)-axis would be \\(b_4\\text{White} + e\\) the component (the systematic part of the model relating to the variable of interest) plus the model residual. The line running through plot has a slope of \\(b_4\\). The pink line is a local polynomial regression fit to the points. This allows us to evaluate whether there are any un-modeled non-linearities in the data. crPlot(mod, &quot;white_pop&quot;, smooth = list(smoother=loessLine, smoother.args=list(var=TRUE))) We could try a couple of different potential solutions to the non-linearity exhibited in the plot. If we thought the non-linearity was simple and monotone, we could use a non-linear transformation. The Box-Tidwell transformation would be appropriate here. This model identifies the optimal power transformation to linearize the relationship. boxTidwell(log(cases_pc) ~ white_pop, ~mur + rep_maj, data=colo_dat) ## MLE of lambda Score Statistic (t) Pr(&gt;|t|) ## 3.1542 -0.3487 0.7286 ## ## iterations = 15 The proposed power transformation is \\(x^{3.15}\\). We could see what it looks like if we wanted. ggplot(colo_dat, aes(x=white_pop, y=I(white_pop^3.15))) + geom_point() + theme_bw() The transformation is actually not very severe at all. Further, the \\(p\\)-value indicates that the transformation isn’t necessary. Here, the null hypothesis is that no transformation is needed. If, instead, we thought that the non-monotonicity in the CR Plot was interesting, we could estimate a polynomial. In R, polynomials are estimated with the poly() function and by default are orthogonalized. This essentially implements a type II test for polynomial regressors. You can turn off the orthogonalization by specifying raw=TRUE as an argument to the function. This would implement a type III test for the polynomial regressors. For example the code below would estimate a third-degree polynomial in white_pop: You Try It! Use the methods discussed above to evaluate linearity for age, sei10 and realinc. For realinc see whether the log is just as good as other transformations. mod2b &lt;- lm(log(cases_pc) ~ mur + rep_maj + poly(white_pop, 3), data=colo_dat) summary(mod2b) ## ## Call: ## lm(formula = log(cases_pc) ~ mur + rep_maj + poly(white_pop, ## 3), data = colo_dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.57772 -0.16630 -0.02752 0.12212 0.60664 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.026087 0.076183 92.227 &lt; 2e-16 *** ## murUP 0.104443 0.075243 1.388 0.17052 ## murMetro -0.059484 0.090855 -0.655 0.51529 ## rep_majYes 0.009006 0.067481 0.133 0.89430 ## poly(white_pop, 3)1 -0.794998 0.264879 -3.001 0.00398 ** ## poly(white_pop, 3)2 -0.072051 0.251579 -0.286 0.77561 ## poly(white_pop, 3)3 0.364747 0.254274 1.434 0.15690 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2473 on 57 degrees of freedom ## Multiple R-squared: 0.2167, Adjusted R-squared: 0.1343 ## F-statistic: 2.629 on 6 and 57 DF, p-value: 0.02551 As you can see, the second and third order terms are not statistically significant, indicating that the linear relationship is sufficient. These findings make sense if we include the confidence bounds around the loess curve in the CR plot. There is a bug in the crPlot() function that doesn’t allow us to do this, but we could make it “by hand” with ggplot2. In the code below, the augment() function comes from the broom package and it puts model results (like fitted values and residuals) in a data frame with the original variables in the model. In addition, we’re obtaining the partial residuals for the white_pop variable and adding those into the data. library(broom) aug &lt;- augment(mod2, data=colo_dat) %&gt;% mutate(p.resid = residuals(mod2, type=&quot;partial&quot;)[,&quot;white_pop&quot;]) ggplot(aug, aes(x=white_pop, y=p.resid)) + geom_point(shape=1) + geom_smooth(method=&quot;lm&quot;, se=FALSE)+ geom_smooth(method=&quot;loess&quot;, se=TRUE, col=&quot;red&quot;, fill=rgb(1,0,0,.25, maxColorValue = 1)) + theme_bw() + labs(x=&quot;Proportion Over 60&quot;, y=&quot;Component + Residual&quot;) In the plot above, you can see that the linear model line is generally within the confidence bounds of the loess curve, making it unsurprising that potential fixes to the non-linearity problem were not necessary. 3.3 Diagnostic Tools: Heteroskedasticity There are tools for detecting heteroskedasticity. The car package has a function called ncvTest() which estimates a score test of the residuals. It starts by calculating the standardized squared residuals \\[U_{i} = \\frac{E_{i}^{2}}{\\hat{\\sigma}^{2}} = \\frac{E_{i}^{2}}{\\frac{\\sum E_{i}^{2}}{n}}\\] Then, it regress the \\(U_{i}\\) on all of the explanatory variable \\(X\\)’s or the fitted values \\(\\hat{y}\\), finding the fitted values: \\[U_{i} = \\eta_{0} + \\eta_{1}X_{i1} + \\cdots + \\eta_{p}X_{ip} + \\omega_{i}\\] The score is then calculated as: \\[S_{0}^{2} = \\frac{\\sum(\\hat{U}_{i} - \\bar{U})^{2}}{2}\\] and \\(S_{0}^{2}\\) is distributed as \\(\\chi^{2}\\) with \\(p\\) degrees of freedom. The results for our model are below: ncvTest(mod2) ## Non-constant Variance Score Test ## Variance formula: ~ fitted.values ## Chisquare = 1.137662, Df = 1, p = 0.28615 Here, we see not much evidence of heteroskedasticity, but it is probably worth looking at the residuals vs fitted plot anyway. ggplot(mapping=aes(x=mod2$fitted.values, y=mod2$residuals)) + geom_point() + theme_bw() Or, we could actually plot out the standardized squared residuals against the fitted values. sigma2 &lt;- sum(mod2$residuals^2)/nobs(mod2) ggplot(mapping=aes(x=mod2$fitted.values, y=I(mod2$residuals^2/sigma2))) + geom_point() + geom_smooth(se=TRUE, alpha=.25) + theme_bw() In this case, even though the test was not significant, we would probably want to investigate potential “fixes” to the problem just the same. In fact, the test is known to have relatively low power in small samples. The advice then is to “fix” the problem if there is any hint of heteroskedasticity even if the test isn’t conclusive. One of the most common fixes is to use robust standard errors. Robust standard errors can be calculated to compensate for an unknown pattern of non-constant error variance. They do not change the OLS coefficient estimates or solve the inefficiency problem, but do give more accurate \\(p\\)-values in the presence of the problem. These come in lots of different “flavors”, most of which are variants on the method originally proposed by White (1980). The covariance matrix of the OLS estimator is: \\[\\begin{aligned} V(\\mathbf{b}) &amp;= \\mathbf{(X^{\\prime}X)^{-1}X^{\\prime}\\Sigma X(X^{\\prime}X)^{-1}}\\\\ &amp;= \\mathbf{(X^{\\prime}X)^{-1}X^{\\prime}}V(\\mathbf{y})\\mathbf{ X(X^{\\prime}X)^{-1}} \\end{aligned}\\] Where \\(V(\\mathbf{y}) = \\sigma_{\\varepsilon}^{2}\\mathbf{I}_{n}\\) if the assumption of homoskedasticity is satisfied. The variance simplifies to: \\[V(\\mathbf{b}) = \\sigma_{\\varepsilon}^{2}(\\mathbf{X^{\\prime}X})^{-1}\\] In the presence of non-constant error variance, however, \\(V(\\mathbf{y})\\) contains nonzero covariance and unequal variance. In these cases, White suggests a consistent estimator of the variance that constrains \\(\\mathbf{\\Sigma}\\) to a diagonal matrix containing only squared residuals. The heteroskedasticity consistent covariance matrix (HCCM) estimator is then: \\[V(\\mathbf{b}) = \\mathbf{(X^{\\prime}X)^{-1}X^{\\prime}\\hat{\\Phi}X (X^{\\prime}X)^{-1}}\\] where \\(\\mathbf{\\hat{\\Phi}} = e^{2}_{i}\\mathbf{I}_{n}\\) and the \\(e_{i}\\) are the OLS residuals. These are known as HC0 robust standdard errors. Other HCCMs use the “hat value” which are the diagonal elements of \\(\\mathbf{X}\\left(\\mathbf{X}^{\\prime}\\mathbf{X}\\right)^{-1}\\mathbf{X}^{\\prime}\\) These give a sense of how far each observation is from the mean of the X’s. Below is a figure that shows two hypothetical \\(X\\) variables and the plotting symbols are proportional in size to the hat value set.seed(123) X &lt;- cbind(1, MASS::mvrnorm(25, c(0,0), diag(2))) h &lt;- diag(X%*% solve(t(X)%*%X)%*%t(X)) plot(X[,2], X[,3], cex = h*10) abline(h=mean(X[,3]), v=mean(X[,2])) MacKinnon and White (1985) considered three alternatives: HC1, HC2 and HC3, each of which offers a different method for finding \\(\\mathbf{\\Phi}\\). HC1: \\(\\frac{N}{N-K}\\times\\text{HC0}\\). HC2: \\(\\hat{\\mathbf{\\Phi}} = \\text{diag}\\left[\\frac{e_{i}^{2}}{1-h_{ii}}\\right]\\) where \\(h_{ii} = \\mathbf{x}_{i}(\\mathbf{X}^{\\prime}\\mathbf{X})^{-1}\\mathbf{x}_{i}^{\\prime}\\) HC3: \\(\\hat{\\mathbf{\\Phi}} = \\text{diag}\\left[\\frac{e_{i}^{2}}{(1-h_{ii})^{2}}\\right]\\) HC3 standard errors are shown to outperform the alternatives in small samples, but can still fail to generate the appropriate Type I error rate when outliers are present. HC4 standard errors can produce the appropriate test statistics even in the presence of outliers: \\[\\hat{\\mathbf{\\Phi}} = \\text{diag}\\left[\\frac{e_{i}^{2}}{(1-h_{ii})^{\\delta_{i}}}\\right]\\] where \\(\\delta_{i} = min\\left\\{4, \\frac{N h_{ii}}{p}\\right\\}\\) with \\(n\\) = number of obs, and \\(p\\) = number of parameters in model. HC4 outperform HC3 in the presence of influential observations, but not in other situations. HC4 standard errors are not universally better than others and as Cribari-Neto and da Silva (2011) show, HC4 SEs have relatively poor performance when there are many regressors and when the maximal leverage point is extreme. Cribari-Neto and da Silva propose a modified HC4 estimator, called HC4m, where, as above: \\[\\hat{\\mathbf{\\Phi}} = \\text{diag}\\left[\\frac{e_{i}^{2}}{(1-h_{ii})^{\\delta_{i}}}\\right]\\] and here, \\(\\delta_{i} = min\\left\\{\\gamma_{1}, \\frac{nh_{ii}}{p}\\right\\} + min\\left\\{\\gamma_{2}, \\frac{nh_{ii}}{p}\\right\\}\\). They find that the best values of the \\(\\gamma\\) parameters are \\(\\gamma_{1}=1\\) and \\(\\gamma_{2}=1.5\\). HC5 standard errors are supposed to also provide different discounting than HC4 and HC4m estimators. The HC5 standard errors are operationalized as: \\[\\hat{\\mathbf{\\Phi}} = \\text{diag}\\left[\\frac{e_{i}^{2}}{(1-h_{ii})^{\\delta_{i}}}\\right]\\] and here, \\(\\delta_{i} = min\\left\\{\\frac{nh_{ii}}{p}, max\\left\\{4, \\frac{nkh_{max}}{p}\\right\\}\\right\\}\\) with \\(k=0.7\\). For observations with bigger hat-values, their residuals get increased in size, thus increasing the standard error (generally). The coeftest() function in the lmtest package allows you to specify any of these HCCMs. library(lmtest) library(sandwich) coeftest(mod2, vcov. = vcovHC, type=&quot;HC5&quot;) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9.311443 1.096578 8.4914 8.117e-12 *** ## murUP 0.095534 0.087910 1.0867 0.28158 ## murMetro -0.084766 0.104986 -0.8074 0.42267 ## rep_majYes 0.013764 0.066645 0.2065 0.83709 ## white_pop -2.476198 1.160102 -2.1345 0.03697 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 You Try It! Now, re-estimate the model with the appropriate functional forms. - Test the model for heteroskedasticity problems. - Use robust standard errors to test model coefficients. - How do results change as you change the type of HCCM? We could use this to provide information to the ggpredict() and stargazer() as well. g &lt;- ggpredict(mod2, terms=&quot;white_pop [all]&quot;, vcov.fun=&quot;vcovHC&quot;, vcov.type=&quot;HC5&quot;) ggplot(g, aes(x=x, y=predicted)) + geom_ribbon(aes(ymin=conf.low, ymax=conf.high), alpha=.25) + geom_line() + theme_bw() + labs(x=&quot;White Population (%)&quot;, y=&quot;Predicted COVID-19 Cases/10k&quot;) We could also provide these to the stargazer() function. library(stargazer) ct &lt;- coeftest(mod2, vcov.=vcovHC(mod2, type=&quot;HC5&quot;)) stargazer(mod2, type=&quot;html&quot;, style=&quot;ajps&quot;, covariate.labels=c( &quot;Urban Area&quot;, &quot;Metro Area&quot;, &quot;Republican Majority (0/1)&quot;, &quot;Whie Population (%)&quot;, &quot;Intercept&quot;), se = list(ct[,2]), t = list(ct[,3]), p = list(ct[,4]), star.cutoffs=.05, star.char = &quot;`*`&quot;, notes=&quot;`*` p &lt; .05&quot;, notes.append=FALSE) log(cases_pc) Urban Area 0.096 (0.088) Metro Area -0.085 (0.105) Republican Majority (0/1) 0.014 (0.067) Whie Population (%) -2.476* (1.160) Intercept 9.311* (1.097) N 64 R-squared 0.187 Adj. R-squared 0.132 Residual Std. Error 0.248 (df = 59) F Statistic 3.393* (df = 4; 59) * p &lt; .05 There are some other options, like variance modeling (e.g., heteroskedastic regression), weighted least squares (WLS) or feasible generalized least squares (FGLS), too. The latter two you would do by specifying a weight= argument to the linear model where the weight is the variable that is proportional to the scale of the residuals. The former requires a maximum likelihood estimator that allows you to simultaneously parameterize the mean and variance (though this is done easily with the gamlss package). 3.4 Diagnostics: Outliers and Influential Data There are lots of diagnostics for outliers and influential data as well. First, it is worth noting that influential points are those that have both leverage and discrepancy. Points with high leverage are those that are far away from the center of the distribution of the \\(X\\) variables. Points with discrepancy are those with large residuals. The most common measure of leverage is the \\(hat-value\\), \\(h_i\\). The name \\(hat-values\\) results from their calculation based on the fitted values (\\(\\hat{Y}\\)): \\[\\begin{aligned} \\hat{Y}_{j} &amp;= h_{1j}Y_{1} + h_{2j}Y_{2} + \\cdots + h_{nj}Y_n\\\\ &amp;= \\sum_{i=1}^{n}h_{ij}Y_{i} \\end{aligned}\\] Recall that the Hat Matrix, \\(\\mathbf{H}\\), projects the \\(Y\\)’s onto their predicted values: \\[\\begin{aligned} \\mathbf{\\hat{y}} &amp;= \\mathbf{Xb}\\\\ &amp;= \\mathbf{X(X^{\\prime}X)^{-1}X^{\\prime}y}\\\\ &amp;= \\mathbf{Hy}\\\\ \\underset{(n\\times n)}{\\mathbf{H}} &amp;= \\mathbf{X(X^{\\prime}X)^{-1}X^{\\prime}} \\end{aligned}\\] If \\(h_{ij}\\) is large, the \\(i^{th}\\) observation has a substantial impact on the \\(j^{th}\\) fitted value. Since \\(\\bm{H}\\) is symmetric and idempotent1, the hat value \\(h_{i}\\) measures the potential leverage of \\(Y_{i}\\) on all the fitted values. In multiple regression, \\(h_i\\) measures the distance from the centroid point of all of the \\(X\\)’s (point of means). Hat values range from \\(\\frac{1}{n}\\) to 1 with a mean of \\(\\frac{k+1}{n}\\). Values more than twice the mean are considered “big”, though this is not a formal test. We could make a plot of the hat values. hbar &lt;- mod2$rank/nobs(mod2) ggplot(mapping=aes(x=1:nobs(mod2), y=hatvalues(mod2))) + geom_point() + geom_hline(yintercept=2*hbar, lty=2) + theme_bw() + labs(x=&quot;Observation Number&quot;, y=&quot;Hat Value&quot;) As you can see above, there are a few points with higher than expected hat values. We could find them as follows: aug2 &lt;- augment(mod2) %&gt;% mutate(obs = 1:nobs(mod2)) aug2 %&gt;% filter(.hat &gt; 2*hbar) %&gt;% select(`log(cases_pc)`, mur, rep_maj, .hat, obs) ## # A tibble: 2 × 5 ## `log(cases_pc)` mur rep_maj .hat obs ## &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; ## 1 7.96 Rural Yes 0.184 23 ## 2 7.10 Metro No 0.241 62 So, observations 23 and 62 are the ones with high leverage. Now remember, they are not necessarily influential because we don’t know whether they are also models with big residuals. Unusual observations typically have large residuals but not necessarily so - high leverage observations can have small residuals because they pull the line towards them: \\[V(E_{i}) = \\sigma^{2}_{\\varepsilon}(1-h_{i})\\] Standardized residuals provide one possible, though unsatisfactory, way of detecting outliers: \\[E_{i}^{\\prime} = \\frac{E_{i}}{S_{E}\\sqrt{1-h_{i}}}\\] The numerator and denominator are not independent and thus \\(E_{i}^{\\prime}\\) does not follow a \\(t\\)-distribution: If \\(\\mid E_{i} \\mid\\) is large, the standard error is also large: \\[S_{E} = \\sqrt{\\frac{\\sum E_{i}^{2}}{n-k-1}}\\] However, if we refit the model deleting the \\(i^{th}\\) observation we obtain an estimate of the standard deviation of the residuals \\(S_{E(-i)}\\) (standard error of the regression) that is based on the \\(n-1\\) observations. We then calculate the studentized residuals \\(E_{i}^{*}\\)’s, which have an independent numerator and denominator: \\[E_{i}^{*} = \\frac{E_{i}}{S_{E(-i)}\\sqrt{1-h_{i}}}\\] Studentized residuals follow a \\(t\\)-distribution with \\(n-k-2\\) degrees of freedom. Observations that have a studentized residual outside the \\(\\pm 2\\) range are considered statistically significant at the 95% level. Since we are looking for the furthest outliers, it is not legitimate to use a simple \\(t\\)-test. We would expect that \\(5\\%\\) of the studentized residuals would be beyond \\(t_{.025}\\pm2\\) by chance alone. To remedy this we can make a Bonferroni adjustment to the \\(p\\)-value. The Bonferroni \\(p\\)-value for the largest outlier is: \\(p=2np^{\\prime}\\) where \\(p^{\\prime}\\) is the unadjusted \\(p\\)-value from a \\(t\\)-test with \\(n-k-2\\) degrees of freedom. The outlierTest() function in the car package gives Bonferroni \\(p\\)-value for the largest absolute studentized residual outlierTest(mod2) ## No Studentized residuals with Bonferroni p &lt; 0.05 ## Largest |rstudent|: ## rstudent unadjusted p-value Bonferroni p ## 23 3.243943 0.0019581 0.12532 Note from the above that the adjusted \\(p\\)-value suggests that we do not have significant outliers. Recall that influential observations are those that have both discrepancy and leverage. There are a few different ways of measuring this. One common way is with DFBeta - a difference in the coefficient induced by removing a single observation. \\[D_{ij} = B_{j} - B_{j(-i)}\\quad \\forall \\quad i=1, \\ldots, n; \\quad j=1, \\ldots, k\\] The \\(B_{j}\\) are the coefficients for all the data and the \\(B_{j(-i)}\\) are the coefficients for the same model with the \\(i^{th}\\) observation removed. A standard cut-off for an influential observation is: \\(D_{ij} \\geq \\frac{2}{\\sqrt{n}}\\). The dfbeta() function calculates these values for us. There is a scaled version that permits more reasonable comparison: \\[D_{ij}^{(s)} = \\frac{B_j - B_{j(-i)}}{s_{E(-i)}\\sqrt{(\\mathbf{X}^{\\prime}\\mathbf{X})_{jj}}}\\] The main problem with the \\(D_{ij}\\) and \\(D_{ij}^{(s)}\\) is that it produces a value for every observation and coefficient sometimes requiring lots of investigation. We could plot them in R with: Ds &lt;- dfbetas(mod2) Before we plot them, we’ll arrange them in log format with the pivot_longer() function from the tidyr package. In the plot below, we should be looking for values bigger than \\(\\frac{2}{\\sqrt{n}}\\). We can see below that several of the coefficients appear to be affected by some influential observations. library(tidyr) Ds &lt;- Ds %&gt;% as.data.frame %&gt;% select(-1) %&gt;% mutate(obs = 1:nobs(mod2)) %&gt;% pivot_longer(cols = -obs, names_to = &quot;var&quot;, values_to=&quot;value&quot;) ggplot(Ds, aes(x=obs, y=abs(value))) + geom_point() + geom_hline(yintercept=2/sqrt(nobs(mod2b)), lty=2) + facet_wrap(~var) + theme_bw() + labs(x=&quot;Observation&quot;, y=&quot;DFBeta Scaled&quot;) Cook’s D measures the distance between \\(B_j\\) and \\(B_{j(-i)}\\) by calculating an \\(F\\)-statistic for the hypothesis that \\(B_j=B_{j(-i)}\\), for \\(j=0,1,\\ldots,k\\). An \\(F\\)-test is calculated for each observation as follows: \\[D_{i} = \\frac{E_{i}^{\\prime 2}}{k+1} \\times \\frac{h_{i}}{1-h_{i}}\\] where \\(h_{i}\\) is the hat value for each observation and \\(E_{i}^{\\prime}\\) is the standardized residual. The first fraction measures discrepancy; the second fraction measures leverage. There is no significance test for \\(D_i\\) (i.e., the \\(F\\)-statistic here measures only distance) but a commonly used cut-off is: \\(D_{i} &gt; \\frac{4}{n-k-1}\\) ggplot(aug2, aes(x=obs, y=.cooksd)) + geom_point() + geom_hline(yintercept=4/mod2b$rank, lty=2) + theme_bw() + labs(x=&quot;Observation&quot;, y=&quot;Cook&#39;s Distance&quot;) We could plot many of these diagnostics together in a “bubble plot”. ggplot(aug2, aes(x=.hat, y=.std.resid, size=sqrt(.cooksd))) + geom_point(show.legend = FALSE, shape=1) + geom_hline(yintercept=c(-2,2), lty=2) + geom_vline(xintercept=2/sqrt(nobs(mod2)), lty=2) + theme_bw() + labs(x=&quot;Leverage (Hat Values)&quot;, y=&quot;Studentized Residual&quot;) Finally, we could evaluate the potential for joint influence. Subsets of cases can jointly influence a regression line, or can offset the influence of other points. Cook’s D can help us determine joint influence if there are relatively few influential cases. That is, we can delete cases sequentially, updating the model each time and exploring the Cook’s D’s again. This approach is impractical if there are potentially a large number of subsets to explore. Added-variable plots (also called partial-regression plots) provide a more useful method of assessing joint influence. These plots essentially show the partial relationships between \\(Y\\) and each \\(X\\). Let \\(Y_{i}^{(1)}\\) represent the residuals from the least-squares regression of \\(Y\\) on all of the \\(X\\)’s except for \\(X_1\\): \\[Y_{i} = A^{(1)} + B_{2}^{(1)}X_{i2} + \\cdots + B_{k}^{(1)}X_{ik} + Y_{i}^{(1)}\\] Similarly, \\(X_{i}^{(1)}\\) are the residuals from the regression of \\(X_{1}\\) on all the other \\(X\\)’s \\[X_{i1} = C^{(1)} + D_{2}^{(1)}X_{i2} + \\cdots + D_{k}^{(1)}X_{ik} + X_{i}^{(1)}\\] These two equations determine the residuals \\(X^{(1)}\\) and \\(Y^{(1)}\\) as parts of \\(X_{1}\\) and \\(Y\\) that remain when the effects of \\(X_{2}, \\ldots, X_{k}\\) are removed. The Residuals \\(Y^{(1)}\\) and \\(X^{(1)}\\) have the following properties: Slope of the regression of \\(Y^{(1)}\\) on \\(X^{(1)}\\) is the least-squares slope \\(B_1\\) from the full multiple regression Residuals from the regression of \\(Y^{(1)}\\) on \\(X^{(1)}\\) are the same as the residuals from the full regression: \\(Y_{i}^{(1)} = B_{1}X_{i1}^{(1)} + E_{i}\\) Variation of \\(X^{(1)}\\) is the conditional variance of \\(X_1\\) holding the other \\(X\\)’s constant. Consequently, except for the df the standard error from the partial simple regression is the same as the multiple regression SE of \\(B_1\\). We can make these in R with avPlots() avPlots(mod2) The only thing that really stands out as a potential problem is that for the white_pop|others figure, observations 23 and 47 are both working to decrease the regression slope (make it more negative). You Try It! Use the methods discussed above to evaluate potential outliers and influential data in the model you’ve estimated. 3.5 Diagnostics for Normality Diagnostics for normality are slightly less plentiful, but there are a couple of options. First, we could simply look at the distribution of the residuals and compare that to a normal distribution. ggplot(aug2, aes(x=.std.resid)) + stat_density(geom=&quot;line&quot;, aes(color=&quot;Empirical&quot;)) + stat_function(aes(color=&quot;Theoretical&quot;), fun=dnorm, args=list(sd = sd(aug2$.std.resid)), geom=&quot;line&quot;) + labs(x=&quot;Residuals&quot;, color = &quot;Distribution&quot;) The distribution above doesn’t look all that normal. If we wanted a sense of how not normal it looks, we could look at a quantile-quantile plot. We’ll use the one in the ggpubr package. library(ggpubr) ggqqplot(aug2$.std.resid) This looks alright, but we could also do a formal test. The Shapiro-Wilk’s test is a good option here. shapiro.test(aug2$.std.resid) ## ## Shapiro-Wilk normality test ## ## data: aug2$.std.resid ## W = 0.97828, p-value = 0.3186 The null hypothesis here is normality, so we cannot reject the null hypothesis. There is no indication of non-normality here. You Try It! Are the residuals from your model normally distributed? You’ll note that when we started, we were using the log of cases. It might be that there is a better normalizing transformation than the log. We could consider that with the powerTransform() function summary(powerTransform(mod2, family=&quot;bcPower&quot;)) ## bcPower Transformation to Normality ## Est Power Rounded Pwr Wald Lwr Bnd Wald Upr Bnd ## Y1 -1.878 1 -6.3521 2.596 ## ## Likelihood ratio test that transformation parameter is equal to 0 ## (log transformation) ## LRT df pval ## LR test, lambda = (0) 0.680966 1 0.40925 ## ## Likelihood ratio test that no transformation is needed ## LRT df pval ## LR test, lambda = (1) 1.603868 1 0.20536 This suggests that no transformation is needed in addition to the log. 3.5.1 Bootstrapping Regression Models One way to deal with non-normality (if non-normality of the errors is the only problem) is to use the bootstrap. The “wild bootstrap” is known to handle problems with heteroskedasticity appropriately. We can use the wild.boot() function in the lmboot package to accomplish this. library(lmboot) attributes(colo_dat$cases_pc) &lt;- NULL w &lt;- wild.boot(log(cases_pc) ~ mur + rep_maj + white_pop, B = 2500, data=colo_dat) We could calculate percentile confidence intervals for the parameters in the model by summarising the bootEstParam element of the w object. library(tibble) bs.ci &lt;- w$bootEstParam %&gt;% as.data.frame %&gt;% summarise(across(everything(), ~ quantile(.x, probs=c(.025,.975)))) %&gt;% t() %&gt;% as_tibble(., .name_repair=&quot;minimal&quot;) names(bs.ci) &lt;- c(&quot;lower&quot;, &quot;upper&quot;) bs.ci &lt;- bs.ci %&gt;% mutate(est = c(w$origEstParam), param = names(coef(mod2))) For comparison, we could make the original and HC5 confidence intervals, too: orig.ci &lt;- as_tibble(confint(mod2), .name_repair=&quot;minimal&quot;) names(orig.ci) &lt;- c(&quot;lower&quot;, &quot;upper&quot;) orig.ci &lt;- orig.ci %&gt;% mutate(est = coef(mod2), param = names(coef(mod2))) hc5t &lt;- coeftest(mod2, vcov.=vcovHC, type=&quot;HC5&quot;) hc5.ci &lt;- as_tibble(confint(hc5t), .name_repair=&quot;minimal&quot;) names(hc5.ci) &lt;- c(&quot;lower&quot;, &quot;upper&quot;) hc5.ci &lt;- hc5.ci %&gt;% mutate(est = coef(mod2), param = names(coef(mod2))) Now, we could just look at them, but it’s probably more interesting to make a graph. To do that, we’ll have to combine everything together after making a flag in each dataset for which model the confidence intervals come from. all.ci &lt;- bind_rows(orig.ci, hc5.ci, bs.ci) all.ci &lt;- all.ci %&gt;% mutate(type=factor(rep(1:3, each=5), labels=c(&quot;Raw&quot;, &quot;HC5&quot;, &quot;BS&quot;))) Now, we could make the graph ggplot(all.ci, aes(y=est, x=param, colour=type)) + geom_point(position = position_dodge(width=.3)) + geom_linerange(aes(ymin=lower, ymax=upper), position= position_dodge(width=.3)) + geom_hline(yintercept=0, lty=2) + theme_bw() + labs(x = &quot;&quot;, y=&quot;Coefficient (95% CI)&quot;, colour = &quot;CI Type&quot;) + coord_flip() You Try It! What do you find if you bootstrap your model instead of using robust standard errors? 3.6 Interactions We have addressed most of the main issues covered in basic linear model discussions, but we have yet to discuss interactions. Let’s start by talking about interactions between categorical variables and continuous variables. We’ll model the interaction between white_pop and rep_maj. In R, we do this simply by putting an asterisk between the two terms: mod3 &lt;- lm(log(cases_pc) ~ rep_maj*white_pop + mur , data=colo_dat) summary(mod3) ## ## Call: ## lm(formula = log(cases_pc) ~ rep_maj * white_pop + mur, data = colo_dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.54568 -0.16279 0.00512 0.16205 0.47957 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.42881 0.99413 7.473 4.71e-10 *** ## rep_majYes 3.62991 1.34817 2.692 0.00926 ** ## white_pop -0.40962 1.07829 -0.380 0.70542 ## murUP 0.07534 0.07046 1.069 0.28935 ## murMetro -0.06365 0.08416 -0.756 0.45249 ## rep_majYes:white_pop -3.93514 1.46544 -2.685 0.00943 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2355 on 58 degrees of freedom ## Multiple R-squared: 0.2769, Adjusted R-squared: 0.2146 ## F-statistic: 4.443 on 5 and 58 DF, p-value: 0.001708 From the regression output, we see that the interaction of the dummy and continuous variables is significant. If the categorical variable had had more than two levels, we would have needed to look at the Anova() output to see whether the interaction was significant. This means that the effect of white_pop is significantly different in republican majority counties versus republican minority counties. The converse is also true - the effect of being a majority republican county changes as a function of the white population. We’ll investigate how those two things work below. The intQualQuant() function in the DAMisc package allows us to evaluate interactions between quantitative and qualitative variables. Specifying type='slopes' and plot=FALSE will give you all of the simple slopes, the conditional partial effects of the continuous variable given different values of the categorical variable. library(DAMisc) intQualQuant(mod3, c(&quot;rep_maj&quot;, &quot;white_pop&quot;), type=&quot;slopes&quot;, plot=FALSE) ## $out ## eff se tstat pvalue ## No -0.4096249 1.078292 -0.3798829 7.054198e-01 ## Yes -4.3447670 1.026966 -4.2306835 8.388801e-05 ## ## $varcor ## [,1] [,2] ## [1,] 1.16271458 0.03493473 ## [2,] 0.03493473 1.05465859 ## ## $mainvar ## [1] &quot;white_pop&quot; ## ## $givenvar ## [1] &quot;rep_maj&quot; ## ## attr(,&quot;class&quot;) ## [1] &quot;iqq&quot; If we set plot=TRUE, we see the two lines that give the effect of white_pop for each of the rep_maj conditions. The rug plot gives the distribution of white_pop for each of the groups on rep_maj. The benefit of the rug plot is to identify places where inferences are plausible and where they are not. intQualQuant(mod3, c(&quot;rep_maj&quot;, &quot;white_pop&quot;), type=&quot;slopes&quot;, plot=TRUE) We can also plot the “other side” of the interaction. It basically identifies the difference between the two lines for every different value of white_pop. We get this by setting type=\"facs\". Here, we see that the difference between minority and majority Republican counties becomes statistically isignificant at around 0.875, which is the 12.5 percentile of the white_pop variable. intQualQuant(mod3, c(&quot;rep_maj&quot;, &quot;white_pop&quot;), type=&quot;facs&quot;, plot=TRUE) To see how the interaction works out on the un-transformed cases variable, we’re best off using the ggpredict() function. ggpredict(mod3, terms=c(&quot;white_pop [all]&quot;, &quot;rep_maj&quot;)) %&gt;% ggplot(aes(x=x, y=predicted, colour=group, fill=group)) + geom_ribbon(aes(ymin=conf.low, ymax=conf.high), alpha=.15, col=&quot;transparent&quot;) + geom_line() + theme_bw() + labs(x=&quot;White Population %&quot;, y=&quot;Predicted Cases of COVID-19/10k&quot;, colour=&quot;Republican Majority&quot;, fill=&quot;Republican Majority&quot;) You Try It! Add to your model an interaction of tax and sei10. - What does the interaction say about the conditional nature of this relationship. We can also look at two continuous variable interactions. Here, we switch back to an interaction of BAplus and white_pop. Because the interaction term is significant, it means there is a significant conditional relationship. mod4 &lt;- lm(log(cases_pc) ~ white_pop*BAplus + rep_maj, data=colo_dat) summary(mod4) ## ## Call: ## lm(formula = log(cases_pc) ~ white_pop * BAplus + rep_maj, data = colo_dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.56930 -0.16154 0.03626 0.15905 0.36885 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 16.8170 1.8300 9.189 5.53e-13 *** ## white_pop -10.7807 2.0036 -5.381 1.34e-06 *** ## BAplus -25.6989 5.7234 -4.490 3.36e-05 *** ## rep_majYes 0.1273 0.0794 1.603 0.114 ## white_pop:BAplus 28.2401 6.2249 4.537 2.86e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2218 on 59 degrees of freedom ## Multiple R-squared: 0.3474, Adjusted R-squared: 0.3032 ## F-statistic: 7.853 on 4 and 59 DF, p-value: 3.826e-05 The main way of looking at these effects is through a so-called marginal effects plot. This plots the effect of one variable against the values of the other variable. The DAintfun2() function from the DAMisc package does this for us. DAintfun2(mod4, c(&quot;white_pop&quot;, &quot;BAplus&quot;), rug=FALSE, hist=TRUE, scale.hist = .3) Here, we see that the effect of white_pop is significant and negative when BAplus is less than around 0.3 and it is significant and positive when BAplus is greater than around .48 (more precise estimates of this below). On the other hand (in the right-hand panel), we see that BAplus is significant and negative when white_pop takes on values smaller than around .875 and is positive and significant when white_pop is above around 0.925. If we wanted to figure out where those values are exactly, we could use the changeSig() function (also in the DAMisc package). changeSig(mod4, c(&quot;white_pop&quot;, &quot;BAplus&quot;)) ## LB for B(white_pop | BAplus) = 0 when BAplus=0.47 (94th pctile) ## UB for B(white_pop | BAplus) = 0 when BAplus=0.3317 (67th pctile) ## LB for B(BAplus | white_pop) = 0 when white_pop=0.9324 (45th pctile) ## UB for B(BAplus | white_pop) = 0 when white_pop=0.8837 (16th pctile) We were pretty close with the visual inspection, but the results from changeSig() are more precise. You Try It! Now, do the following: - Make a new variable loginc which is the log of realinc and replace log(realinc) with loginc in your model. - Instead of an interaction between tax and sei10, use an interaction between sei10 and loginc. - Evaluate the interaction. Exercises For this set of exercise, we’ll be working with the wvsb.rda data set. Create an index of variables related to different form of cheating being justifiable or not using the following variables: V198, V199, V200, V201. Make sure that the resulting index varies from 0 to 1. Regress the cheating index on the secular values index (sacsecval) in an OLS model. Produce a table of the model using stargazer. Create an effect plot of the model using the ggeffects package. Let’s add some variables to the model. Create a compound index representing trust in institutions by adding the variables I_TRUSTPOLICE, I_TRUSTCOURTS, and I_TRUSTARMY, and dividing the resulting vector by 3. Create a binary variable from V240, 1 representing female respondents, 0 otherwise. Create 3 binary variables for age from V242, one for people 16 to 35, one for people 36 to 50, and one for people 51 plus. Notice how these three variables are perfectly colinear, meaning that once we know two of them, we know the value for the other one. Hence, you need only choose two out of the three to include in your model. Finally, subset the data such that only. Now that we have a more refined model, let’s do some diagnostics. First, create a component + residual plot using crPlot() for the secular values index (sacsecval). The residuals appear to be linear. Let’s now add a confidence interval arround the estimated line. Use augment function from broom to do so. Now let’s look for homoskedasticity violations. Use ncvTest to estimate a score test of the residuals. Then create a graph with the fitted values on the x-axis and the residuals on the y-axis. Since we have evidence of heteroskedasticity, we need to correct our standard errors. Using the lmtest and sandwich packages, create a table for the model with HC5 standard errors. Now, let’s run some outlier diagnostics. 9.1 Produce a hat value graph for the sacsecval variable. 9.2 Extract the outliers according to the hat values. 9.3 Compute the Bonferroni p-value for the model. Do we have significant outliers? 9.4 Calculate the difference in the coefficients induced by removing a single observation. Present the results in a graph. 9.5 Produce a Cook’s D graph. 9.6 Produce a bubble plot. 9.7 Produce an added-variable plot. Now, let’s look at the normality of the residuals: 10.1 Create a graph that shows the distribution of the residuals against a normal distribution. 10.2 Present a quantile-quantile plot using the ggpubr package. 10.3 Perform the Shapiro-Wilk’s test. Are the residuals normal? Reproduce the model from question 4, this time with an interaction between sacsecval and female. Compute the conditional partial effect of female on sacsecval using the avg_comparisons function. Plot your results. For matrix \\(\\bm{H}\\), idempotent imply that \\(\\bm{H}=\\bm{H}\\times\\bm{H&#39;}\\)↩︎ "],["generalized-linear-models---logit..html", "Chapter 4 Generalized Linear Models - Logit. 4.1 Evaluating Model Fit 4.2 Marginal Effects 4.3 Marginal Effect Plots 4.4 Interactions in Binary DV Models 4.5 Other GLMs. Exercises", " Chapter 4 Generalized Linear Models - Logit. Next, we can talk about generalized linear models (GLMs). The GLM is how we estimate the logistic and probit regression model in R as well as the poisson and negative binomial models. We are going to focus on the logistic regression model. As a substantive example, we’re going to look at some work that I’ve done in the past. In particular, this is data that I used with Christian Davenport and Sarah Soule in an article called Protesting While Black published in the American Sociological Review in 2011. We were particularly interested in the use of arrests or violence on the part of police at protests that were characterized as either largely African American or not. First, we can read in the data: library(dplyr) library(rio) dat &lt;- import(&quot;data/Davenport_Soule_ASR.dta&quot;) %&gt;% ## chose variables we need for the model select(police1, afam, arrdum, police5, pre65, iblackpre65, ny, south, logpart, propdam, counterd, standard2, extcont2, govtarg, stratvar, viold, evyy) %&gt;% ## turn afam into a factor mutate(afam = factor(afam, labels=c(&quot;No&quot;,&quot;Yes&quot;))) %&gt;% ## keep only those where police were present filter(police1 == 1) %&gt;% ## listwise delete na.omit() Now, let’s estimate a model of arrests (arrdum) on a set of controls and the important variables afam identifying whether or not it was an African American protest and evyy, the event year. For now, we’re not going to make an interaction between these two variables. Specifying family=binomial indicates a 0/1 variable (or that you want to use n choose k data where your dependent variable is a two columns - successes and failures). The default link is the logit link, but you can switch to probit with family=binomial(link=\"probit\"). mod &lt;- glm(arrdum ~ poly(evyy,3) + afam + ny + south + logpart + propdam + counterd + standard2 + extcont2 + govtarg + stratvar + viold, data=dat, family=binomial) summary(mod) ## ## Call: ## glm(formula = arrdum ~ poly(evyy, 3) + afam + ny + south + logpart + ## propdam + counterd + standard2 + extcont2 + govtarg + stratvar + ## viold, family = binomial, data = dat) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.53652 0.11649 4.606 4.11e-06 *** ## poly(evyy, 3)1 7.74804 2.32726 3.329 0.000871 *** ## poly(evyy, 3)2 8.39345 2.26072 3.713 0.000205 *** ## poly(evyy, 3)3 5.05174 2.16950 2.329 0.019884 * ## afamYes 0.31330 0.06688 4.685 2.80e-06 *** ## ny -0.51309 0.06676 -7.686 1.52e-14 *** ## south -0.24535 0.07522 -3.262 0.001107 ** ## logpart -0.11038 0.01475 -7.485 7.14e-14 *** ## propdam 0.23645 0.08492 2.784 0.005364 ** ## counterd -0.41626 0.08963 -4.644 3.41e-06 *** ## standard2 0.20604 0.10020 2.056 0.039750 * ## extcont2 -0.10711 0.11257 -0.952 0.341332 ## govtarg -0.23341 0.05748 -4.061 4.89e-05 *** ## stratvar 0.29818 0.05346 5.577 2.45e-08 *** ## viold 0.16939 0.08799 1.925 0.054220 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 7719.0 on 5701 degrees of freedom ## Residual deviance: 7461.1 on 5687 degrees of freedom ## AIC: 7491.1 ## ## Number of Fisher Scoring iterations: 4 You Try It! Using the 2016 GSS data again: Make a new variable called inews from newsfrom that is 1 if the respondent gets news primarily from the internet and 0 for any other source. Make sure to keep the missing values missing. Make a variable called college_ed from educ which is coded such that 0-12 = none, 13:15 = some and 16+ = degree. Estimate and summarise a logit model of inews on age, sei10, college_ed and sex. 4.1 Evaluating Model Fit With all non-linear models, we need to figure out how to interpret the coefficients and figure out how well the model fits. Let’s think about model fit, first. There are a few different things that we can do. The DAMisc package has a function called binfit() which produces measure of fit for binary models: library(DAMisc) binfit(mod) ## Names1 vals1 Names2 vals2 ## 1 Log-Lik Intercept Only: -3859.513 Log-Lik Full Model: -3730.537 ## 2 D(5687): 7461.075 LR(14): 257.951 ## 3 Prob &gt; LR: 0.000 ## 4 McFadden&#39;s R2: 0.033 McFadden&#39;s Adk R2: 0.030 ## 5 ML (Cox-Snell) R2: 0.044 Cragg-Uhler (Nagelkerke) R2: 0.060 ## 6 McKelvey &amp; Zavoina R2: 0.057 Efron&#39;s R2: 0.046 ## 7 Count R2: 0.626 Adj Count R2: 0.088 ## 8 BIC: 7590.804 AIC: 7491.075 This produces a number of different pseudo \\(R^2\\) measures that all have some analogy to the computation of \\(R^2\\) from the linear model. The only one that’s not really appropriate as a measure of fit is the “Count R2”. This is really just the proportion correctly predicted and needs to be adjusted for the number in the modal category to be useful - that’s what the “Adj Count R2” does. There’s another measure of fit that may be useful, too - the proportional reduction in error (PRE) and the expected PRE (or ePRE). These can be obtained with the pre() function from DAMisc. Specifying sim=TRUE will use a parametric bootstrap to get confidence bounds for the proportional reduction in error. pre(mod, sim=TRUE) ## mod1: arrdum ~ poly(evyy, 3) + afam + ny + south + logpart + propdam + counterd + standard2 + extcont2 + govtarg + stratvar + viold ## mod2: arrdum ~ 1 ## ## Analytical Results ## PMC = 0.590 ## PCP = 0.626 ## PRE = 0.088 ## ePMC = 0.516 ## ePCP = 0.538 ## ePRE = 0.045 ## ## Simulated Results ## median lower upper ## PRE 0.084 0.068 0.098 ## ePRE 0.045 0.037 0.053 If you wanted to look at the ROC for the model, you could do the following: library(pROC) roc(dat$arrdum ~ predict(mod, type=&quot;response&quot;), print.auc=TRUE, plot=TRUE) ## ## Call: ## roc.formula(formula = dat$arrdum ~ predict(mod, type = &quot;response&quot;), print.auc = TRUE, plot = TRUE) ## ## Data: predict(mod, type = &quot;response&quot;) in 2338 controls (dat$arrdum 0) &lt; 3364 cases (dat$arrdum 1). ## Area under the curve: 0.623 The model doesn’t provide us a lot of value-added, but more than zero, so we’ll move on. Note that these models are not built to maximize predictive capacity, though considering the predictive accuracy isn’t the worst thing ever. You Try It! How does the model you estimated above fit? Use the methods discussed above to evaluate the model of inews. 4.2 Marginal Effects The next thing to do is to figure out what the parameter estimates mean. There are lots of ways to do this. In the logit model, like we’ve estimated here, we could exponentiate the coefficients to get the odds ratio. These can be useful for single coefficient terms, but are less useful for things like the polynomial in year where one coefficient cannot really be interpreted independent of the others. exp(coef(mod)) ## (Intercept) poly(evyy, 3)1 poly(evyy, 3)2 poly(evyy, 3)3 afamYes ny ## 1.7100374 2317.0322566 4418.0303543 156.2936582 1.3679333 0.5986400 ## south logpart propdam counterd standard2 extcont2 ## 0.7824287 0.8954906 1.2667435 0.6595099 1.2287974 0.8984265 ## govtarg stratvar viold ## 0.7918282 1.3473985 1.1845790 The glmChange() function in the DAMisc package calculates first differences for each of the variables. It uses a parametric bootstrap to generate confidence intervals for the differences. glmChange(mod, data=dat, diffchange = &quot;unit&quot;, sim=TRUE) ## # A tibble: 12 × 8 ## focal val_Low val_High fit_Low fit_High diff q_2.5 q_97.5 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 evyy 1968.500 1969.500 0.624 0.621 -0.00296 -0.00594 -0.0000125 ## 2 afam No Yes 0.623 0.693 0.0703 0.0427 0.0976 ## 3 ny -0.500 0.500 0.681 0.561 -0.120 -0.149 -0.0912 ## 4 south -0.500 0.500 0.651 0.594 -0.0576 -0.0926 -0.0237 ## 5 logpart 3.831 4.831 0.636 0.610 -0.0259 -0.0325 -0.0192 ## 6 propdam -0.500 0.500 0.595 0.650 0.0555 0.0161 0.0938 ## 7 counterd -0.500 0.500 0.670 0.573 -0.0975 -0.136 -0.0555 ## 8 standard2 0.500 1.500 0.598 0.647 0.0484 0.0000680 0.0907 ## 9 extcont2 -0.500 0.500 0.635 0.610 -0.0252 -0.0767 0.0243 ## 10 govtarg -0.500 0.500 0.650 0.595 -0.0548 -0.0805 -0.0273 ## 11 stratvar 0.500 1.500 0.587 0.657 0.0699 0.0452 0.0941 ## 12 viold -0.500 0.500 0.603 0.643 0.0398 -0.00303 0.0824 marginaleffects::comparisons(mod, newdata=&quot;median&quot;, comparison = &quot;difference&quot;) ## ## Term Contrast Estimate Std. Error z Pr(&gt;|z|) S 2.5 % 97.5 % ## afam Yes - No 0.0703 0.01474 4.770 &lt; 0.001 19.1 0.041418 0.099193 ## counterd 1 - 0 -0.1015 0.02226 -4.560 &lt; 0.001 17.6 -0.145134 -0.057876 ## evyy +1 -0.0030 0.00165 -1.816 0.06929 3.9 -0.006244 0.000237 ## extcont2 1 - 0 -0.0255 0.02712 -0.939 0.34766 1.5 -0.078623 0.027684 ## govtarg 1 - 0 -0.0562 0.01373 -4.091 &lt; 0.001 14.5 -0.083097 -0.029266 ## logpart +1 -0.0263 0.00355 -7.405 &lt; 0.001 42.8 -0.033207 -0.019307 ## ny 1 - 0 -0.1257 0.01620 -7.760 &lt; 0.001 46.7 -0.157451 -0.093950 ## propdam 1 - 0 0.0537 0.01871 2.871 0.00409 7.9 0.017047 0.090403 ## south 1 - 0 -0.0591 0.01827 -3.235 0.00122 9.7 -0.094931 -0.023300 ## standard2 1 - 0 0.0495 0.02432 2.034 0.04192 4.6 0.001808 0.097137 ## stratvar +1 0.0671 0.01193 5.623 &lt; 0.001 25.7 0.043698 0.090461 ## viold 1 - 0 0.0389 0.01987 1.956 0.05042 4.3 -0.000071 0.077837 ## ## Type: response The output above shows a number of things. In the diffs element are the differences. The min column gives the predicted probability when the variable is held constant at the value given in the min row for the corresponding variable in the minmax element. For example, the min column entry for the logpart variable is calculated holding logpart at 0.00 and all of the other variables at the values that are in the typical row. The max column entry for the logpart variable is calculated holding logpart at 11.51 and holding all of the other variables constant at the values in the typical row. The diff column just calculates the difference in those two min and max values. The lower and upper columns (which you get if you specify sim=TRUE) are the lower and upper \\(95\\%\\) confidence bounds. There is a debate in the literature about what kind of effects are most representative of the “true” effect of the variable (if it even makes sense to talk about that). One approach is the one mentioned above - usually referred to as the marginal effect at means (MEM) or marginal effect at reasonable values (MER) approach. The main feature of this approach is that all of the other covariates (save the one you’re changing) are held constant at a single value. Another approach is called the average marginal effect (AME) appoach. The main feature of this approach is that one variable is changed and all other variables are held constant at their observed values. So, instead of calculating a single effect, you calculate \\(n\\) different effects. These different effects are then averaged across all of the \\(n\\) values. The glmChange2() function calculates AMEs in R. glmChange2(mod, &quot;logpart&quot;, data=dat, diffchange=&quot;sd&quot;) ## mean lower upper ## logpart -0.05177293 -0.06545373 -0.03827276 marginaleffects::avg_comparisons(mod,variables = list(&quot;logpart&quot; = &quot;sd&quot;), comparison = &quot;difference&quot;) # ??? ## ## Estimate Std. Error z Pr(&gt;|z|) S 2.5 % 97.5 % ## -0.0524 0.00697 -7.52 &lt;0.001 44.1 -0.0661 -0.0388 ## ## Term: logpart ## Type: response ## Comparison: (x + sd/2) - (x - sd/2) Note here that the estimate of the difference is about the same, but the confidence bounds are a lot narrower here. This isn’t a generalizable result, but it does happen to be the case here. Both of these results are first differences. These tell us how the predicted probability changes as a function of a discrete change in the independent variable of interest. The mathematical expression for the first difference is as below. For example, we could write the model above as: \\[\\log\\left(\\frac{Pr(\\text{Arrest})}{Pr(\\text{No Arrest})}\\right) = a + d\\text{logPart} + XB\\] where logPart is the log of protest participation with coefficient \\(d\\) and \\(X\\) are all of the other covariates with coefficient vector \\(B\\). A discrete change, is: \\[D = F(a + b(\\overline{\\text{logPart}} + .5\\delta) + x_{h}B) - F(a + b(\\overline{\\text{logPart}} - .5\\delta) + x_{h}B)\\] where \\(\\delta\\) is the amount of change being evaluated (often either a unit or a standard deviation) and \\(x_h\\) is a vector of hypothetical values for the other covriates and \\(F()\\) is the CDF of the probability distribution. This was for the MER approach. The AME approach is: \\[\\bar{D} = \\frac{1}{n}\\sum_{i}^{n}\\left(F(a + b(\\text{logPart}_i + .5\\delta) + x_{i}B) - F(a + b(\\text{logPart}_i - .5\\delta) + x_{i}B)\\right)\\] An alternative approach, particularly for continuous covariates, is to calculate the marginal effect - the partial first derivative of the predicted probabilities with respect to the variable of interest. In this case, it would be: \\[M = \\frac{\\partial \\hat{p}}{\\partial \\text{logPart}} = b\\times f\\left(a + b\\text{logPart}_{h} + x_hB\\right)\\] where \\(f()\\) is the PDF of the appropriate distribution (logistic for logit or normal for probit) and the \\(h\\) subscript identifies a hypothetical value (or vector of values). The above would be the MER approach and the AME approach would be: \\[M = \\frac{\\partial \\hat{p}}{\\partial \\text{logPart}} = \\frac{1}{n}\\sum_{i}b\\times f\\left(a + b\\text{logPart}_{i} + x_iB\\right)\\] This finds the slope of the line tangent to the logit curve at the vector of covariate values in the calculation. The marginaleffects package helps us calculate these values. The avg_comparisons() function uses the average marginal effect approach. mod &lt;- glm(arrdum ~ stats::poly(evyy,3) + afam + ny + south + logpart + propdam + counterd + standard2 + extcont2 + govtarg + stratvar + viold, data=dat, family=binomial) marginaleffects::avg_comparisons(mod,variables=&quot;logpart&quot;,comparison=&quot;dydx&quot;) ## ## Estimate Std. Error z Pr(&gt;|z|) S 2.5 % 97.5 % ## -0.0255 0.00334 -7.63 &lt;0.001 45.3 -0.0321 -0.019 ## ## Term: logpart ## Type: response ## Comparison: dY/dX You Try It! Use the methods discussed above to understand the substantive impact of the variables in the inews model. 4.3 Marginal Effect Plots Marginal effect plots work the same way here as they do in linear models. We could either use the effects package or the ggeffects package to make predictions and plot them. We will focus on the ggeffects package. Let’s look at the effect of logpart. library(ggeffects) library(ggplot2) g &lt;- ggpredict(mod, terms=&quot;logpart [n=50]&quot;) g2 &lt;- marginaleffects::predictions(mod,newdata=marginaleffects::datagrid(logpart=seq(0,11.51293,0.1))) ggplot(g) + geom_ribbon(aes(x=x, y=predicted, ymin=conf.low, ymax=conf.high), alpha=.25) + geom_line(aes(x=x, y=predicted)) + geom_rug(data=dat, aes(x=logpart), sides=&quot;b&quot;, alpha=.25) + theme_bw() + labs(x = &quot;log(Participation)&quot;, y = &quot;Predicted Pr(Arrest)&quot;) ggplot(g2) + geom_ribbon(aes(x=logpart, y=estimate, ymax=conf.high, ymin=conf.low),alpha=.25) + geom_line(aes(x=logpart, y=estimate)) + geom_rug(data=dat, aes(x=logpart), sides=&quot;b&quot;, alpha=.25) + theme_bw() + labs(x = &quot;log(Participation)&quot;, y = &quot;Predicted Pr(Arrest)&quot;) If you prefer the average marginal effect approach, there are fewer options for making a marginal effect plot. The aveEffPlot() function in the DAMisc package will do this. By default, the function will make the plot for you using R’s base graphics package. You can also use the argument plot=FALSE and returnSim=TRUE and then the data for plotting will be returned. ap &lt;- aveEffPlot(mod, data=dat, varname = &quot;logpart&quot;, nvals = 35, plot=FALSE, returnSim=TRUE) ap2 &lt;- marginaleffects::avg_predictions(mod,variables=list(logpart=seq(0,11.51293,0.1))) ggplot(ap$ci) + geom_ribbon(aes(x=s, y=mean, ymin=lower, ymax=upper), alpha=.15) + geom_line(aes(x=s, y=mean)) + geom_rug(data=dat, aes(x=logpart), sides=&quot;b&quot;, alpha=.25) + theme_bw() + labs(x = &quot;log(Participation)&quot;, y = &quot;Predicted Pr(Arrest)&quot;) ggplot(ap2) + geom_ribbon(aes(x=logpart, y=estimate, ymax=conf.high, ymin=conf.low), alpha=.25) + geom_line(aes(x=logpart, y=estimate)) + geom_rug(data=dat, aes(x=logpart), sides=&quot;b&quot;, alpha=.25) + theme_bw() + labs(x = &quot;log(Participation)&quot;, y = &quot;Predicted Pr(Arrest)&quot;) Let’s look at both of the effects together. g &lt;- g %&gt;% as.data.frame() %&gt;% select(x, predicted, conf.low, conf.high) %&gt;% mutate(type = factor(1, levels=1:2, labels=c(&quot;MER&quot;, &quot;AME&quot;))) ap$ci %&gt;% rename(&quot;x&quot; = &quot;s&quot;, &quot;predicted&quot; = &quot;mean&quot;, &quot;conf.low&quot; = &quot;lower&quot;, &quot;conf.high&quot; = &quot;upper&quot;) %&gt;% mutate(type = factor(2, levels=1:2, labels=c(&quot;MER&quot;, &quot;AME&quot;))) %&gt;% bind_rows(g, .) %&gt;% ggplot(aes(x=x, y=predicted)) + geom_ribbon(aes(ymin=conf.low, ymax=conf.high, fill=type), alpha=.15, col=&quot;transparent&quot;) + geom_line(aes(colour=type)) + theme_bw() + theme(legend.position = &quot;top&quot;) + labs(x=&quot;log(Participation)&quot;, y=&quot;Predicted Arrests&quot;, colour = &quot;Effect Type&quot;, fill = &quot;Effect Type&quot;) g2 &lt;- g2 %&gt;% as.data.frame() %&gt;% select(logpart, estimate, conf.low, conf.high) %&gt;% mutate(type = factor(1, levels=1:2, labels=c(&quot;MER&quot;, &quot;AME&quot;))) ap2 %&gt;% as.data.frame() %&gt;% select(logpart, estimate, conf.low, conf.high) %&gt;% mutate(type = factor(2, levels=1:2, labels=c(&quot;MER&quot;, &quot;AME&quot;))) %&gt;% bind_rows(g2, .) %&gt;% ggplot(aes(x=logpart, y=estimate)) + geom_ribbon(aes(ymin=conf.low, ymax=conf.high, fill=type), alpha=.15, col=&quot;transparent&quot;) + geom_line(aes(colour=type)) + theme_bw() + theme(legend.position = &quot;top&quot;) + labs(x=&quot;log(Participation)&quot;, y=&quot;Predicted Arrests&quot;, colour = &quot;Effect Type&quot;, fill = &quot;Effect Type&quot;) We could also plot this by two different conditions (say the effect of year for afam=“No”, and afam=“Yes”). library(ggeffects) g &lt;- ggpredict(mod, terms=c(&quot;evyy [n=50]&quot;, &quot;afam&quot;)) g2 &lt;- marginaleffects::predictions(mod, marginaleffects::datagrid(evyy = seq(1960,1990,0.5), afam = c(&quot;No&quot;,&quot;Yes&quot;))) ggplot(g) + geom_ribbon(aes(x=x, y=predicted, ymin=conf.low, ymax=conf.high, fill=group), alpha=.25, col=&quot;transparent&quot;) + geom_line(aes(x=x, y=predicted, colour=group)) + theme_bw() + theme(legend.position=&quot;top&quot;) + labs(x = &quot;Year&quot;, y = &quot;Predicted Pr(Arrest)&quot;, colour=&quot;African American\\nProtest&quot;, fill=&quot;African American\\nProtest&quot;) ggplot(g2) + geom_ribbon(aes(x=evyy, y=estimate, ymin=conf.low, ymax=conf.high, fill=afam), alpha=.25, col=&quot;transparent&quot;) + geom_line(aes(x=evyy, y=estimate, colour=afam)) + theme_bw() + theme(legend.position=&quot;top&quot;) + labs(x = &quot;Year&quot;, y = &quot;Predicted Pr(Arrest)&quot;, colour=&quot;African American\\nProtest&quot;, fill=&quot;African American\\nProtest&quot;) As you can see, African American protests were more likely to experience arrests than primarily white protests over the entire period, though the gap seems to close with both groups seeing more arrests toward the end of the period in the late 1980s. You can see that the difference between the two is bigger at the beginning of the period than later in the period. This is due to the effects of “compression” which brings up the debate about how interactions work in non-linear models like this. We turn to that below. You Try It! Plot the effect of sei10 and age for different values of college_ed. - How do the effects differ based on different methods? 4.4 Interactions in Binary DV Models Interactions in binary DV models are more complicated than in linear models. In addition to the general added complexity - there is some debate about a) the right method for calculating the effect and b) whether or not a product term is required. We will discuss the different methods for dealing with the first issue below. As for whether a product term is required initially hinged on whether the user believed that “compression” indicating an interesting interaction. Compression is the phenomenon where marginal effects get smaller in the extremes of the distribution. If \\(Pr(y=1) = .99\\) already, then the positive effect of any variable is constrained to a really small value (i.e., can’t be more than .01 in terms of a first difference). Those same sorts of constraints are less interesting when \\(Pr(y=1) = .5\\), where there is lots of room for a variable to have a bigger effect. The two schools of thought are Compression is an artifact of the model and as such doesn’t represent a statistically interesting phenomenon. To model a truly conditional relationship, a product term is required (e.g., Bartels). Compression does, in fact, identify a certain kind of interaction and as such is statistically meaningful (e.g., Berry, Demeritt and Esarey). These two sides of the debate are extremes with the right answer probably being somewhere in the middle. For example, without a product term, we showed above that the marginal effect of a variable \\(x_j\\) is: \\[\\frac{\\partial \\hat{p}}{\\partial x_j} = b_j\\times f\\left(a + bx_j + x_hB\\right)\\] Note that since the \\(f(\\cdot)\\) is the evaluation of a probability density function, it’s values are non-negative. So if \\(b_j\\) is positive, the equation above could not possibly generate a negative effect. I could generate differently sized positive effects, which would be one form of interaction, but the effect could not be negative. So, to have conditional effects that switch signs, you would need a product term in the model. Carlisle Rainey has an interesting article suggesting, counterintuitively, that including the product term actually allows the model to look more like an additive model by mitigating the effects of compression if the patterns in the data are such. So, here, the inclusion of the product term is not only to “allow for” a conditional effect, it’s also to allow for a more nearly additive effect. All of this is to say that you can include a product term or not and still potentially have interesting interactivity. Those questions should be addressed by following the procedures described below. In what follows, we’ll consider three different scenarios. In each scenario, I will simulate some data to demonstrate the method’s properties. Both variables in the interaction are dummy variables. One variable in the interaction is continuous and one is binary. Both variables in the interaction are continuous. 4.4.1 Both binary Let’s make some data first and then estimate the model. set.seed(1234) df1 &lt;- tibble( x1 = as.factor(rbinom(1000, 1, .5)), x2 = as.factor(rbinom(1000, 1, .4)), z = rnorm(1000), ystar = 0 + as.numeric(x1 == &quot;1&quot;) - as.numeric(x2 == &quot;1&quot;) + 2*as.numeric(x1==&quot;1&quot;)*as.numeric(x2==&quot;1&quot;) + z, p = plogis(ystar), y = rbinom(1000, 1, p) ) mod1 &lt;- glm(y ~ x1*x2 + z, data=df1, family=binomial) The Norton, Wang and Ai discussion suggests taking the discrete double-difference of the model above with respect to x1 and x2. This is just the probability where x1 and x2 are equal to 1, minus the probability where x1 is 1 and x2 is 0 minus the probability where x2 is 1 and x1 is 0 plus the probability where x1 and x2 are both 0. We could calculate this “by hand” as: ## make the model matrix for all conditions X11 &lt;- X10 &lt;- X01 &lt;- X00 &lt;- model.matrix(mod1) ## set the conditions for each of the four different ## scenarios above ## x1 = 1, x2=1 X11[,&quot;x11&quot;] &lt;- X11[,&quot;x21&quot;] &lt;- X11[,&quot;x11:x21&quot;] &lt;- 1 ## x1=1, x2=0 X10[,&quot;x11&quot;] &lt;- 1 X10[,&quot;x21&quot;] &lt;- X10[,&quot;x11:x21&quot;] &lt;- 0 ## x1=0, x2=1 X01[,&quot;x21&quot;] &lt;- 1 X01[,&quot;x11&quot;] &lt;- X01[,&quot;x11:x21&quot;] &lt;- 0 ## x1=0, x2=0 X00[,&quot;x11&quot;] &lt;- X00[,&quot;x21&quot;] &lt;- X00[,&quot;x11:x21&quot;] &lt;- 0 ## calculate the probabilities p11 &lt;- plogis(X11 %*% coef(mod1)) p10 &lt;- plogis(X10 %*% coef(mod1)) p01 &lt;- plogis(X01 %*% coef(mod1)) p00 &lt;- plogis(X00 %*% coef(mod1)) eff1 &lt;- p11 - p10 - p01 + p00 This is just what the intEff() function does. i1 &lt;- intEff(mod1, c(&quot;x1&quot;, &quot;x2&quot;), df1) The byob$int element of the i1 object above gives the interaction effect, particularly the first column. We can just plot that relative to the effect calculated above to see that they’re the same. library(ggplot2) tibble(e1 = eff1, i1 = i1$byobs$int$int_eff) %&gt;% ggplot(mapping= aes(x=e1, y=i1)) + geom_point(pch=1) + theme_bw() + labs(x=&quot;Calculated by Hand&quot;, y=&quot;intEff Function Output&quot;) So, the byobs list has two elements - the int element holds the interaction effects for each individual observation. The X element holds the original data. These data were used to calculate the interaction effect, except that the variables involved in the interaction effect were changed as we did above. Here, you could plot a histogram of the effects: i1$byobs$int %&gt;% ggplot(mapping=aes(x=int_eff)) + geom_histogram() + theme_bw() + labs(x=&quot;Interaction Effect&quot;) In this case, all of the effects are significant, but you could also break these out by significant and not significant effects: i1$byobs$int %&gt;% mutate(sig = ifelse(abs(i1$byobs$int$zstat) &gt; 1.96, 1, 0), sig = factor(sig, levels=c(0,1), labels=c(&quot;No&quot;, &quot;Yes&quot;))) %&gt;% ggplot(mapping=aes(x=int_eff)) + geom_histogram() + theme_bw() + facet_wrap(~sig) + labs(x=&quot;Interaction Effect&quot;) The function avg_predictions in the marginaleffects package does this more generally. This function calculates second differences at user-defined values. and summarises all of the individual second differences like those created above. marginaleffects::avg_predictions(mod1, variables=list(x1=c(0,1), x2=c(0,1)), hypothesis = &quot;(b2-b1)-(b4-b3)=0&quot;) ## ## Hypothesis Estimate Std. Error z Pr(&gt;|z|) S 2.5 % 97.5 % Df ## (b2-b1)-(b4-b3)=0 -0.32 0.0548 -5.84 &lt;0.001 27.5 -0.427 -0.212 Inf ## ## Type: response These results all tell us about the change in probability when x2 changes from 0 to 1 under two conditions one when x1 is 1 and one when it is 0. For example, the first row of the byobs$int element of the output from intEff(): i1$byobs$int[1,] ## int_eff linear phat se_int_eff zstat ## 1 0.3584801 0.2141966 0.1384038 0.06166805 5.813061 suggests that for the first observation, as x2 changes from 0 to 1, the first difference is .35 higher when x1 is 1 than when x1 is 0. The atmean element of i1 shows what this difference is at the average of all of the covariates: i1$atmean ## [[1]] ## int_eff linear phat se_int_eff zstat ## 1 0.3446747 0.4126915 0.6422852 0.05928781 5.813584 ## ## $X ## (Intercept) x11 x21 z x11:x21 ## [1,] 1 0.518 0.381 0.01450824 0.21 4.4.2 One binary, one continuous. With one binary and one continuous variable, the Norton, Wang and Ai model would have us calculate the slope of the line tangent to the logit curve for the continuous variable at both of the values of the categorical variable. First, let’s make the data and run the model: set.seed(1234) df2 &lt;- tibble( x2 = as.factor(rbinom(1000, 1, .5)), x1 = runif(1000, -2,2), z = rnorm(1000), ystar = 0 + as.numeric(x2 == &quot;1&quot;) - x1 + .75*as.numeric(x2==&quot;1&quot;)*x1 + z, p = plogis(ystar), y = rbinom(1000, 1, p) ) mod2 &lt;- glm(y ~ x1*x2 + z, data=df2, family=binomial) Norton, Wang and Ai show that the interaction effect is the difference in the first derivatives of the probability with respect to x1 when x2 changes from 0 to 1. In the following model: \\[\\begin{aligned} log\\left(\\frac{p_i}{1-p_{i}}\\right) &amp;= u_i\\\\ log\\left(\\frac{p_i}{1-p_{i}}\\right) &amp;= b_0 + b_1x_{1i} + b_2x_{2i} + b_3x_{1i}x_{2i} + \\mathbf{Z\\theta}, \\end{aligned}\\] The first derivative of the probability with respect to x1 when `x2 is equal to 1 is: \\[\\frac{\\partial F(u_i)}{\\partial x_{1i}} = (b_1 + b_3)f(u_i)\\] where \\(F(u_i)\\) is the predicted probability for observation \\(i\\) (i.e., the CDF of the logistic distribution evaluated at \\(u_i\\)) and \\(f(u_{i})\\) is the PDF of the logistic distribution distribution evaluated at \\(u_i\\). We could also calculate this for the condition when x2 = 0: \\[\\frac{\\partial F(u_i)}{\\partial x_{1i}} = b_1f(u_i)\\] In both cases, this assumes that the values of \\(\\mathbf{x}_{i}\\) are consistent with the condition. For example in the first partial derivative above, \\(x_{2i}\\) would have to equal 1 and in the second partial derivative, \\(x_{2i}\\) would have to equal zero. We could do this by hand just to see how it works: X0 &lt;- X1 &lt;- model.matrix(mod2) ## set the conditions for each of the four different ## scenarios above ## x1 = 1, x2=1 X1[,&quot;x21&quot;] &lt;- 1 X1[,&quot;x1:x21&quot;] &lt;- X1[,&quot;x1&quot;] ## x1=1, x2=0 X0[,&quot;x21&quot;] &lt;- 0 X0[,&quot;x1:x21&quot;] &lt;- 0 b &lt;- coef(mod2) ## print the coefficients to show that the two coefficients ## we want are the second and fifth ones. b ## (Intercept) x1 x21 z x1:x21 ## 0.3414514 -0.8653259 0.6186529 0.8542471 0.6109292 ## calculate the first effect e1 &lt;- (b[2] + b[5])*dlogis(X1 %*% b) ## calculate the second effect e2 &lt;- (b[2] )*dlogis(X0 %*% b) ## calculate the probabilities eff2 &lt;- e1 - e2 Just like before, we can also estimate the same effect with intEff() and show that the two are the same. i2 &lt;- intEff(mod2, c(&quot;x1&quot;, &quot;x2&quot;), df2) ggplot(mapping=aes(y = i2$byobs$int[,1], x=eff2)) + geom_point(pch=1) + theme_bw() + labs(x=&quot;Calculated by hand&quot;, y= &quot;intEff Output&quot;) Looking at the first line of the output of i2$byobs$int, i2$byobs$int[1,] ## int_eff linear phat se_int_eff zstat ## 1 0.04013175 0.07137251 0.1350701 0.0233773 1.716697 We see that the slope of the line tangent to the logit curve when x1 takes on the value of the first observation (1.350536) is 0.04 higher when x2 = 1 than when x2 = 0. We could visualize this as in the figure below. The solid lines are the logit curves and the dotted lines are the lines tangent to the curves at \\(x_1 = 1.350536\\). The slope of the blue dotted line is -0.060961 and the slope of the orange dotted line is -0.1010927. You can see that the difference 0.0401317 is the first entry in the int_eff column displayed above. tmpX1 &lt;- X1[rep(1, 51), ] tmpX0 &lt;- X0[rep(1, 51), ] tmpX1[,&quot;x1&quot;] &lt;- tmpX1[,&quot;x1:x21&quot;] &lt;- c(seq(-2, 2, length=50), 1.350536) tmpX0[,&quot;x1&quot;] &lt;- c(seq(-2, 2, length=50), 1.350536) tmpX0[,&quot;x1:x21&quot;] &lt;- 0 phat1 &lt;- plogis(tmpX1 %*% b) phat0 &lt;- plogis(tmpX0 %*% b) plot.df &lt;- tibble(phat = c(phat0[1:50], phat1[1:50]), x = rep(seq(-2,2,length=50), 2), x2 = factor(rep(c(0,1), each=50), levels=c(0,1), labels=c(&quot;x2 = 0&quot;, &quot;x2 = 1&quot;))) yint1 &lt;- phat1[51] - e1[1]*tmpX1[51, &quot;x1&quot;] yint0 &lt;- phat0[51] - e2[1]*tmpX0[51, &quot;x1&quot;] plot.df %&gt;% ggplot(aes(x=x, y=phat, colour = x2)) + geom_line() + scale_colour_manual(values=c(&quot;#0072B2&quot;, &quot;#D55E00&quot;)) + geom_abline(slope=e1[1], intercept=yint1, colour=&quot;#D55E00&quot;, lty=3) + geom_abline(slope=e2[1], intercept=yint0, colour=&quot;#0072B2&quot;, lty=3) + theme_bw() + labs(x=&quot;x1&quot;, y=&quot;Predicted Probability of y=1&quot;, colour=&quot;x2&quot;) From the zstat entry, we see that the effect is not significant. We can plot the effects by significance. i2$byobs$int %&gt;% mutate(sig = ifelse(abs(zstat) &gt; 1.96, 1, 0), sig = factor(sig, levels=c(0,1), labels=c(&quot;No&quot;, &quot;Yes&quot;))) %&gt;% ggplot(mapping=aes(x=int_eff)) + geom_histogram() + theme_bw() + facet_wrap(~sig) + labs(x=&quot;Interaction Effect&quot;) + theme(aspect.ratio=1) Another option here is to do a second difference. Instead of looking at the difference in the slope of the line tangent to the curve for x1, it looks at how the effect of a discrete change in x1 differs across two values of x2. Using the minimum and maximum as the two values to make the change for x1 is the default. But let’s say that we wanted to see how changing x1 from -2 to -1 change the predicted probability of success for x2=0 and x2=1. The result here is the first \\[\\begin{aligned} &amp; \\left\\{Pr(y=1|x_1=\\text{high}, x_2=\\text{high}) - Pr(y=1|x_1=\\text{low}, x_2=\\text{high})\\right\\} - \\\\ &amp; \\left\\{Pr(y=1|x_1=\\text{high}, x_2=\\text{low}) - Pr(y=1|x_1=\\text{low}, x_2=\\text{low})\\right\\} \\end{aligned}\\] marginaleffects::avg_predictions(mod2, variables=list(x1=c(-2,-1), x2=c(0,1)), hypothesis = &quot;(b4-b3)-(b2-b1)=0&quot;) ## ## Hypothesis Estimate Std. Error z Pr(&gt;|z|) S 2.5 % 97.5 % Df ## (b4-b3)-(b2-b1)=0 0.0792 0.0148 5.36 &lt;0.001 23.5 0.0503 0.108 Inf ## ## Type: response The output shows that the change in probabilities is bigger when x2 is low than when x2 is high. This corroborates what we saw in the plot above. 4.4.3 Both Continuous With two continuous variables using this model: \\[\\begin{aligned} log\\left(\\frac{p_i}{1-p_{i}}\\right) &amp;= u_i\\\\ log\\left(\\frac{p_i}{1-p_{i}}\\right) &amp;= b_0 + b_1x_{1i} + b_2x_{2i} + b_3x_{1i}x_{2i} + \\mathbf{Z\\theta}, \\end{aligned}\\] Norton, Wang and Ai show that the cross-derivative, rate of change in the probabilities as a function of x1 changes as the rate of change in x2 changes. \\[\\frac{\\partial^2 F(u_{i})}{\\partial x_{1i} \\partial x_{2i}} = b_3f(u_i) + (b1 + b_3x_{2i})(b_2+b_3x_{1i})f&#39;(u_i)\\] where \\(f&#39;(u_i) = f(u_i)\\times (1-2F(u_{i}))\\). We could calculate this “by hand” as: set.seed(1234) df3 &lt;- tibble( x2 = runif(1000, -2,2), x1 = runif(1000, -2,2), z = rnorm(1000), ystar = 0 + as.numeric(x2 == &quot;1&quot;) - x1 + .75*as.numeric(x2==&quot;1&quot;)*x1 + z, p = plogis(ystar), y = rbinom(1000, 1, p) ) mod3 &lt;- glm(y ~ x1*x2 + z, data=df3, family=binomial) X &lt;- model.matrix(mod3) b &lt;- coef(mod3) e3 &lt;- b[5]*dlogis(X%*% b) + (b[2] + b[5]*X[,&quot;x2&quot;])*(b[3] + b[5]*X[,&quot;x1&quot;])*dlogis(X%*%b)*(1-(2*plogis(X %*% b))) i3 &lt;- intEff(mod3, c(&quot;x1&quot;, &quot;x2&quot;), data=df3) We can content ourselves that these are the same: e3[1] ## [1] 0.002296147 i3$byobs$int[1,] ## int_eff linear phat se_int_eff zstat ## 1 0.002296147 -0.01145364 0.1248453 0.005366089 0.4278994 So, the effects are the cross-derivative with respect to both x1 and x2. I don’t find this to be a particularly intuitive metric, though we can consider the significance of these values and look at their distribution. I find the second difference metric to be a bit more intuitive because it still gives the difference in change in predicted probabilities for a change in another variable. For example, we could do: marginaleffects::avg_predictions(mod3, variables=list(x1=c(-1,0), x2=c(-2,2)), hypothesis = &quot;(b4-b3)-(b2-b1)=0&quot;) ## ## Hypothesis Estimate Std. Error z Pr(&gt;|z|) S 2.5 % 97.5 % Df ## (b4-b3)-(b2-b1)=0 -0.0832 0.0451 -1.85 0.065 3.9 -0.172 0.00517 Inf ## ## Type: response This shows us what happens when we change x1 from -1 to 0 for the two conditions - when x2 is at its minmum versus its maximum. We see that on average the second difference is around -.08 and about 20% of the differences are statistically significant. The average second difference is not, itself, significant. The -0.083 number means the same thing here as it did above. It’s the difference in the first difference of x1 when x2 is high and when x2 is low. Now, let’s move back to the interaction of year and African American status that we discussed above. We can estimate the model with an interaction between the year polynomial and afam and then look at the analysis of deviance table to evaluate the improvement in fit. mod2 &lt;- glm(arrdum ~ stats::poly(evyy,3)*afam + ny + south + logpart + propdam + counterd + standard2 + extcont2 + govtarg + stratvar + viold, data=dat, family=binomial) car::Anova(mod2) ## Analysis of Deviance Table (Type II tests) ## ## Response: arrdum ## LR Chisq Df Pr(&gt;Chisq) ## stats::poly(evyy, 3) 33.730 3 2.259e-07 *** ## afam 22.074 1 2.623e-06 *** ## ny 51.173 1 8.456e-13 *** ## south 15.001 1 0.0001074 *** ## logpart 58.439 1 2.097e-14 *** ## propdam 6.199 1 0.0127846 * ## counterd 19.643 1 9.333e-06 *** ## standard2 2.998 1 0.0833841 . ## extcont2 0.388 1 0.5331956 ## govtarg 22.367 1 2.252e-06 *** ## stratvar 34.081 1 5.287e-09 *** ## viold 5.657 1 0.0173847 * ## stats::poly(evyy, 3):afam 65.918 3 3.192e-14 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Note here that the interaction regressors are jointly significant (as evidenced by the last line of the output). We can now look at the second difference. For this to work, we’ll have to use the raw polynomials. dat$evyy2 &lt;- dat$evyy - 1975 mod2 &lt;- glm(arrdum ~ stats::poly(evyy2,3, raw=TRUE)*afam + ny + south + logpart + propdam + counterd + standard2 + extcont2 + govtarg + stratvar + viold, data=dat, family=binomial) Let’s see what the effect looks like here: library(ggeffects) g &lt;- ggpredict(mod2, terms=c(&quot;evyy2&quot;, &quot;afam&quot;)) ggplot(g) + geom_ribbon(aes(x=x+1975, y=predicted, ymin=conf.low, ymax=conf.high, fill=group), alpha=.25, col=&quot;transparent&quot;) + geom_line(aes(x=x+1975, y=predicted, colour=group)) + theme_bw() + theme(legend.position=&quot;top&quot;) + labs(x = &quot;Year&quot;, y = &quot;Predicted Pr(Arrest)&quot;, colour=&quot;African American\\nProtest&quot;, fill=&quot;African American\\nProtest&quot;) We can look at the output of the second difference function, too: marginaleffects::avg_predictions(mod2, variables=list(evyy2=c(-15,15), afam=c(&quot;No&quot;,&quot;Yes&quot;)), hypothesis = &quot;(b2-b1)-(b4-b3)=0&quot;) ## ## Hypothesis Estimate Std. Error z Pr(&gt;|z|) S 2.5 % 97.5 % Df ## (b2-b1)-(b4-b3)=0 0.428 0.099 4.32 &lt;0.001 16.0 0.233 0.622 Inf ## ## Type: response The average second difference here is .428, indicating: \\[\\begin{eqnarray}\\left(Pr(\\text{Arrest}|\\text{AfAm=Y}, 1990) - Pr(\\text{Arrest}|\\text{AfAm=N}, 1990)\\right) &amp;-&amp; \\\\ \\left(Pr(\\text{Arrest}|\\text{AfAm=Y}, 1960) - Pr(\\text{Arrest}|\\text{AfAm=N}, 1960)\\right) &amp;&gt;&amp; 0 \\end{eqnarray}\\] Thus, there is a significant interaction here of a quite large magnitude. If we had looked at the second difference for the model without the product regressors, here’s what we would have seen. mod1a &lt;- glm(arrdum ~ stats::poly(evyy2,3, raw=TRUE) + afam + ny + south + logpart + propdam + counterd + standard2 + extcont2 + govtarg + stratvar + viold, data=dat, family=binomial) marginaleffects::avg_predictions(mod1a, variables=list(evyy2=c(-15,15), afam=c(&quot;No&quot;,&quot;Yes&quot;)), hypothesis = &quot;(b4-b3)-(b2-b1)=0&quot;) ## ## Hypothesis Estimate Std. Error z Pr(&gt;|z|) S 2.5 % 97.5 % Df ## (b4-b3)-(b2-b1)=0 -0.0165 0.00563 -2.93 0.00342 8.2 -0.0275 -0.00545 Inf ## ## Type: response The significant negative interaction is a function of the constraints placed on the model by only having additive regressors and letting compression do the work. The fact that the model with the product regressors fits better means that the positive interaction effect is the one we should pay attention to. You Try It! Let’s say that we wanted to evaluate whether or not there is an interaction between age and college_ed. - Do you need a product term? - Is there a significant second difference? - What does the effect look like? The probci() function in the DAMisc package will calculate AMEs for any number of variables changing at the same time. Here is the code we could use: pci &lt;- probci(mod2, data=dat, changeX = c(&quot;evyy2&quot;, &quot;afam&quot;), xvals = list(&quot;evyy2&quot; = seq(-15, 15, length=12)), returnProbs=FALSE) Now, we could use the plot.data element of the pci object to make a plot: pci$plot.data %&gt;% setNames(c(&quot;evyy2&quot;, &quot;afam&quot;, &quot;pred_prob&quot;, &quot;lower&quot;, &quot;upper&quot;)) %&gt;% ggplot(aes(x=evyy2+1975, y=pred_prob, colour = afam)) + geom_ribbon(aes(ymin =lower, ymax=upper, fill=afam), alpha=.1, col=&quot;transparent&quot;) + geom_line() + theme_bw() + theme(legend.position = &quot;top&quot;) + labs(x=&quot;Year&quot;, y=&quot;Predicted Pr(Arrest)&quot;, colour=&quot;African American\\nProtest&quot;, fill=&quot;African American\\nProtest&quot;) 4.5 Other GLMs. The methods discussed above generally work for other GLMs as well. The discussion of interactions in other GLMs should have a similar flavor. The methods described above should be general enough to handle any model estimated with the glm() function. Much of the literature on interactions in non-linear models is really based on what happens in logistic regression models, though. That said, it is not obvious why these same ideas would apply differently for other probability distributions and likelihood functions. There is a poisfit() function that returns scalar measures of fit for the Poisson and Negative Binomial models. There is also a poisGOF() function that returns a test of \\(V(y) = \\mu\\) that would identify whether the model has an overdispersed outcome. The glmChange() and glmChange2() function should also work as do the ggeffect package functions. Because the functions perform in similar ways with other GLMs, I will not belabor them here. Exercises Model the relationship between thinking imagination is a good quality in kids (I_IMAGIN) and secular values, including the same controls as for exercise 4 of chapter 3, using a logit model. Evaluate the model’s fit using the binfit and pre functions from the DAMisc package and plot the ROC of the model using the pROC package. Produce odds ratio for the model. Produce the MER and AME for sacsecval using the functions glmChange and glmChange2 from the DAMisc package. Create a graph comparing the MER and AME effects for sacsecval. Create a graph of the MER when we vary sacsecval and age_51p. Now let’s add an interaction between sacsecval and trustInsti in the model. Vizualise this effect for low (0), neutral (0.5), and high (1) trust on the trustInsti scale. "],["ordered-and-multinomial-logit.html", "Chapter 5 Ordered and Multinomial Logit 5.1 Ordinal DV Models 5.2 Multinomial Logit 5.3 Conclusion", " Chapter 5 Ordered and Multinomial Logit There are lots of situations where we have a categorical dependent variable with more than two categories. In this case, either ordered logit (if the categories are inherently ordered) or multinomial logit (if they are un-ordered) is often a suitable choice. These models predict the probability that an observation is in each of the categories. 5.1 Ordinal DV Models In the ordered logit (or probit) model, the idea is that there is a latent continuous variable about which we only observe a discrete realization. That is, we don’t know the observation’s exact value on the underlying latent variable; instead, we know whether it falls between two different threshold parameters. That is, we know whether it falls into the same category with a bunch of other unknown values. The goal of the model is to estimate the regression parameters along with these threshold parameters. These are often called cumulative probability models because the model is estimating the following: \\[Pr(y &lt;= m) = F(\\tau_m - Xb)\\] Which leads to: \\[ Pr(y=m) = F(\\tau_{m} - Xb) - F(\\tau_{m-1} - Xb)\\] where, as above, \\(F()\\) is the CDF of a probability distribution - the logistic for logit or normal for probit. There are a couple of different ways to fit these models. The polr() function in the MASS package is one way. This method will probably respond to post-estimation functions better because it has been around for longer. There is also a package called ordinal that has functions for fitting different kinds of ordered DV models. We will spend a bit more time talking about this package because it has implications for chapters later on in the book. Just know, that if you run into a situation where output from the clm() function doesn’t respond well to a post-estimation technique that you might try the polr() function from MASS. library(ordinal) library(DAMisc) data(france) ologit.mod &lt;- clm(retnat ~ lrself + male + age, data=france) summary(ologit.mod) ## formula: retnat ~ lrself + male + age ## data: france ## ## link threshold nobs logLik AIC niter max.grad cond.H ## logit flexible 542 -566.76 1143.53 5(0) 5.98e-14 5.7e+04 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## lrself -0.154106 0.037345 -4.127 3.68e-05 *** ## male -0.342544 0.162003 -2.114 0.0345 * ## age 0.010258 0.004911 2.089 0.0367 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Threshold coefficients: ## Estimate Std. Error z value ## Better|Same -1.6237 0.2951 -5.503 ## Same|Worse 0.3419 0.2857 1.197 The summary function for the clm object is a bit better because it returns \\(p\\)-values, whereas the summary for the polr object does not. Once we’ve estimated the model, just like above we can see how the model fits. The ordfit() function in the DAMisc package will work on both polr and clm objects. The various peseudo-\\(R^{2}\\) measures are defined as they were in the binary model. ordfit(ologit.mod) ## Estimate ## Count R2 0.465 ## Count R2 (Adj) 0.049 ## ML R2 0.045 ## McFadden R2 0.021 ## McFadden R2 (Adj) 0.013 ## McKelvey &amp; Zavoina R2 0.049 We could also look at the PRE just as we did with the binary model. pre(ologit.mod, data=france, sim=TRUE) ## mod1: retnat ~ lrself + male + age ## mod2: retnat ~ 1 ## ## Analytical Results ## PMC = 0.437 ## PCP = 0.465 ## PRE = 0.049 ## ePMC = 0.353 ## ePCP = 0.367 ## ePRE = 0.021 ## ## Simulated Results ## median lower upper ## PRE 0.033 -0.003 0.062 ## ePRE 0.021 0.004 0.036 We see not an excellent fit here - we improve ever so slightly on the null model, though that’s a pretty low bar. You Try It! Using the GSS 2016 data, estimate and summarise an ordered logit model of sparts on college_ed (which we created in the previous chapter), aid_scale, sex and age. We can use the ggeffects package to plot the fitted values. library(ggeffects) library(ggplot2) g &lt;- ggpredict(ologit.mod, terms=&quot;lrself&quot;) %&gt;% mutate(response.level = factor(response.level, labels=levels(france$retnat))) ggplot(g, aes(x=x, y=predicted, colour=response.level)) + geom_ribbon(aes(ymin=conf.low, ymax=conf.high, fill=response.level), alpha=.15, col=&quot;transparent&quot;) + geom_line() + theme_bw() + theme(legend.position=&quot;top&quot;) + labs(x=&quot;Left-Right Self-Placement&quot;, y = &quot;Predicted Probability&quot;, colour = &quot;Response\\nCategory&quot;, fill = &quot;Response\\nCategory&quot;) In the figure above, the prediction we would get is from the line that has the highest probability. In this case, you see that it’s mostly “Same”, except for when left-right self-placement is at its most left-leaning (i.e., lowest) values - in which case the prediction would be for “Worse”. The “Better” and “Worse” lines basically trade off moving across the left-right spectrum. You Try It! Using the model you estimated above, plot the effect of aid_scale for the three different college_ed groups. The clm function also does the generalized ordered logit model wherein some of the coefficients are allowed to change across response categories. This is essentially a ordinal-multinomial model hybrid. gologit.mod &lt;- clm(retnat ~ male + age, nominal = ~ lrself, data=france) summary(gologit.mod) ## formula: retnat ~ male + age ## nominal: ~lrself ## data: france ## ## link threshold nobs logLik AIC niter max.grad cond.H ## logit flexible 542 -566.20 1144.40 5(0) 3.15e-13 5.8e+04 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## male -0.345173 0.162037 -2.130 0.0332 * ## age 0.010367 0.004912 2.111 0.0348 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Threshold coefficients: ## Estimate Std. Error z value ## Better|Same.(Intercept) -1.78421 0.33463 -5.332 ## Same|Worse.(Intercept) 0.44284 0.30183 1.467 ## Better|Same.lrself 0.18569 0.04792 3.875 ## Same|Worse.lrself 0.13092 0.04290 3.052 It is simple to do a test between the of the generalization: anova(ologit.mod, gologit.mod) ## Likelihood ratio tests of cumulative link models: ## ## formula: nominal: link: threshold: ## ologit.mod retnat ~ lrself + male + age ~1 logit flexible ## gologit.mod retnat ~ male + age ~lrself logit flexible ## ## no.par AIC logLik LR.stat df Pr(&gt;Chisq) ## ologit.mod 5 1143.5 -566.76 ## gologit.mod 6 1144.4 -566.20 1.1232 1 0.2892 The plot above uses the MER approach, if you wanted the AME approach, you could use ordAveEffPlot() from the DAMisc package. Just like aveEffPlot(), you can have the function return the data to plot with returnMprob=TRUE and plot=FALSE. oap &lt;- ordAveEffPlot(ologit.mod, &quot;lrself&quot;, data=france, plot=FALSE, returnMprob=TRUE) ggplot(oap$data, aes(x=s, y=mean, colour=y)) + geom_ribbon(aes(ymin=lower, ymax=upper, fill=y), alpha=.15, col=&quot;transparent&quot;) + geom_line() + theme_bw() + theme(legend.position=&quot;top&quot;) + labs(x=&quot;Left-Right Self-Placement&quot;, y=&quot;Predicted Probability&quot;, colour = &quot;Response Category&quot;, fill = &quot;Response Category&quot;) Note that this looks really similar to the plot we made above. You Try It! Using the ordAveEffPlot() function, replicate the same graph you made in the previous exercise. - How do the two compare to each other? One last thing we could do here is to test one of the important assumptions in this model - the Parallel Regressions Assumption. This assumption states that the effect on the cumulative probabilities is the same for all levels of the dependent variable. If this assumption doesn’t hold, then ordered logit or probit is not the best model. The Brant test is a test of this assumption. There is a package called brant that has this test in it. To use the brant pacakge, you’ll need to re-estimate the model using polr() from the MASS package. ologit.mod2 &lt;- MASS::polr(retnat ~ lrself + male + age, data=france) library(brant) brant(ologit.mod2) ## -------------------------------------------- ## Test for X2 df probability ## -------------------------------------------- ## Omnibus 1.19 3 0.76 ## lrself 0.7 1 0.4 ## male 0.24 1 0.62 ## age 0.16 1 0.69 ## -------------------------------------------- ## ## H0: Parallel Regression Assumption holds You could also do a test much like this with the clm() function. The test below is similar in spirit to the “Omnibus” test above. To my mind, you should only really pay attention to the omnibus test and if it fails, switch to multinomial logit. null.mod &lt;- clm(retnat ~ lrself + male + age, data=france) alt.mod &lt;- clm(retnat ~ 1, nominal = ~ lrself + male + age, data=france) You Try It! Does the parallel regressions assumption hold in your model? 5.2 Multinomial Logit The multinomial logit model allows us to model un-ordered choices or choices that appear ordered by where the parallel regressions assumption doesn’t hold. In the simplest version of the model that we’ll talk about first, we’re modeling: \\[Pr(y=m) = \\frac{e^{XB_j}}{\\sum_{k=1}^{M}e^{XB_k}}\\] where in R, the default is to set \\(B_1 = \\mathbf{0}\\) for identification purposes. That is, the reference category is the first category in the factor. In R, the first category is always the reference level of a factor. It can be changed by rearranging the levels so a different one is the first one. Here, we’re going to use another dataset from France. In this case, we’ll be modeling vote as a function of some demographic covariates and left-right placement variables. First we’ll read in the data and make a factor variable out of vote by using the factorize() function. library(rio) france2 &lt;- import(&quot;data/France_2004.dta&quot;) france2 &lt;- france2 %&gt;% mutate(vote_fac = factorize(vote), demsat = droplevels(factorize(demsat)), eusup = factor(eusup, levels=c(-1,0,1), labels=c(&quot;No&quot;, &quot;Neutral&quot;, &quot;Yes&quot;)), retnat = droplevels(factorize(retnat)), urban = droplevels(factorize(urban)), soclass = droplevels(factorize(soclass))) Just as with the ordered logit, there are two prominent ways of estimating the multinomial logit model. The first is with the multinom() function in the nnet package. This model only allows individual-specific information - it doesn’t do the conditional logit specification that includes choice-specific information. The mlogit() function in the mlogit package does also estimate the conditional logit specification along with the more traditional multinomial logit model and others. library(nnet) mod1 &lt;- multinom(vote_fac ~ age + retnat + demsat + eusup + union + soclass + urban, data=france2, trace=FALSE) The summary function for multinom objects is not that helpful because it gives a matrix of coefficients and separately a matrix of standard errors. The mnlSig() function in the DAMisc package helps by printing the matrix of coefficients with significance flags. noquote(t(mnlSig(mod1))) ## PS Green UDF UMP* FN ## (Intercept) 3.055* 0.980 -0.025 2.608 -11.154* ## age -0.021* -0.046* 0.009 0.009 -0.005 ## retnatsame -1.020 -1.278 -2.126 -1.905 -1.129 ## retnatworse -1.053 -1.732 -2.592* -2.950* -1.569 ## demsatassez satisfait 0.359 1.502 -0.136 -0.369 12.661* ## demsatpeu satisfait 0.720 2.013 0.223 -0.227 13.690* ## demsatpas satisfait du tout 0.213 2.293* -0.878 -1.259 13.627* ## eusupNeutral 0.409 0.603 1.345 -0.207 -0.133 ## eusupYes 1.452* 2.494* 2.258* 0.780 -0.367 ## union -1.538* -1.371* -1.988* -2.026* -2.228* ## soclassclasse moyenne inférieure 1.312* 1.515* 2.188* 1.983* 0.848 ## soclassclasse moyenne 1.115* 0.974* 2.227* 2.293* 0.836 ## soclassclasse moyenne supérieure 2.062 2.515* 3.416* 3.199* 0.421 ## soclassclasse favorisée 14.800* 15.354* 16.493* 16.669* 15.305* ## urbandans une ville petite ou moyenne -0.331 -0.553 -0.447 -0.528 0.055 ## urbandans une grande ville -0.319 -1.417* -0.464 -0.409 -0.101 We could also do this with the mlogit() function, but first we have to turn the data into a format amenable for the function with mlogit.data(). library(mlogit) mldat &lt;- mlogit.data(france2, shape=&quot;wide&quot;, choice=&quot;vote_fac&quot;) mlmod &lt;- mlogit(vote_fac ~ 1 | age + retnat + demsat + eusup + union + soclass + urban, data=mldat) You Try It! Estimate the same model as above, but as a multinomial logit model instead. Summarise the model and consider the differences. Note that we will focus on this format later on, but for now, we’ll use the multinom() function because it plays better with the post-estimation tools we want to use. If we wanted to generate scalar measures of fit for the mode, we could use the mnlfit() function from the DAMisc package. mnlfit(mod1) ## $result ## Estimate p-value ## Fagerland, Hosmer and Bonfi 39.58523646 0.4887696 ## Count R2 0.56634304 NA ## Count R2 (Adj) 0.12418301 NA ## ML R2 0.33441756 NA ## McFadden R2 0.14452618 NA ## McFadden R2 (Adj) 0.07865403 NA ## Cragg-Uhler(Nagelkerke) R2 0.35568788 NA ## ## attr(,&quot;class&quot;) ## [1] &quot;mnlfit&quot; We could also calculate the proportional reduction in error: pre(mod1, sim=TRUE) ## mod1: vote_fac ~ age + retnat + demsat + eusup + union + soclass + urban ## mod2: vote_fac ~ 1 ## ## Analytical Results ## PMC = 0.505 ## PCP = 0.566 ## PRE = 0.124 ## ePMC = 0.322 ## ePCP = 0.394 ## ePRE = 0.105 ## ## Simulated Results ## median lower upper ## PRE 0.094 0.063 0.120 ## ePRE 0.093 0.067 0.119 The effects plot look a lot like those for the ordered logit model. The ggpredict() function will generate predictions, but it doesn’t generate standard errors or confidence bounds. Instead, we could use the mnlAveEffPlot() function from the DAMisc package if we wanted a plot with confidence intervals. You Try It! How does your multinomial logit model fit? mne &lt;- mnlAveEffPlot(mod1, &quot;age&quot;, france2, nvals = 15, plot=FALSE) ggplot(mne, aes(x=s, y=mean, colour=y)) + geom_ribbon(aes(ymin=lower, ymax=upper, fill=y), alpha=.15, col=&quot;transparent&quot;) + geom_line() + theme_bw() + theme(legend.position=&quot;top&quot;) + labs(x=&quot;Age&quot;, y = &quot;Predicted Probability&quot;, colour = &quot;Response\\nCategory&quot;, fill = &quot;Response\\nCategory&quot;) If the superposition of the lines makes it too difficult to see, you could use faceting. ggplot(mne, aes(x=s, y=mean)) + geom_ribbon(aes(ymin=lower, ymax=upper), alpha=.15, col=&quot;transparent&quot;) + geom_line() + theme_bw() + theme(legend.position=&quot;top&quot;) + labs(x=&quot;Age&quot;, y = &quot;Predicted Probability&quot;) + facet_wrap(~y) You Try It! Plot the effect of aid_scale for men and women. There is another version of the MNL model, called the conditional logit model, where you can include both individual-specific information (like we’ve already done), but you can also include choice-specific information. This modifies the model that we discussed above in the following way: \\[\\begin{aligned} \\eta_{j} &amp;= XBj + z_jd\\\\ Pr(Y=m) &amp;= \\frac{e^{\\eta_j}}{\\sum_{k=1}^{M}e^\\eta_{k}} \\end{aligned}\\] Here \\(B_{j}\\) are the coefficients for each response category that apply to the individual-specific information and \\(d\\) is a single coefficient that applies to the choice-varying information. We’ll use this model to add in information about the individual’s ideological distance to each party. This would help us evaluate models of spatial voting (i.e., that people vote for parties that are ideologically closer to them). This is going to require us to do some modification to the data. First, we need to re-organize the lrX variables where X refers to a party number. These variables measure idiosyncratic (i.e., person-specific) placements of the parties on the ideological spectrum. We need to change those into lr_X where X will now represent the same party abbreviations as the vote variable. france2w &lt;- france2 %&gt;% rename(&quot;lr_PCF&quot; = &quot;lr10&quot;, &quot;lr_PS&quot; = &quot;lr20&quot;, &quot;lr_Green&quot; = &quot;lr50&quot;, &quot;lr_UDF&quot; = &quot;lr70&quot;, &quot;lr_UMP*&quot; = &quot;lr73&quot;, &quot;lr_FN&quot; = &quot;lr80&quot;) mldat2 &lt;- mlogit.data(france2w, choice=&quot;vote_fac&quot;, varying = grep(&quot;lr\\\\_&quot;, names(france2w)), sep=&quot;_&quot;) The mlogit.data() function does all of the necessary reshaping. Now, we need to calculate the distance from each individual to each party. mldat2$dist &lt;- abs(mldat2$lr - mldat2$lrself) Now, we could add that into the model. mlmod2 &lt;- mlogit(vote_fac ~ dist | age + retnat + demsat + eusup + union + soclass + urban, data=mldat2) Now, we come to what is probably the hardest task so far. That is - making an effect plot for distance. This is hard because we can’t just change the values of distance because it’s really a function of lots of variables. As we get farther from some parties, we get closer to others in systematic (deterministic) ways. We have to make sure we represent those appropriately. Let’s first think about using the MER approach. First, we would probably need to find the average of the party placements and the average or mode of the other variables. To do this, we’ll take the first observation’s values. Then, we’ll find the appropriate central values to use instead. tmp &lt;- mldat2[1:6, ] ## numeric variables tmp$age &lt;- median(france2$age) tmp$hhincome &lt;- median(france2$hhincome) tmp$male &lt;- median(france2$male) tmp$union &lt;- median(france2$union) ## factor variables tmp$demsat &lt;- central(france2$demsat) tmp$eusup &lt;- central(france2$eusup) tmp$retnat &lt;- central(france2$retnat) tmp$soclass &lt;- central(france2$soclass) tmp$urban &lt;- central(france2$urban) Next, let’s find the mean of the party placements and replace those in tmp. pty &lt;- mldat2 %&gt;% as.data.frame %&gt;% group_by(alt) %&gt;% summarise(lr = mean(lr)) %&gt;% rename(&quot;party&quot; = &quot;alt&quot;) tmp$lr &lt;- pty$lr Next we need to change the lrself variable, recalculate distance and then calculated predicted probabilities. tmp$lrself &lt;- 1 tmp$dist &lt;- abs(tmp$lr - tmp$lrself) predict(mlmod2, newdata=tmp) ## FN Green PCF PS UDF UMP* ## 0.003081705 0.093902365 0.031763672 0.812112143 0.025329307 0.033810808 We could then do this for lots of different values of lrself and the combine the results. lrs &lt;- seq(1,10, length=25) preds &lt;- NULL for(i in 1:length(lrs)){ tmp$lrself &lt;- lrs[i] tmp$dist &lt;- abs(tmp$lr - tmp$lrself) preds &lt;- rbind(preds, predict(mlmod2, newdata=tmp)) } Now, we can put those predictions into a format that’s amenable to plotting: library(tidyr) preds &lt;- as.data.frame(preds) preds$lrself &lt;- lrs preds &lt;- preds %&gt;% pivot_longer(cols=-lrself, names_to=&quot;party&quot;, values_to=&quot;prob&quot;) Next, we can make a plot: ggplot(preds, aes(colour=party)) + geom_line(aes(x=lrself, y=prob)) + geom_point(data=pty, aes(x=lr, y=-.05)) + theme_bw() + theme(legend.position=&quot;top&quot;) + labs(x=&quot;Left-Right Self-Placement&quot;, y=&quot;Pr(Vote for Party)&quot;) Or, with faceting: ggplot(preds) + geom_line(aes(x=lrself, y=prob)) + geom_point(data=pty, aes(x=lr, y=-.05)) + theme_bw() + facet_wrap(~party) + theme(legend.position=&quot;top&quot;) + labs(x=&quot;Left-Right Self-Placement&quot;, y=&quot;Pr(Vote for Party)&quot;) Now, what we haven’t done is to find the uncertainty around the predictions. This is where things get more challenging. Because the predict function doesn’t return standard errors, we would need to calculate them in a different way if we wanted them. One way to do this would be with a parametric bootstrap. I’ve alluded to this a couple of times, but we’ll put a finer point on it here. 5.2.1 Parametric Bootstrap for Confidence Intervals. Bootstrapping is a resampling technique that is used to generate uncertainty estimates for parameters. There are two main types: Non-parametric bootstrap - resample the original data with replacement and re-estimate the model \\(R\\) times, each time saving the parameter(s) of interest. Then, the distribution of the parameter across the bootstrap samples is an estimate of the sampling distribution. Parametric bootstrap - resampling from a known distribution to induce uncertainty in some quantity of interest. So, the first thing we have to do is draw a bunch of samples from the distribution of the coefficients. B &lt;- MASS::mvrnorm(1000, coef(mlmod2), vcov(mlmod2)) Now, let’s think about how we would generate a prediction for a single draw (i.e., row) of \\(B\\). If we looked into what the mlogit.predict() function looks like, that might give us some sense of what to do. 5.2.1.1 Brief Aside - Generic Methods In R, you will have noticed by now that we can use the predict() function on lots of different objects. You might wonder, how does R know what to do with each differnet kind of object. It knows because objects have different predict methods. For example, the people who wrote the mlogit() function also wrote a function called predict.mlogit() which tells R what to do when the predict() function is called on an object of class mlogit. If you want to see what that looks like, you can do the following: mlogit:::predict.mlogit Note that the use of the three colons (:::) which allow you to see a function not exported from the namespace. The main part of the function that will help us is the following. newobject &lt;- update(object, start = coef(object, fixed = TRUE), data = newdata, iterlim = 0, print.level = 0) What happens here is that the update() function re-estimates the model. The start argument starts the model at the coefficient values for the original model. The data argument provides the new data to the model. Importantly, iterlim = 0 tells the update() function to do zero iterations (that is, not not update beyond the starting values). This then generates a vector of probabilities. We can use this by replacing the argument to start with the draw from the coefficients. pred.dist &lt;- NULL for(i in 1:nrow(B)){ p &lt;- update(mlmod2, start = B[i,], data = tmp, iterlim = 0, print.level = 0) pred.dist &lt;- rbind(pred.dist, p) } This is how it would work for a single value of lrself. To do it for all of the values of lrself we chose above, we would have to put the loop above in another loop. pred.dist &lt;- vector(mode=&quot;list&quot;, length=length(lrs)) for(j in 1:length(lrs)){ for(i in 1:nrow(B)){ tmp$lrself &lt;- lrs[j] tmp$dist &lt;- abs(tmp$lr - tmp$lrself) p &lt;- update(mlmod2, start = B[i,], data = tmp, iterlim = 0, print.level = 0) pred.dist[[j]] &lt;- rbind(pred.dist[[j]], p$probabilities) } } Next, we could calculate the confidence intervals for the predicted probabilities and then reshape for plotting: lower &lt;- t(sapply(pred.dist, function(x)apply(x, 2, quantile, .025))) upper &lt;- t(sapply(pred.dist, function(x)apply(x, 2, quantile, .975))) lower &lt;- lower %&gt;% as.data.frame %&gt;% mutate(lrself = lrs) %&gt;% pivot_longer(cols=-lrself, names_to=&quot;party&quot;, values_to=&quot;lower&quot;) upper &lt;- upper %&gt;% as.data.frame %&gt;% pivot_longer(cols=everything(), names_to=&quot;party&quot;, values_to=&quot;upper&quot;) pred.cis &lt;- lower %&gt;% mutate(upper = upper$upper, prob = preds$prob) To make the plot, we should probably use facets to make it readable. ggplot(pred.cis) + geom_ribbon(aes(x=lrself, ymin=lower, ymax=upper), alpha=.15, col=&quot;transparent&quot;) + geom_line(aes(x=lrself, y=prob)) + geom_point(data=pty, aes(x=lr, y=-.05)) + theme_bw() + facet_wrap(~party) + theme(legend.position=&quot;top&quot;) + labs(x=&quot;Left-Right Self-Placement&quot;, y=&quot;Pr(Vote for Party)&quot;) We could follow this same framework for making any effect plot with confidence bounds from an mlogit object. 5.3 Conclusion Here we have worked through ordinal and multinomial logit models. They have some similar features, though we also discussed the conditional logit model which allows both individual- and choice-specific information in the model. "],["handling-complex-survey-data.html", "Chapter 6 Handling Complex Survey Data 6.1 Frequency Distributions and Contingency Tables 6.2 Summary Statistics 6.3 Estimating Models", " Chapter 6 Handling Complex Survey Data The models we have talked about already assumed a simple random sample. Obviously, simple random sampling does not always reflect how the data were collected, especially for surveys. Surveys often use stratified or cluster random sampling to reduce costs and increase precision. In these settings, the models need to accommodate themselves to this different sampling structure. The survey package in R allows you to specify the particulars of your survey design and then use the survey design object as data to some models that are specifically designed to deal with complex survey data. Here, we’ll work through some of the options. The svydesign() function in the survey package defines your sampling design for the data. There are a few arguments that make this possible. ids are the primary sampling unit (PSU) for cluster-random sampling. If you’re doing doing cluster-random sampling, then use ids=~1 to indicate no PSU. strata are the strata used. The assumption is that the population is put in groups (strata) and that sampling is done within each one of those groups. If you haven’t stratified, then you can use strata=~1. weights are probability weights - the inverse of the probability of being selected into the sample. If instead of probability weights, you are using replicate weights, you should use svrepdesign(). We’re going to use some data from the Canadian Election Study. The weights here account for both province of respondent and phone ownership rates (land line, wireless, both). In this case, the stratum is the province and weights are based on phone ownership typology. load(&quot;data/ces19.rda&quot;) library(survey) d &lt;- svydesign(ids=~1, strata=~province19, weights=~weight_CES, data=ces19) d ## Stratified Independent Sampling design (with replacement) ## svydesign(ids = ~1, strata = ~province19, weights = ~weight_CES, ## data = ces19) You Try It! Using the GSS 2016 data, identify the appropriate survey design object. Note that the weights we are going to use are wtssnr and we are not going to specify the strata argument. 6.1 Frequency Distributions and Contingency Tables Now, we can use the d object as input data to functions meant to deal with survey data. We can start with the descriptive functions. Two of the functions we’ve already looked at work here, too. The xt() and sumStats() functions from the DAMisc package take survey weights objects as input. For example, if we wanted to make a weighted frequency distribution of the pid variable (party id), we could do the following: library(DAMisc) xt(d, &quot;pid&quot;) ## $tab ## $tab[[1]] ## pid Freq ## Lib 32% (704) ## Con 32% (714) ## NDP 18% (401) ## BQ 6% (136) ## Green 9% (192) ## Other/DK 2% (53) ## Total 100% (2,200) ## ## ## $chisq ## NULL ## ## $stats ## list() ## ## attr(,&quot;class&quot;) ## [1] &quot;xt&quot; The frequencies are weighted frequencies rounded to the nearest integer value. We could compare this to the unweighted frequencies: xt(ces19, &quot;pid&quot;) ## $tab ## $tab[[1]] ## Var1 Freq ## Lib 31% (674) ## Con 32% (709) ## NDP 19% (426) ## BQ 5% (115) ## Green 10% (218) ## Other/DK 2% (51) ## Total 100% (2,193) ## ## ## $chisq ## NULL ## ## $stats ## list() ## ## attr(,&quot;class&quot;) ## [1] &quot;xt&quot; The xt() function is just a wrapper to the svytable() function which produces cross-tabulations from survey design objects. For example, we could do the same thing with: svytable(~pid, d) ## pid ## Lib Con NDP BQ Green Other/DK ## 704.02534 714.45985 401.07926 135.62555 191.91618 52.51284 The benefit of the xt() function is more aesthetic than anything in that it takes the information and presents it in a way that is easier to digest (focusing on proportions/percentages, rather than counts). You Try It! Replicate our earlier descriptive analysis with weights. Look at the distribution of two variables we’re going to consider - aidhouse (the government should provide housing to the poor) and partyid (partisan identification). - For now, just look at the univariate distributions. - Make a bar plot of aidhouse and of partyid, each independently. If we wanted to make a contingency table of pid by agegrp (age group), we could do that with xt(). xt(d, &quot;pid&quot;, byvar=&quot;agegrp&quot;) ## $tab ## $tab[[1]] ## pid/agegrp 18-34 35-54 55+ Total ## Lib 20% (69) 32% (228) 36% (407) 32% (704) ## Con 24% (82) 31% (220) 36% (412) 32% (714) ## NDP 37% (128) 18% (131) 13% (142) 18% (401) ## BQ 3% (11) 7% (54) 6% (71) 6% (136) ## Green 14% (47) 9% (67) 7% (78) 9% (192) ## Other/DK 3% (9) 3% (21) 2% (22) 2% (52) ## Total 100% (346) 100% (721) 100% (1,132) 100% (2,199) ## ## ## $chisq ## $chisq[[1]] ## ## Pearson&#39;s X^2: Rao &amp; Scott adjustment ## ## data: svychisq(as.formula(paste0(&quot;~&quot;, var, &quot;+&quot;, byvar)), d) ## F = 11.102, ndf = 9.9149, ddf = 27652.7701, p-value &lt; 2.2e-16 ## ## ## ## $stats ## $stats[[1]] ## statistic p-value ## Chi-squared 145.45405382 0.000 ## Cramers V 0.18185921 0.000 ## Lambda 0.03636364 0.002 ## ## ## attr(,&quot;class&quot;) ## [1] &quot;xt&quot; As people get older, they get more likely to identify with the mainstream Liberal, BQ, and Conservative parties and less likely to identify with NDP and Green parties. Again, the svytable() function could be used here, too, along with the svychisq() function to calculate the appropriate \\(\\chi^2\\) statistic. tab &lt;- svytable(~pid + agegrp, d) svychisq(~pid + agegrp, d) ## ## Pearson&#39;s X^2: Rao &amp; Scott adjustment ## ## data: svychisq(~pid + agegrp, d) ## F = 11.102, ndf = 9.9149, ddf = 27652.7701, p-value &lt; 2.2e-16 The time when svytable() might be more useful is if you wanted to make a graph of the responses. For example, let’s imagine we wanted to visualize these proportions. We could use a mosaic plot or a stacked bar chart of which people seem to be so enamored. Let’s look at a couple of examples. library(tibble) library(scales) library(ggplot2) tab &lt;- as_tibble(tab) ggplot(tab, aes(x=agegrp, y=n, fill=reorder(pid, n, mean))) + geom_bar(stat=&quot;identity&quot;, position=&quot;fill&quot;) + theme_bw() + theme(legend.position = &quot;top&quot;) + scale_y_continuous(labels=percent) + labs(x=&quot;Age Group&quot;, y=&quot;&quot;, fill=&quot;Party ID&quot;) + coord_flip() In the example above, note that we were able to reorder the categories based on average sample size. This put the biggest category first and the smallest category last. We could also make a side-by-side barplot with the following. Note that we turn the counts into percentages in the data by using the group_by() and mutate() functions. tab &lt;- tab %&gt;% group_by(agegrp) %&gt;% mutate(pct = n/sum(n)) ggplot(tab, aes(x=agegrp, y=pct, fill=reorder(pid, n, mean))) + geom_bar(stat=&quot;identity&quot;, position=&quot;dodge&quot;) + theme_bw() + theme(legend.position = &quot;top&quot;) + scale_y_continuous(labels=percent) + labs(x=&quot;Age Group&quot;, y=&quot;&quot;, fill=&quot;Party ID&quot;) You Try It! Make the bar plots of partyid by college_ed 6.2 Summary Statistics The sumStats() function from the DAMisc package also takes a survey design object as data. Just like the xt() function is a wrapper to the survey.table() and related functions, the sumStats() function is a wrapper to svymean(), svyquantile(), svyvar() and svyby() functions. Let’s look at the summary statistics for thoughts on market capitalism, the market variable. sumStats(d, vars=&quot;market&quot;) ## # A tibble: 1 × 10 ## variable mean sd min q25 median q75 max n nNA ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 market -0.285 0.414 -1 -0.553 -0.330 0 1 2743. 34.5 We could even get summary statistics by some other variable, say agegrp: sumStats(d, &quot;market&quot;, byvar = &quot;agegrp&quot;) ## # A tibble: 3 × 11 ## agegrp variable mean sd min q25 median q75 max n nNA ## &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 18-34 market -0.418 0.420 -1 -0.777 -0.443 -0.110 0.777 436. 0.315 ## 2 35-54 market -0.286 0.414 -1 -0.553 -0.330 0 1 935. 15.7 ## 3 55+ market -0.242 0.404 -1 -0.553 -0.223 0 1 1372. 18.6 You can even get summary statistics for more than one variable by age group. sumStats(d, c(&quot;market&quot;, &quot;continent&quot;), byvar=&quot;agegrp&quot;) ## # A tibble: 6 × 11 ## agegrp variable mean sd min q25 median q75 max n nNA ## &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 18-34 continent 0.103 0.473 -1 0 0 0.330 1 434. 2.24 ## 2 35-54 continent 0.118 0.470 -1 0 0 0.330 1 946. 4.08 ## 3 55+ continent 0.100 0.473 -1 0 0 0.330 1 1381. 9.48 ## 4 18-34 market -0.418 0.420 -1 -0.777 -0.443 -0.110 0.777 436. 0.315 ## 5 35-54 market -0.286 0.414 -1 -0.553 -0.330 0 1 935. 15.7 ## 6 55+ market -0.242 0.404 -1 -0.553 -0.223 0 1 1372. 18.6 You Try It! Make a graph of the means of aid_scale with \\(95\\%\\) confidence bounds as a function of partyid from the US GSS data. Just like above, it might be that you’re better off using svymean() and svyby() if you wanted to make a graph, because svymean() returns not only the mean, but the standard error as well. We could make a dot plot of means and confidence intervals like this: out &lt;- svyby(~market, by=~pid, d, svymean, na.rm=TRUE) %&gt;% as_tibble %&gt;% mutate(lower = market - 1.96*se, upper = market + 1.96*se) ggplot(out, aes(x=reorder(pid, market, mean), y=market)) + geom_pointrange(aes(ymin=lower, ymax=upper)) + geom_point() + theme_bw() + labs(x=&quot;Party ID&quot;, y=&quot;Market Liberalism&quot;) We could compare this to the un-weighted one if we wanted. d0 &lt;- svydesign(ids=~1, weights=~1, data=ces19) out0 &lt;- svyby(~market, by=~pid, d0, svymean, na.rm=TRUE) %&gt;% as_tibble %&gt;% mutate(lower = market - 1.96*se, upper = market + 1.96*se) out0 &lt;- out0 %&gt;% mutate(weighted = factor(1, levels=c(1,2), labels=c(&quot;No&quot;, &quot;Yes&quot;))) out &lt;- out %&gt;% mutate(weighted = factor(2, levels=c(1,2), labels=c(&quot;No&quot;, &quot;Yes&quot;))) out &lt;- bind_rows(out0, out) ggplot(out, aes(x=reorder(pid, market, mean), y=market, colour=weighted)) + geom_pointrange(aes(ymin=lower, ymax=upper), position=position_dodge(width=.3)) + geom_point(position=position_dodge(width=.3)) + theme_bw() + labs(x=&quot;Party ID&quot;, y=&quot;Market Liberalism&quot;) 6.3 Estimating Models The main modeling function for survey design objects is svyglm(). This will estimate linear binary and count models. The svykm() and svycoxph() function will do survival analysis models. The svyolr() function does ordered logit survey models. There is also a svymle() function that would allow you to maximize any likelihood function, so you could program other models as well. tl;dr - these generally work with the post-estimation tools we’ve encountered so far. Rather than belaboring those points again, I’ll point you back to the sections on linear models and binary dependent variable models. You Try It! Use the last model you estimated in the linear models section and estimate it again here using survey weights. "],["multilevel-models.html", "Chapter 7 Multilevel Models 7.1 Data Structure. 7.2 Clustering Standard Errors 7.3 Between vs Within Relationships. 7.4 Random Intercepts 7.5 Estimating the Multilevel Model 7.6 Other Random-Effects Models 7.7 Bayesian Models with BRMS", " Chapter 7 Multilevel Models Multilevel data structures are very prevalent in the social sciences and beyond. Depending on how you’re thinking about your data, they show up not only in the classical nested hierarchical format (e.g., kids in classrooms in schools), but we can also think about longitudinal data as having multilevel structure as well. In this chapter we will restrict our discussion to the more classical hierarchical data structure and in the next chapter, we’ll discuss models for panel/longitudinal/TSCS data. 7.1 Data Structure. The classical multilevel data structure is to have observations nested in higher level groups. For example, we could be interested in both the characteristics of voters and the districts in which they live to predict how the people will vote. We could be interested in the characteristics of elementary school students and their teachers to help understand performance. One of the key features of data like these is that observations within groups are often more similar than they are with observations from other groups. This measure of inter-group (relative to across-group) similarity is called the intraclass correlation. To the extent that this is non-zero, it suggests that naive assumptions about our observations being exchangeable (or conditionally independent) will not hold. If observations are more alike those in their group, then their residuals will be correlated with the residuals of those in their group. There may also be heteroskedasticity because variation is coming not just from the individual level, but also from the group level. We’re going to use the mlbook data from the second edition of Tom Snijders and Roel Bosker’s book “Multilevel Analysis”. For those interested, there are great ML resources for R on the book’s website. library(rio) mlbook &lt;- import(&quot;data/mlbook2_r.txt&quot;) The dependent variable of interest here is langpost - the language post-test score. We are interested in the student and school verbal IQ scores iq_verb and sch_iqv, respectively as well as some demographic information about the students and schools. 7.2 Clustering Standard Errors One thing that people often consider is just clustering their standard errors (or estimating cluster-robust standard errors). These are standard errors that will give approximately right inferences in the presence of arbitrary non-independence within clusters, but assuming independence between clusters. These solve one problem, but as we will discuss later, there remain other problems that could inhibit your ability to make inferences from the model. The multiwayvcov package has what you need to generate clustered standard errors for GLMs. Let’s consider a linear model of language post-test score, first without considering the non-independence problem. library(lmtest) mod.lm &lt;- glm(langPOST ~ IQ_verb + ses+ sex + Minority, data=mlbook) ct.lm &lt;- coeftest(mod.lm) ct.lm ## ## z test of coefficients: ## ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 40.095979 0.156561 256.1038 &lt;2e-16 *** ## IQ_verb 2.399263 0.057741 41.5523 &lt;2e-16 *** ## ses 0.149206 0.010770 13.8542 &lt;2e-16 *** ## sex 2.496239 0.221021 11.2941 &lt;2e-16 *** ## Minority -0.264290 0.531290 -0.4974 0.6189 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Now, let’s see what happens when we use clustered standard errors. library(multiwayvcov) ct.cluster &lt;- coeftest(mod.lm, vcov. = cluster.vcov(mod.lm, cluster=mlbook$schoolnr)) ct.cluster ## ## z test of coefficients: ## ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 40.095979 0.280367 143.0125 &lt;2e-16 *** ## IQ_verb 2.399263 0.074535 32.1898 &lt;2e-16 *** ## ses 0.149206 0.013553 11.0090 &lt;2e-16 *** ## sex 2.496239 0.220226 11.3349 &lt;2e-16 *** ## Minority -0.264290 0.668792 -0.3952 0.6927 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We could see how different the standard errors are (note that the coefficients don’t change, this is a post hoc fix to the standard errors). ct.cluster[,2]/ct.lm[,2] ## (Intercept) IQ_verb ses sex Minority ## 1.7907791 1.2908548 1.2584485 0.9964031 1.2588071 For everything except the sex variable, the clustered standard errors are about \\(25\\%\\) to \\(30\\%\\) bigger than their corresponding un-clustered counterparts. This is what we would imagine would be the case because essentially we are arguing that we have fewer pieces of independent information that it would seem. That should generally result in larger standard errors, as it did here. You Try It! We’re going to be using data from wave 6 of the World Values Survey. I’ve chosen just a few variables and 50 observations from each country to make things a bit quicker to run. You can load in the data with load(&quot;data/wvs6.rda&quot;) This will make an object called wvs in your workspace. Estimate a model of resemaval on moral, sacsecval, income, educ, pr, cl, gdppc and civ. See what happens when you use standard errors clustered on country. 7.3 Between vs Within Relationships. Lots of people think lots of different ways about why we might use a multilevel model and what it could do for us. The way I like to think about this choice is by considering what kind of effect you want to see. Here’s a toy example from the Snijders and Bosker book that will help motivate the discussion. Here’s what the data look like: \\(j\\) \\(i\\) \\(X_{ij}\\) \\(\\bar{X}_{\\cdot j}\\) \\(Y_{ij}\\) \\(\\bar{Y}_{\\cdot j}\\) 1 1 1 2 5 6 1 2 3 2 7 6 2 1 2 3 4 5 2 2 4 3 6 5 3 1 3 4 3 4 3 2 5 4 5 4 4 1 4 5 2 3 4 2 6 5 4 3 5 1 5 6 1 2 5 2 7 6 3 2 where \\(j\\) is the individual identifier and \\(i\\) is the group identifier. l \\(X_{ij}\\) is an individual-level variable and \\(\\bar{X}_{\\cdot j}\\) is the group-mean of \\(X\\). The same convention is used for \\(Y\\). If we looked at these points without knowing there was a grouping variable and fit a line, we would see the following: library(ggplot2) ggplot(sbex, aes(x=X, y=Y)) + geom_point() + geom_smooth(method=&quot;lm&quot;, se=FALSE) + theme_bw() It looks like that describes the points reasonably well. However, if we recognize that there is a grouping variable maybe we would think differently. library(dplyr) ggplot(sbex, aes(x=X, y=Y, colour=j, shape=j)) + geom_point() + geom_smooth(method=&quot;lm&quot;) + theme_bw() The lines connecting each of the points are the within relationship - these tell us within each group, how much we expect \\(y\\) to change as a function of a one-unit change in \\(x\\). We could estimate this relationship by using a within transformation, which simply subtracts the group means from each variable in the model: sbex &lt;- sbex %&gt;% group_by(j) %&gt;% mutate(Xw = X - Xm, Yw = Y - Ym) Here’s the original model: m1 &lt;- lm(Y ~ X, data=sbex) summary(m1) ## ## Call: ## lm(formula = Y ~ X, data = sbex) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.667 -1.167 0.000 1.167 2.667 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.3333 1.4530 3.671 0.0063 ** ## X -0.3333 0.3333 -1.000 0.3466 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.826 on 8 degrees of freedom ## Multiple R-squared: 0.1111, Adjusted R-squared: 2.22e-16 ## F-statistic: 1 on 1 and 8 DF, p-value: 0.3466 and here’s the within model: m2 &lt;- lm(Yw ~ Xw, data=sbex) summary(m2) ## ## Call: ## lm(formula = Yw ~ Xw, data = sbex) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.690e-16 1.056e-17 1.056e-17 6.724e-17 6.724e-17 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.000e+00 3.403e-17 0.000e+00 1 ## Xw 1.000e+00 3.403e-17 2.938e+16 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.076e-16 on 8 degrees of freedom ## Multiple R-squared: 1, Adjusted R-squared: 1 ## F-statistic: 8.633e+32 on 1 and 8 DF, p-value: &lt; 2.2e-16 Note that the within effect is exactly 1 and the original effect is actually in the opposite direction. Another effect we could be interested in is the between effect. This is the regression of the group means of \\(Y\\) on the group means of \\(X\\). This tells us what the expected difference between groups on \\(Y\\) is for a one-unit mean difference across groups on \\(X\\). This is both interesting in its own right (at least sometimes) and can be quite different. Let’s have a look: tmp &lt;- sbex %&gt;% group_by(j) %&gt;% summarise(X = mean(X), Y = mean(Y)) m3 &lt;- lm(Y ~ X , data=tmp) summary(m3) ## ## Call: ## lm(formula = Y ~ X, data = tmp) ## ## Residuals: ## 1 2 3 4 5 ## 1.749e-16 -1.329e-16 -1.289e-16 -4.328e-17 1.301e-16 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.000e+00 2.241e-16 3.570e+16 &lt;2e-16 *** ## X -1.000e+00 5.281e-17 -1.894e+16 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.67e-16 on 3 degrees of freedom ## Multiple R-squared: 1, Adjusted R-squared: 1 ## F-statistic: 3.585e+32 on 1 and 3 DF, p-value: &lt; 2.2e-16 Here, the effect is the same size, but in the opposite direction as the within effect. Which one of these effects do we want? Well, it depends on the research hypothesis/question. Sometimes within, sometimes between, maybe sometimes both. As you can see, it would be hard to estimate the between and within effects in the same model. Plus, the degrees of freedom are even different for the two models. For the between effect, we only have \\(\\# \\text{groups}\\) degrees of freedom, but for the within effect, we have \\(\\# \\text{Individuals}\\) degrees of freedom. The mutlilevel model is built to deal with just this situation. You Try It! Estimate the within and between relationships for the model you estimated above. 7.4 Random Intercepts In its simplest form, the multilevel model is one that allows for a random intercept. That is, it allows for a different intercept for each group. The mathematical notation for the model is: \\[\\begin{aligned} y_{ij} &amp;= \\alpha_{j} + \\beta_{1}X_{ij} + \\varepsilon_{ij}\\\\ \\alpha_{j} &amp;= \\gamma_{00} + \\nu_{j} \\end{aligned}\\] where \\(X_{ij}\\) is an individual-level covariate with coefficient \\(\\beta\\). \\(\\alpha_{j}\\) is the group level intercept. This is clear because it is indexed by the group-indicator \\(j\\). \\(\\gamma_{00}\\) is the overall or global intercept (essentailly the average of the group intercepts). \\(\\varepsilon_{ij}\\) and \\(\\nu_{j}\\) are idiosyncratic residuals at the individual and group levels respectively. The form of the equation above is called the “structural form”. We often see it in “reduced form” where we substitute the equation for \\(\\alpha_j\\) into the equation for \\(y\\). \\[y_{ij} = \\gamma_{00} + \\beta_{1}X_{ij} + \\nu_{j} + \\varepsilon_{ij}\\] Now, we can think of the term \\(\\nu_{j} + \\varepsilon_{ij}\\) as our composite error term that captures variability from both the group and individual levels. It is important to note here that we don’t actually estimate \\(\\nu_{j}\\). Instead, we directly estimate \\(\\sigma_{\\nu}\\) where \\(\\nu_{j} \\sim N(0, \\sigma_{\\nu})\\). That is, we estimate the variance of the individual intercepts. Bigger values mean bigger baseline differences in \\(Y\\) across groups. 7.5 Estimating the Multilevel Model The main package for estimating multilevel models (in the frequentist context, at least) is the lme4 package. First, we’ll just talk about estimating a two-level model with only level-1 covariates. library(lme4) mlm &lt;- lmer(Y ~ X + (1|j), data=sbex) summary(mlm) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: Y ~ X + (1 | j) ## Data: sbex ## ## REML criterion at convergence: -45.3 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.893e-05 -1.266e-05 0.000e+00 1.266e-05 2.893e-05 ## ## Random effects: ## Groups Name Variance Std.Dev. ## j (Intercept) 5.000e+00 2.236e+00 ## Residual 1.308e-09 3.617e-05 ## Number of obs: 10, groups: j, 5 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 2.093e-09 1.000e+00 0 ## X 1.000e+00 1.144e-05 87433 ## ## Correlation of Fixed Effects: ## (Intr) ## X 0.000 ## optimizer (nloptwrap) convergence code: -4 (NLOPT_ROUNDOFF_LIMITED: Roundoff errors led to a breakdown of the optimization algorithm. In this case, the returned minimum may still be useful. (e.g. this error occurs in NEWUOA if one tries to achieve a tolerance too close to machine precision.)) ## unable to evaluate scaled gradient ## Model failed to converge: degenerate Hessian with 1 negative eigenvalues Above, you’ll see a couple of things. For example, you see that the effect of \\(X\\) is essentially the within-unit effect of 1. Also, you’ll notice that under the “Random effects:” heading, the Groups: j row shows a variance of 5, meaning that the group level intercepts are quite variable while the level 1 residuals have basically no variance. If we wanted to estimate the between relationship, too, we could add the group-mean of \\(X\\) in to the model. You Try It! Use lmer() to estimate a multilevel model similar to the one you estimated above. - What are the differences? This would result in a somewhat different model than above, at least from a mathematical point of view. In this case, we would be estimating: \\[\\begin{aligned} y_{ij} &amp;= \\alpha_{j} + \\beta_{1}X_{ij} + \\varepsilon_{ij}\\\\ \\alpha_{j} &amp;= \\gamma_{00} + \\gamma_{01}\\bar{X}_{j} + \\nu_{j} \\end{aligned}\\] Or in the reduced form: \\[y_{ij} = \\gamma_{00} + \\gamma_{01}\\bar{X}_{j} + \\beta_{1}X_{ij} + \\nu_{j} + \\varepsilon_{ij}\\] mlm2 &lt;- lmer(Y ~ X + Xm + (1|j), data=sbex) summary(mlm2) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: Y ~ X + Xm + (1 | j) ## Data: sbex ## ## REML criterion at convergence: -450.9 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -0.8693 -0.8693 -0.8693 -0.5433 0.4347 ## ## Random effects: ## Groups Name Variance Std.Dev. ## j (Intercept) 2.088e-30 1.445e-15 ## Residual 1.044e-30 1.022e-15 ## Number of obs: 10, groups: j, 5 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 8.000e+00 2.167e-15 3.691e+15 ## X 1.000e+00 3.231e-16 3.095e+15 ## Xm -2.000e+00 6.044e-16 -3.309e+15 ## ## Correlation of Fixed Effects: ## (Intr) X ## X 0.000 ## Xm -0.797 -0.535 ## optimizer (nloptwrap) convergence code: 0 (OK) ## Model failed to converge with max|grad| = 0.531251 (tol = 0.002, component 1) ## Model is nearly unidentifiable: very large eigenvalue ## - Rescale variables? In the model above, the between effect would actually be calculated by taking group means on both sides of the equation. \\[\\bar{y}_{j} = \\gamma_{00} + \\gamma_{01}\\bar{X}_{j} + \\beta_{1}\\bar{X}_{j} + \\nu_{j} + \\bar{\\varepsilon}_{j}\\] So the between effect is really just: \\(\\gamma_{01} + \\beta_1\\), which we can see is -1, just like it was in the model above. In this toy example, the model automatically estimated the within effect for you. The effects in MLMs outside of perfectly-fitting models are not exactly the within effects, though they are generally quite close if the groups are of reasonable size. Let’s go back to the example about the language test from Snijders and Bosker. To make the previous point clear, just look at the true within effect model: mlb &lt;- mlbook %&gt;% group_by(schoolnr) %&gt;% mutate(iq_w = IQ_verb - mean(IQ_verb), ses_w = ses - mean(ses), sex_w = sex - mean(sex), Minority_w = Minority - mean(Minority), lp_w = langPOST - mean(langPOST)) summary(wmod &lt;- lm(lp_w ~ iq_w + ses_w + sex_w + Minority_w, data=mlb)) ## ## Call: ## lm(formula = lp_w ~ iq_w + ses_w + sex_w + Minority_w, data = mlb) ## ## Residuals: ## Min 1Q Median 3Q Max ## -24.425 -3.808 0.330 4.191 20.271 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.896e-16 9.611e-02 0.000 1.000 ## iq_w 2.250e+00 5.397e-02 41.691 &lt;2e-16 *** ## ses_w 1.651e-01 1.115e-02 14.804 &lt;2e-16 *** ## sex_w 2.389e+00 1.985e-01 12.033 &lt;2e-16 *** ## Minority_w -6.974e-02 5.588e-01 -0.125 0.901 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.892 on 3753 degrees of freedom ## Multiple R-squared: 0.414, Adjusted R-squared: 0.4134 ## F-statistic: 662.9 on 4 and 3753 DF, p-value: &lt; 2.2e-16 versus the MLM: mod.mlm &lt;- lmer(langPOST ~ IQ_verb + ses+ sex + Minority + (1|schoolnr), data=mlbook) summary(mod.mlm) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: langPOST ~ IQ_verb + ses + sex + Minority + (1 | schoolnr) ## Data: mlbook ## ## REML criterion at convergence: 24571.6 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -4.1036 -0.6321 0.0704 0.6913 3.4538 ## ## Random effects: ## Groups Name Variance Std.Dev. ## schoolnr (Intercept) 9.403 3.066 ## Residual 36.804 6.067 ## Number of obs: 3758, groups: schoolnr, 211 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 39.94788 0.25775 154.984 ## IQ_verb 2.28643 0.05474 41.772 ## ses 0.16281 0.01108 14.691 ## sex 2.40778 0.20301 11.860 ## Minority -0.14197 0.55032 -0.258 ## ## Correlation of Fixed Effects: ## (Intr) IQ_vrb ses sex ## IQ_verb -0.028 ## ses 0.013 -0.276 ## sex -0.379 0.034 -0.030 ## Minority -0.115 0.129 0.092 0.007 You’ll note that the effects are very close for most of the variables, though the one for Minority is reasonably different in the MLM versus the true within effect. In fact, what Andrew Bell and Kelvyn Jones (PSRM) suggest is to calculate both the within and between transformations on the variables and then put them both in. This will ensure that you are able to calculate the appropriate effect. In this particular example with all numeric variables, it’s quite easy, though we’ll talk later on about how it gets more difficult. mlb &lt;- mlbook %&gt;% select(schoolnr, IQ_verb, ses, sex, Minority, langPOST) %&gt;% group_by(schoolnr) %&gt;% mutate(across(everything(), .fns = list(w = function(x)x-mean(x), b = mean))) mod.mlm2 &lt;- lmer(langPOST ~ IQ_verb_w + IQ_verb_b + ses_w + ses_b + sex_w + sex_b + Minority_w + Minority_b + (1|schoolnr), data=mlb) You can verify by looking above that the _w variables have the same coefficients as those from the within model estimated with the lm() function above. You Try It! Estimate the multilevel model with both the between and within variables. 7.5.1 P-values in LMER Output One thing you’ll notice is that by default, there are no \\(p\\)-values in the output. This is considered a feature rather than a flaw by the creator of the function (Douglas Bates). There is an interesting exchange on the R listserv about just this point. To save you the time of reading, Bates’ point is that the appropriate reference distribution for the \\(p\\)-value remains an open question and just because other software produces \\(p\\)-values, doesn’t mean that R should, too. If, like lots of people, you want \\(p\\)-values anyway, there are a few ways you can accomplish this. We’ll focus on the one that’s implemented in the lmerTest package for now. The lmerTest package loads an alternative version of lmer that produces standard errors. Here’s how it would work. library(lmerTest) mod.mlm2 &lt;- lmer(langPOST ~ IQ_verb_w + IQ_verb_b + ses_w + ses_b + sex_w + sex_b + Minority_w + Minority_b + (1|schoolnr), data=mlb) summary(mod.mlm2) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: langPOST ~ IQ_verb_w + IQ_verb_b + ses_w + ses_b + sex_w + sex_b + ## Minority_w + Minority_b + (1 | schoolnr) ## Data: mlb ## ## REML criterion at convergence: 24555.3 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -4.1048 -0.6299 0.0708 0.6891 3.4571 ## ## Random effects: ## Groups Name Variance Std.Dev. ## schoolnr (Intercept) 8.838 2.973 ## Residual 36.793 6.066 ## Number of obs: 3758, groups: schoolnr, 211 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 39.35372 0.87775 224.14387 44.835 &lt;2e-16 *** ## IQ_verb_w 2.25023 0.05557 3541.36099 40.495 &lt;2e-16 *** ## IQ_verb_b 3.47030 0.31195 219.02545 11.125 &lt;2e-16 *** ## ses_w 0.16513 0.01148 3541.36099 14.380 &lt;2e-16 *** ## ses_b 0.06908 0.04421 199.21218 1.562 0.120 ## sex_w 2.38858 0.20436 3541.36099 11.688 &lt;2e-16 *** ## sex_b 3.54642 1.72557 224.87064 2.055 0.041 * ## Minority_w -0.06974 0.57535 3541.36099 -0.121 0.904 ## Minority_b 0.56204 1.89904 222.00697 0.296 0.768 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) IQ_vrb_w IQ_vrb_b ses_w ses_b sex_w sex_b Mnrty_w ## IQ_verb_w 0.000 ## IQ_verb_b -0.058 0.000 ## ses_w 0.000 -0.264 0.000 ## ses_b 0.106 0.000 -0.498 0.000 ## sex_w 0.000 0.035 0.000 -0.027 0.000 ## sex_b -0.958 0.000 0.027 0.000 -0.108 0.000 ## Minority_w 0.000 0.122 0.000 0.085 0.000 0.003 0.000 ## Minority_b -0.214 0.000 0.258 0.000 0.074 0.000 0.107 0.000 In the summary above, you can see that the degrees of freedom are different for each of the _b variables. These are close to the number of groups (211). You Try It! Use the lmerTest package to get \\(p\\)-values for your coefficients in the previous model. We can use the ggeffects package to help us plot the effects here. library(ggeffects) g1 &lt;- ggpredict(mod.mlm2, &quot;IQ_verb_w&quot;, type=&quot;random&quot;) g2 &lt;- ggpredict(mod.mlm2, &quot;IQ_verb_w&quot;, type=&quot;fixed&quot;) g1$type = factor(1, levels=1:2, labels=c(&quot;Fixed&quot;, &quot;Random&quot;)) g2$type = factor(2, levels=1:2, labels=c(&quot;Fixed&quot;, &quot;Random&quot;)) g &lt;- bind_rows(g1, g2) Let’s look at the two effect types: ggplot(g, aes(x=x, y=predicted, colour=type, fill=type)) + geom_ribbon(aes(ymin=conf.low, ymax=conf.high), alpha=.15, col=&quot;transparent&quot;) + geom_line() + theme_bw() + theme(legend.position = &quot;top&quot;) + labs(x=&quot;Verbal IQ (within)&quot;, y=&quot;Predicted Language Post-test Score&quot;, colour=&quot;Interval Type&quot;, fill=&quot;Interval Type&quot;) We can add random coefficients to the model - these allow the coefficients of variables to vary by group. The model would look like this: \\[\\begin{aligned} y_{ij} &amp;= \\alpha_{j} + \\beta_{1j}X_{ij} + \\varepsilon_{ij}\\\\ \\alpha_{j} &amp;= \\gamma_{00} + \\gamma_{01}\\bar{X}_{j} + \\nu_{1j}\\\\ \\beta_{1j} &amp;= \\gamma_{10} + \\gamma_{11}\\bar{X}_{j} + \\nu_{2j} \\end{aligned}\\] Or in the reduced form: \\[y_{ij} = \\gamma_{00} + \\gamma_{01}\\bar{X}_{j} + \\gamma_{10}X_{ij} + \\gamma_{11}X_{ij}\\bar{X}_{j} + \\nu_{2j}X_{ij} + \\nu_{j} + \\varepsilon_{ij}\\] There are a few things to note here. First, it is assumed that \\(\\nu_{j}\\sim N_{2}\\left(\\mathbf{\\mu}, \\mathbf{\\Sigma}\\right)\\). Also you can see some heteroskedasticity built in to the model as the residuals are a function of \\(X\\) through the \\(\\nu_{2j}X_{ij}\\) term. Finally, you can see that the way that level-2 variables effect the random coefficients is through an intercation (in this context, called a “cross-level interaction”) between the level-1 variable that has the random coefficient and the level-2 variable meant to predict that random coefficient. We could do this with the verbal IQ score and its school-level mean. mod.mlm3 &lt;- lmer(langPOST ~ IQ_verb_w*IQ_verb_b + ses_w + ses_b + sex_w + sex_b + Minority_w + Minority_b + (1+ IQ_verb_w| schoolnr), data=mlb) summary(mod.mlm3) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: langPOST ~ IQ_verb_w * IQ_verb_b + ses_w + ses_b + sex_w + sex_b + ## Minority_w + Minority_b + (1 + IQ_verb_w | schoolnr) ## Data: mlb ## ## REML criterion at convergence: 24526.7 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -4.1201 -0.6297 0.0726 0.6835 3.0316 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## schoolnr (Intercept) 8.946 2.9910 ## IQ_verb_w 0.188 0.4335 -0.70 ## Residual 36.083 6.0069 ## Number of obs: 3758, groups: schoolnr, 211 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 39.22722 0.84962 232.30831 46.170 &lt;2e-16 *** ## IQ_verb_w 2.27104 0.06397 197.15338 35.500 &lt;2e-16 *** ## IQ_verb_b 3.43997 0.30715 228.15952 11.200 &lt;2e-16 *** ## ses_w 0.16626 0.01143 3536.93552 14.545 &lt;2e-16 *** ## ses_b 0.04483 0.04210 196.29448 1.065 0.2882 ## sex_w 2.37629 0.20346 3536.35974 11.680 &lt;2e-16 *** ## sex_b 3.97564 1.66790 232.32685 2.384 0.0179 * ## Minority_w 0.12227 0.58238 3273.74604 0.210 0.8337 ## Minority_b -1.19277 1.78364 194.44191 -0.669 0.5045 ## IQ_verb_w:IQ_verb_b -0.13398 0.06995 211.18343 -1.915 0.0568 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) IQ_vrb_w IQ_vrb_b ses_w ses_b sex_w sex_b Mnrty_w Mnrty_b ## IQ_verb_w -0.068 ## IQ_verb_b -0.049 -0.009 ## ses_w 0.006 -0.229 0.008 ## ses_b 0.101 -0.001 -0.484 -0.015 ## sex_w 0.000 0.030 0.004 -0.025 -0.006 ## sex_b -0.956 -0.013 0.020 -0.006 -0.102 0.000 ## Minority_w -0.010 0.101 0.019 0.084 -0.008 0.006 0.005 ## Minority_b -0.205 -0.013 0.247 -0.002 0.070 0.000 0.101 0.047 ## IQ_vr_:IQ__ -0.003 0.023 -0.227 -0.017 -0.028 -0.022 0.004 -0.101 -0.029 You Try It! Estimate the multilevel model with a random coefficient on the within effect of democ_max and a cross-level interaction with pr and cl We could then plot out the different random coefficients. g2 &lt;- ggpredict(mod.mlm3, terms=c(&quot;IQ_verb_w&quot;, &quot;IQ_verb_b [.04]&quot;, &quot;schoolnr [sample=10]&quot;), type=&quot;random&quot;) ggplot(g2, aes(x=x, y=predicted, colour=facet)) + geom_line() + theme_bw() + theme(legend.position = &quot;top&quot;) + labs(x=&quot;IQ Verbal Score (within)&quot;, y=&quot;Predicted Language Post-test Score&quot;, colour=&quot;School&quot;) You Try It! Plot out some of the random effects from the model above. 7.6 Other Random-Effects Models The lme4 package also has a function glmer() which estimates random-effect GLMs in R with exactly the same syntax and the addition of a family= argument which takes as values any of the GLM families in R. The ordinal package has a function clmm() which has similar syntax to the lmer() function, but for ordinal logistic and probit models. Here, the thershold parameters are all shifted up or down by the same amount by the random effect. The mlogit() function in the mlogit package can estimate a random-effect multinomial logit through the rpar argument. 7.7 Bayesian Models with BRMS One of the potential problems above is that with random effects the frequentist models do not produce confidence intervals for the predictions. The brms package has a function brm, which uses the same syntax as glmer(), but estimates a Bayesian model with Stan. If you look at the help for brmsfamily(), you will see all of the different families you can use. These include all of the GLM families and many others including multinomial and cumulative links - unifying all of the models we’ve talked about before. Let’s look at the same model as above, but using the brm() function. library(brms) b.mlm3 &lt;- brm(langPOST ~ IQ_verb_w*IQ_verb_b + ses_w + ses_b + sex_w + sex_b + Minority_w + Minority_b + (1|schoolnr), data=mlb) You Try It! Use the brms package to estimate the above model in the Bayesian context. We can plot marginal effects from this model either by using the conditional_effects() function or by using the predict() function handing the new data. Here is the effect if IQ_verb_w on langPOST for a few different values of IQ_verb_b, just considering the fixed effect parameters. c1 &lt;- conditional_effects(b.mlm3, effects=&quot;IQ_verb_w:IQ_verb_b&quot;) plot(c1)$`IQ_verb_w:IQ_verb_b` + theme_bw() We could do the same thing, but including the random effect variance. c2 &lt;- conditional_effects(b.mlm3, effects=&quot;IQ_verb_w:IQ_verb_b&quot;, re_formula = ~ (1|schoolnr)) plot(c2)$`IQ_verb_w:IQ_verb_b` + theme_bw() Here, the intervals are understandably larger. We could also look at a few schools in particular. In the code below, we are picking three schools (numbered 1, 2 and 3) and we’re also going to supply their specific values of IQ_verb_b as conditions to plot. conditions &lt;- mlb %&gt;% filter(schoolnr %in% 1:3) %&gt;% select(schoolnr, IQ_verb_b) %&gt;% group_by(schoolnr) %&gt;% summarise_all(mean) c3 &lt;- conditional_effects(b.mlm3, effects=&quot;IQ_verb_w&quot;, re_formula = ~ (1|schoolnr), conditions = conditions) plot(c3)$`IQ_verb_w` + theme_bw() + theme(aspect.ratio=1) The IQ_verb_w element of c3 holds all of the data for plotting. You could make your own plot if you wanted. ggplot(c3$IQ_verb_w, aes(x=IQ_verb_w, y=estimate__, colour=cond__, fill = cond__)) + geom_ribbon(aes(ymin=lower__, ymax=upper__), alpha=.15, col=&quot;transparent&quot;) + geom_line() + theme_bw() + theme(legend.position=&quot;top&quot;) + labs(x=&quot;IQ Verbal Score (within)&quot;, y = &quot;Predicted Language Post-test Score&quot;, colour=&quot;School&quot;, fill=&quot;School&quot;) You Try It! Calculate and plot the conditional effect of democ_max from the model above. This is a quite powerful tool for understanding the variability of effects in a way that is quite difficult in the frequentist inferential paradigm. "],["time-series-cross-sectional-models.html", "Chapter 8 Time-Series Cross-Sectional Models 8.1 Preliminaries and Assumptions 8.2 Unit Effects 8.3 The plm Package", " Chapter 8 Time-Series Cross-Sectional Models TSCS Data have both cross-sectional (data on different units) and time-series (data over time) properties. As such: For each unit (e.g., country) there are many different time-periods. For each different time-period, there are many different units 8.1 Preliminaries and Assumptions In the presence of such data structure, conventional cross-sectional results are wrong: At least inefficient - standard errors will be wrong. Could also be biased depending on the particular nature of the data. There are several issues that come up when we consider this particular dependence structure in the data. Non-independence: the assumption that all observations are independent is almost certainly violated. Observations close in time for the same unit are often more alike each other than observations far away in time. Consequence I: Serial correlation (errors are correlated with each other). Consequence II: Model mis-specification: if \\(y_{t}\\) is a function of \\(y_{t-1}\\) and we don’t include it, then we have mis-specified the model. -Trending - series that tend to increase or decrease by the same amount over time. Two series that are trending will tend to show a strong relationship, but the relationship could be spurious as both variables are a function of time. Also violates the assumption of stationarity - finite, constant variance. Structural breaks - rapid and immediate changes in mean or variance While there are lots of things we could care about with respect to time-series, for TSCS data, it’s almost always about autoregressive errors. \\[y_{t} = \\alpha_{0} + \\alpha_{1} y_{t-1} + \\varepsilon_{t}\\] Here, the \\(\\alpha_{0}\\) allows the long-run equilibrium to be non-zero. Assuming \\(|\\alpha_{1}| &lt; 1\\), the series will eventually, though not immediately, return to its long-term equilibrium after an intervention (without further interventions). The correlation between temporally adjacent values of \\(y\\) is: \\[Corr\\left(y_{t}, y_{t-1}\\right) \\equiv \\rho \\equiv \\frac{E\\left(\\left(y_{t} - \\mu_{t}\\right)\\left(y_{t-1}-\\mu_{t-1}\\right)\\right)}{E\\left(y_{t} - \\mu_{t}\\right)^{2}}\\] We can estimate \\(\\hat{\\rho}\\) by plugging \\(\\bar{y}\\) in for \\(\\mu\\). There are a few assumptions that we should make sure to understand. The first is stationarity. Covariance stationarity is defined by: \\(E(y_{t})\\) is constant \\(\\text{Var}(y_{t})\\) is constant \\(\\text{Cov}(y_{t=k}, y_{t=k+h})\\) depends only on \\(h\\) and not \\(k\\). Another is exogeneity. Strict exogeneity is defined as: \\[\\begin{aligned} y_{t} &amp;= \\beta_{0} + \\beta_{1}x_{t} + \\varepsilon_{t}, \\\\ E(\\varepsilon_{t} | \\mathbf{X}) &amp;= 0 \\implies E(\\varepsilon_{t} | x_{t + h}) = 0 \\quad \\forall h \\end{aligned}\\] If exogeneity is violated, results will be biased. A weaker version (contemporaneous exogeneity) will suffice if \\(T\\) is large. \\[E(\\varepsilon_{t}|x_{t}) = 0\\] One of the ways that we deal with some of these problems could be through the Autoregressive Distributive Lag (ADL) model. Here we assume: \\[y_{t} = \\alpha_{0} + \\alpha_{1}y_{t-1} + \\varepsilon_{t}\\] Where \\(\\alpha_{0}\\) assumes that the series has a non-zero mean. In the static process the long-run equilibrium is \\(\\beta_{0} = \\frac{\\alpha_{0}}{1-\\alpha_{1}}\\). However, we will tend to focus on the lagged dependent variable (LDV) model: \\[y_{t} = \\alpha_{0} + \\alpha_{1}y_{t-1} + \\beta_{1}x_{t} + \\varepsilon_{t}\\] Where the short-term (immediate) effect of \\(x_{t}\\) is \\(\\beta_{1}\\) and the long-term effect of \\(x_{t}\\) is \\(\\frac{\\beta_{1}}{1-\\alpha_{1}}\\) Much of the discussion above is derived from Mark Pickup’s excellent primer on Time Series models. The book, though, is really about single-series models. Here, we are interested in time-series cross-sectional models, which have multiple series. All of the issues mentioned above get much more complicated in TSCS data becuse there are, in effect, many different time-series that we’re trying to model simultaneously. Further, the parameters are often constrained to be the same across the different series, which may or may not be appropriate for the data. 8.2 Unit Effects In the chapter discussing multilevel models, we already discussed between- and within-estimators. In the TSCS literature, this same idea often originates from a different concern - how to deal with unit effects. That is - what is the appropriate way to incorporate in our model the unobserved heterogeneity that comes from unobserved group-specific characteristics? Modeling unit effects refers to acknowledging that there may be differences in the average level of the dependent variable across groups. \\[y_{i} = \\alpha_{j[i]} + \\beta x_{i} + \\varepsilon_{i}; \\quad \\varepsilon_{i}\\sim N(0, \\sigma_{y}^{2})\\] After controlling for \\(x\\), there may remain unexplained variation relating to groups in \\(y\\). That is what \\(\\alpha_{j[i]}\\) is meant to capture. Failing to account for \\(\\alpha_{j[i]}\\) will usually result in biased estimates of \\(\\beta\\). There are two approaches to solving this problem - fixed effects and random effects. We would define the fixed-effects estimator as: \\[y_{i} = \\sum_{j=1}^{N}\\alpha_{j}z_{j[i]} + \\beta x_{i} + \\varepsilon_{i}; \\quad \\varepsilon_{i}\\sim N(0, \\sigma_{y}^{2})\\] Here, the unit effects are a systematic part of the model. That means that the unobserved characteristics captured by the different intercepts could be correlated with the other variables in the systematic component of the model. Because these different intercepts are estimated as part of the systematic component, it wouldn’t make sense to think about what other “new” groups might look like. So, this is really a method that is appropriate when the groups are meaningful in their own right and not just a sample from a much larger set of groups. Because the fixed effects are in the systematic part of the model, they they would be perfectly collinear with any other variable that varied only by group, so those variables cannot be in the model. It is also worth noting that the fixed-effects estimator is a within-estimator. The random effect model is defined as follows: \\[y_{i} = \\alpha_{j[i]} + \\beta x_{i} + \\varepsilon_{i}; \\quad \\alpha_{j}\\sim N(\\mu_{\\alpha}, \\sigma_{\\alpha}^{2}); \\quad \\varepsilon_{i}\\sim N(0, \\sigma_{y}^{2})\\] Here, the unit-specific effects are actually in the stochastic part of the model. As such, there is an assumption that they are uncorrelated with anything in the systematic part of the model. Bell and Jones suggest including all group-mean and within variables in the model, therefore purging the errors of any systematic relationship with the mean of the covariates in the model. It is also worth noting here that we are not modeling the unit-specific effects directly, rather the model accounts for the added variability that comes from the grouping. Specifically, it estimates the variance of the unit effects which are assumed to have a normal distribution with mean equal to 0. The random effect model, as we saw in the chapter on multilevel models, can incorporate group-specific variables and can accommodate itself to both between and within relationships. Clark and Linzer (PSRM) give the following advice about the choice. - In the standard case (non-sluggish variables) - both RE and FE do equally well as regards bias, use whichever one you want. - Your hypothesis should be the guide - if you have a within hypothesis, you should use a within estimator. - With “sluggish” independent variables, - RE is better with few observations overall and few observations per unit. - FE is better with larger N when the correlation between unit effects and independent variable are moderate to high. - FE is better in all cases when the number of observations per unit (times in our case) is bigger than 20. The first-difference (FD) estimator also solves some problems - particularly related to stationarity. We would define the estimator as: \\[\\Delta y_{it} = \\beta \\Delta x_{it} + \\Delta \\varepsilon_{it}\\] where: - \\(\\Delta y_{it} = y_{it} - y_{i,t-1}\\) - \\(\\Delta x_{it} = x_{it} - x_{i,t-1}\\) - \\(\\Delta \\varepsilon_{it} = \\varepsilon_{it} - \\varepsilon_{i,t-1}\\) This model makes sense particularly if you’re interested in changes. 8.3 The plm Package The plm package in R has a number of functions that will help us out here. We’ll load in some data to use below: library(rio) library(plm) dat &lt;- import(&quot;data/repress_data.dta&quot;) The fixed effect model is estimated with the model=\"within\" and effect=\"individual\" argument. mod1.fe &lt;- plm(repress ~ voice*veto + log(pop) + log(rgdpch) + iwar + cwar, data=dat, model = &quot;within&quot;, index= c(&quot;ccode&quot;, &quot;year&quot;), effect=&quot;individual&quot;) summary(mod1.fe) ## Oneway (individual) effect Within Model ## ## Call: ## plm(formula = repress ~ voice * veto + log(pop) + log(rgdpch) + ## iwar + cwar, data = dat, effect = &quot;individual&quot;, model = &quot;within&quot;, ## index = c(&quot;ccode&quot;, &quot;year&quot;)) ## ## Unbalanced Panel: n = 159, T = 6-28, N = 3876 ## ## Residuals: ## Min. 1st Qu. Median 3rd Qu. Max. ## -4.8942061 -0.6903648 0.0022614 0.7214781 4.4375575 ## ## Coefficients: ## Estimate Std. Error t-value Pr(&gt;|t|) ## voice 0.063040 0.014638 4.3065 1.702e-05 *** ## veto 0.233100 0.067497 3.4535 0.0005595 *** ## log(pop) -1.430969 0.146190 -9.7884 &lt; 2.2e-16 *** ## log(rgdpch) -0.125093 0.089594 -1.3962 0.1627308 ## iwar -0.205658 0.148735 -1.3827 0.1668359 ## cwar -0.633611 0.106042 -5.9751 2.516e-09 *** ## voice:veto 0.013440 0.012657 1.0619 0.2883504 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Total Sum of Squares: 5062.1 ## Residual Sum of Squares: 4797.3 ## R-Squared: 0.052307 ## Adj. R-Squared: 0.010159 ## F-statistic: 29.2526 on 7 and 3710 DF, p-value: &lt; 2.22e-16 You Try It! For the applied exercises in this chapter, we’re using data from: Ross, Michel (2001) “Does Oil Hinder Democracy?” World Politics 53(3): 325-361. You can load the data with: load(&quot;data/oil.rda&quot;) This will make an object called oil in your workspace. Estimate the linear, between and one-way fixed effect model of regime1 on the five-year lags of regime1, oil, metal logl135 as well as the contemporaneous values of islam and oecd. There is also a two-way fixed effect model where there are both unit and time. We can estimate that model with: mod1.twfe &lt;- plm(repress ~ voice*veto + log(pop) + log(rgdpch) + iwar + cwar, data=dat, model = &quot;within&quot;, index= c(&quot;ccode&quot;, &quot;year&quot;), effect=&quot;twoway&quot;) summary(mod1.twfe) ## Twoways effects Within Model ## ## Call: ## plm(formula = repress ~ voice * veto + log(pop) + log(rgdpch) + ## iwar + cwar, data = dat, effect = &quot;twoway&quot;, model = &quot;within&quot;, ## index = c(&quot;ccode&quot;, &quot;year&quot;)) ## ## Unbalanced Panel: n = 159, T = 6-28, N = 3876 ## ## Residuals: ## Min. 1st Qu. Median 3rd Qu. Max. ## -4.6429697 -0.6536746 -0.0081787 0.6951082 4.3962291 ## ## Coefficients: ## Estimate Std. Error t-value Pr(&gt;|t|) ## voice 0.082430 0.014632 5.6334 1.899e-08 *** ## veto 0.298216 0.067789 4.3992 1.117e-05 *** ## log(pop) -0.065765 0.230294 -0.2856 0.77522 ## log(rgdpch) 0.225734 0.108414 2.0822 0.03740 * ## iwar -0.229900 0.149270 -1.5402 0.12361 ## cwar -0.589891 0.106838 -5.5214 3.596e-08 *** ## voice:veto 0.023783 0.012582 1.8902 0.05881 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Total Sum of Squares: 4852.3 ## Residual Sum of Squares: 4633 ## R-Squared: 0.045196 ## Adj. R-Squared: -0.0045795 ## F-statistic: 24.905 on 7 and 3683 DF, p-value: &lt; 2.22e-16 You Try It! Estimate the two-way fixed effects version of the model you estimated above. We could estimate the random effects model in lme4 or brms as discussed above, but we could also estimate it with plm as follows: mod1.re &lt;- plm(repress ~ voice*veto + log(pop) + log(rgdpch) + iwar + cwar, data=dat, model = &quot;random&quot;, index= c(&quot;ccode&quot;, &quot;year&quot;), effect=&quot;individual&quot;) summary(mod1.re) ## Oneway (individual) effect Random Effect Model ## (Swamy-Arora&#39;s transformation) ## ## Call: ## plm(formula = repress ~ voice * veto + log(pop) + log(rgdpch) + ## iwar + cwar, data = dat, effect = &quot;individual&quot;, model = &quot;random&quot;, ## index = c(&quot;ccode&quot;, &quot;year&quot;)) ## ## Unbalanced Panel: n = 159, T = 6-28, N = 3876 ## ## Effects: ## var std.dev share ## idiosyncratic 1.2931 1.1371 0.579 ## individual 0.9409 0.9700 0.421 ## theta: ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.5683 0.7837 0.7837 0.7718 0.7837 0.7837 ## ## Residuals: ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -4.5803 -0.7387 0.0145 0.0037 0.7474 4.3187 ## ## Coefficients: ## Estimate Std. Error z-value Pr(&gt;|z|) ## (Intercept) 3.822831 0.636985 6.0014 1.956e-09 *** ## voice 0.069137 0.014104 4.9020 9.486e-07 *** ## veto 0.203971 0.066453 3.0694 0.0021449 ** ## log(pop) -0.607796 0.048474 -12.5385 &lt; 2.2e-16 *** ## log(rgdpch) 0.191204 0.053444 3.5777 0.0003467 *** ## iwar -0.150622 0.149752 -1.0058 0.3145066 ## cwar -0.774438 0.106113 -7.2983 2.915e-13 *** ## voice:veto 0.057605 0.011702 4.9226 8.541e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Total Sum of Squares: 5628.6 ## Residual Sum of Squares: 5150.5 ## R-Squared: 0.084946 ## Adj. R-Squared: 0.08329 ## Chisq: 358.302 on 7 DF, p-value: &lt; 2.22e-16 You Try It! Estimate the random effect model (both one-way and two-way) similar to the models you estimated above. One of the main benefits of using the plm package is access to a number of tests for residuals and also access to lag(), lead() and diff() functions that work as expected with time-series cross-sectional data. First, we could test for serial correlation in the errors with either the Breusch-Pagan test: pbgtest(mod1.fe, order=1) ## ## Breusch-Godfrey/Wooldridge test for serial correlation in panel models ## ## data: repress ~ voice * veto + log(pop) + log(rgdpch) + iwar + cwar ## chisq = 731.08, df = 1, p-value &lt; 2.2e-16 ## alternative hypothesis: serial correlation in idiosyncratic errors Or the Wooldridge test, which only works on fixed effects models. pwartest(mod1.fe) ## ## Wooldridge&#39;s test for serial correlation in FE panels ## ## data: mod1.fe ## F = 340.26, df1 = 1, df2 = 3715, p-value &lt; 2.2e-16 ## alternative hypothesis: serial correlation The Breusch-Pagan test also works on random effects models. pbgtest(mod1.re, order=1) ## ## Breusch-Godfrey/Wooldridge test for serial correlation in panel models ## ## data: repress ~ voice * veto + log(pop) + log(rgdpch) + iwar + cwar ## chisq = 852.02, df = 1, p-value &lt; 2.2e-16 ## alternative hypothesis: serial correlation in idiosyncratic errors You Try It! Evaluate the two-way fixed effects model for serial correlation using the tests above. - See if a different lag structure solves the problem. One way that we could attempt to solve the problem is by adding a lagged dependent variable to the model. mod2.fe &lt;- plm(repress ~ lag(repress, 1) + voice*veto + log(pop) + log(rgdpch) + iwar + cwar, data=dat, model = &quot;within&quot;, index= c(&quot;ccode&quot;, &quot;year&quot;), effect=&quot;individual&quot;) summary(mod2.fe) ## Oneway (individual) effect Within Model ## ## Call: ## plm(formula = repress ~ lag(repress, 1) + voice * veto + log(pop) + ## log(rgdpch) + iwar + cwar, data = dat, effect = &quot;individual&quot;, ## model = &quot;within&quot;, index = c(&quot;ccode&quot;, &quot;year&quot;)) ## ## Unbalanced Panel: n = 159, T = 5-27, N = 3712 ## ## Residuals: ## Min. 1st Qu. Median 3rd Qu. Max. ## -4.109986 -0.584769 0.012532 0.589705 3.923199 ## ## Coefficients: ## Estimate Std. Error t-value Pr(&gt;|t|) ## lag(repress, 1) 0.4563965 0.0145378 31.3937 &lt; 2.2e-16 *** ## voice 0.0325649 0.0132895 2.4504 0.0143167 * ## veto 0.1305343 0.0608242 2.1461 0.0319333 * ## log(pop) -0.7825166 0.1371398 -5.7060 1.252e-08 *** ## log(rgdpch) -0.1098530 0.0820230 -1.3393 0.1805605 ## iwar -0.0075908 0.1332022 -0.0570 0.9545587 ## cwar -0.3415915 0.0936629 -3.6470 0.0002691 *** ## voice:veto 0.0027028 0.0113383 0.2384 0.8116001 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Total Sum of Squares: 4689.6 ## Residual Sum of Squares: 3484.4 ## R-Squared: 0.25698 ## Adj. R-Squared: 0.22219 ## F-statistic: 153.262 on 8 and 3545 DF, p-value: &lt; 2.22e-16 We could then test for serial correlation again: pbgtest(mod2.fe, order=1) ## ## Breusch-Godfrey/Wooldridge test for serial correlation in panel models ## ## data: repress ~ lag(repress, 1) + voice * veto + log(pop) + log(rgdpch) + ... ## chisq = 65.318, df = 1, p-value = 6.375e-16 ## alternative hypothesis: serial correlation in idiosyncratic errors pwartest(mod2.fe) ## ## Wooldridge&#39;s test for serial correlation in FE panels ## ## data: mod2.fe ## F = 2.527, df1 = 1, df2 = 3551, p-value = 0.112 ## alternative hypothesis: serial correlation We could also test for cross-sectional dependence. This is the situation where there is contemporaneous correlation across groups. Below is Breusch-Pagan’s original LM test with test=\"lm\" and Peseran’s CD statistic with test=\"cd\". These tests also work with random effects models. pcdtest(mod2.fe, test=&quot;lm&quot;) ## ## Breusch-Pagan LM test for cross-sectional dependence in panels ## ## data: repress ~ lag(repress, 1) + voice * veto + log(pop) + log(rgdpch) + iwar + cwar ## chisq = 16535, df = 12492, p-value &lt; 2.2e-16 ## alternative hypothesis: cross-sectional dependence pcdtest(mod2.fe, test=&quot;cd&quot;) ## ## Pesaran CD test for cross-sectional dependence in panels ## ## data: repress ~ lag(repress, 1) + voice * veto + log(pop) + log(rgdpch) + iwar + cwar ## z = 8.1655, p-value = 3.202e-16 ## alternative hypothesis: cross-sectional dependence There are robust standard errors we can use to correct for serial correlation and cross-sectional dependence. These are called Driscoll-Kray standard errors and have only been shown to work with fixed effects estimators. library(lmtest) coeftest(mod2.fe, vcov.=vcovSCC) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## lag(repress, 1) 0.4563965 0.0395825 11.5303 &lt; 2.2e-16 *** ## voice 0.0325649 0.0107890 3.0183 0.002560 ** ## veto 0.1305343 0.0487847 2.6757 0.007491 ** ## log(pop) -0.7825166 0.1203529 -6.5019 9.043e-11 *** ## log(rgdpch) -0.1098530 0.0781457 -1.4057 0.159888 ## iwar -0.0075908 0.0956638 -0.0793 0.936760 ## cwar -0.3415915 0.0543238 -6.2881 3.606e-10 *** ## voice:veto 0.0027028 0.0176934 0.1528 0.878598 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The Arellano standard errors can fix problems of serial correlation and heteroskedasticity. coeftest(mod2.fe, vcov.=vcovHC, type=&quot;HC3&quot;, method=&quot;arellano&quot;) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## lag(repress, 1) 0.4563965 0.0261685 17.4407 &lt; 2.2e-16 *** ## voice 0.0325649 0.0178315 1.8263 0.0678961 . ## veto 0.1305343 0.0665615 1.9611 0.0499445 * ## log(pop) -0.7825166 0.2012610 -3.8881 0.0001029 *** ## log(rgdpch) -0.1098530 0.1004767 -1.0933 0.2743288 ## iwar -0.0075908 0.1320141 -0.0575 0.9541501 ## cwar -0.3415915 0.1013826 -3.3693 0.0007616 *** ## voice:veto 0.0027028 0.0151234 0.1787 0.8581695 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 You Try It! How does the serial correlation robust standard error compare to the lag-structure modeling you did above? We could also get the Beck and Katz panel corrected standard errors (PCSE) with: coeftest(mod2.fe, vcov.=vcovBK(mod2.fe, type=&quot;HC3&quot;)) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## lag(repress, 1) 0.4563965 0.0177010 25.7836 &lt; 2.2e-16 *** ## voice 0.0325649 0.0168745 1.9298 0.0537078 . ## veto 0.1305343 0.0713440 1.8296 0.0673869 . ## log(pop) -0.7825166 0.1912979 -4.0906 4.399e-05 *** ## log(rgdpch) -0.1098530 0.1114558 -0.9856 0.3243872 ## iwar -0.0075908 0.1508133 -0.0503 0.9598603 ## cwar -0.3415915 0.0937059 -3.6454 0.0002709 *** ## voice:veto 0.0027028 0.0141647 0.1908 0.8486821 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 You Try It! How do the results change if you use the BK PCSEs instead? "],["measurement-issues.html", "Chapter 9 Measurement Issues 9.1 Reliability Analysis 9.2 Exploratory Factor Analysis and Principal Components Analysis 9.3 CFA and SEM", " Chapter 9 Measurement Issues I am really interested in measurement issues. In fact, I wrote a book about it with some really smart co-authors. There, we mostly focus on things not well covered in other parts of this vast literature - Nomiante models, optimal classification, a bit of multidimensional scaling and Bayesian implementations of these various tools. Here, I want to talk, instead about some of the more conventional measures for dealing with multiple indicators. 9.1 Reliability Analysis Often times with measurement, a simple solution is sufficient. In this case, we might be interested in assessing the reliability of a set of indicators relative to some unknown, unidimensional latent construct. To illustrate, we’ll use Bollen’s measures of political democracy. data(PoliticalDemocracy, package=&quot;lavaan&quot;) In this dataset, y1-y4 are indicators for 1960 and y5-y8 are indicators for 1965. The variables x1-x3 are different measures of development (GNP, energy consumption and labour force participation, respectively) for 1960. Let’s look at the 1965 democracy indicators. The first thing we could do is to simply look at the relationships in the data. The assumption here is that the relationship between the indicators and the latent dimension is monotonic. library(GGally) ggpairs(PoliticalDemocracy, 5:8, columnLabels=c(&quot;Press Free&quot;, &quot;Opp Free&quot;, &quot;Fair Elec&quot;, &quot;Leg Effect&quot;), lower=list(continuous=&quot;smooth_loess&quot;)) The correlations here are all reasonably strong and the relationsihps seem to be more or less monotonic in nature. You Try It! We’re going to use a different version of the WVS data that has all of the indicators for the secular and emancipatory values and the moral variable. Load the data as follows: load(&quot;data/wvsb.rda&quot;) Now look at the correlations and relationships between the indicators: - V198-V210 are the indicators for the moral variable. - I_INDEP-I_VOICE2 are the indicators for the emancipatory values scale. - I_AUTHORITY-I_TRUSTCOURTS are the indicators for secular values. The psych package has a function called alpha that does reliability analaysis. Let’s see what it looks like. The check.keys=TRUE will reverse items that are negatively related to the underlying dimension. library(psych) a &lt;- alpha(scale(PoliticalDemocracy[,c(&quot;y5&quot;, &quot;y6&quot;, &quot;y7&quot;, &quot;y8&quot;)]), check.keys = TRUE) a ## ## Reliability analysis ## Call: alpha(x = scale(PoliticalDemocracy[, c(&quot;y5&quot;, &quot;y6&quot;, &quot;y7&quot;, &quot;y8&quot;)]), ## check.keys = TRUE) ## ## raw_alpha std.alpha G6(smc) average_r S/N ase mean sd median_r ## 0.88 0.88 0.86 0.66 7.7 0.022 -4.6e-16 0.86 0.65 ## ## 95% confidence boundaries ## lower alpha upper ## Feldt 0.84 0.88 0.92 ## Duhachek 0.84 0.88 0.93 ## ## Reliability if an item is dropped: ## raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r ## y5 0.87 0.87 0.83 0.69 6.7 0.026 0.0055 0.71 ## y6 0.86 0.86 0.81 0.67 6.2 0.028 0.0017 0.68 ## y7 0.85 0.85 0.80 0.65 5.6 0.031 0.0092 0.63 ## y8 0.83 0.83 0.77 0.62 4.8 0.034 0.0033 0.61 ## ## Item statistics ## n raw.r std.r r.cor r.drop mean sd ## y5 75 0.83 0.83 0.74 0.70 2.6e-16 1 ## y6 75 0.85 0.85 0.78 0.73 -5.2e-16 1 ## y7 75 0.87 0.87 0.81 0.76 -1.4e-15 1 ## y8 75 0.90 0.90 0.87 0.81 -2.1e-16 1 The overal alpha of the scale is .88 and that can’t be improved upon by deleting any of the indicators from the analysis. This suggests a pretty good fit, overall. If we wanted to use the variable in subsequent analyses, we could simply get the scores element out of the object a and that is our estimate of the latent construct. We could also construct the democracy variable for 1960: a2 &lt;- alpha(scale(PoliticalDemocracy[,c(&quot;y1&quot;, &quot;y2&quot;, &quot;y3&quot;, &quot;y4&quot;)]), check.keys = TRUE) a2 ## ## Reliability analysis ## Call: alpha(x = scale(PoliticalDemocracy[, c(&quot;y1&quot;, &quot;y2&quot;, &quot;y3&quot;, &quot;y4&quot;)]), ## check.keys = TRUE) ## ## raw_alpha std.alpha G6(smc) average_r S/N ase mean sd median_r ## 0.87 0.87 0.85 0.63 6.7 0.025 -5.3e-16 0.85 0.64 ## ## 95% confidence boundaries ## lower alpha upper ## Feldt 0.81 0.87 0.91 ## Duhachek 0.82 0.87 0.92 ## ## Reliability if an item is dropped: ## raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r ## y1 0.81 0.81 0.77 0.59 4.4 0.038 0.0182 0.61 ## y2 0.85 0.85 0.80 0.66 5.8 0.029 0.0020 0.68 ## y3 0.86 0.86 0.81 0.67 6.1 0.028 0.0037 0.69 ## y4 0.80 0.80 0.75 0.58 4.1 0.040 0.0135 0.60 ## ## Item statistics ## n raw.r std.r r.cor r.drop mean sd ## y1 75 0.88 0.88 0.82 0.77 1.8e-16 1 ## y2 75 0.82 0.82 0.74 0.67 -4.5e-16 1 ## y3 75 0.81 0.81 0.71 0.66 -1.2e-15 1 ## y4 75 0.89 0.89 0.85 0.79 -3.8e-16 1 In Bollen’s model, he also estimates the latent variable relating to the x variables. We could see what that looks like as well. a3 &lt;- alpha(scale(PoliticalDemocracy[,c(&quot;x1&quot;, &quot;x2&quot;, &quot;x3&quot;)]), check.keys=TRUE) a3 ## ## Reliability analysis ## Call: alpha(x = scale(PoliticalDemocracy[, c(&quot;x1&quot;, &quot;x2&quot;, &quot;x3&quot;)]), check.keys = TRUE) ## ## raw_alpha std.alpha G6(smc) average_r S/N ase mean sd median_r ## 0.94 0.94 0.92 0.85 17 0.011 1.1e-15 0.95 0.85 ## ## 95% confidence boundaries ## lower alpha upper ## Feldt 0.92 0.94 0.96 ## Duhachek 0.92 0.94 0.97 ## ## Reliability if an item is dropped: ## raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r ## x1 0.92 0.92 0.85 0.85 11 0.019 NA 0.85 ## x2 0.89 0.89 0.80 0.80 8 0.026 NA 0.80 ## x3 0.94 0.94 0.89 0.89 17 0.013 NA 0.89 ## ## Item statistics ## n raw.r std.r r.cor r.drop mean sd ## x1 75 0.95 0.95 0.91 0.88 5.0e-15 1 ## x2 75 0.97 0.97 0.95 0.92 -1.0e-15 1 ## x3 75 0.93 0.93 0.87 0.85 -4.8e-16 1 PoliticalDemocracy$d65 &lt;- a$scores PoliticalDemocracy$d60 &lt;- a2$scores PoliticalDemocracy$i60 &lt;- a3$scores mod &lt;- lm(d65 ~ d60 + i60, data=PoliticalDemocracy) summary(mod) ## ## Call: ## lm(formula = d65 ~ d60 + i60, data = PoliticalDemocracy) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.08110 -0.26988 0.02386 0.27966 1.12847 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -3.183e-16 4.782e-02 0.000 1.0000 ## d60 7.953e-01 6.173e-02 12.883 &lt;2e-16 *** ## i60 1.823e-01 5.523e-02 3.301 0.0015 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4141 on 72 degrees of freedom ## Multiple R-squared: 0.7755, Adjusted R-squared: 0.7693 ## F-statistic: 124.4 on 2 and 72 DF, p-value: &lt; 2.2e-16 You Try It! Evaluate the reliability of the three different scales. - Then, estimate the model of emancipatory values scale on secular values, moral atitudes as well as V248 (education), V242 (age), V240 (gender), and V238 (income). 9.2 Exploratory Factor Analysis and Principal Components Analysis The EFA and PCA models are both essentially dimension reduction techniques, but with different underlying assumptions. The EFA model is a causal model (or sometimes called a spurious correlation model) where we assume that the inter-relationships among indicators exist because of their relationship to a common latent variable (or set of latent variables). That is, once we account for the latent variable, the observed indicators are independent. The principal components model is just a variance maximizing linear transformation of the observed indicators. Basically, the PCA model is trying to identify independent sources of systematic variation in the observed indicators. One of the benefits of EFA (or PCA) relative to the summated rating model (the one we estimated with alpha()) is that Cronbach’s \\(\\alpha\\) is not a good test of dimensionality. It assumes unidimensional structure and can actually mask multidimensional structure in the data. If there is any question about the underlying dimensionality of the data, then one of these other tools is more appropriate. One of the potential problems here is that despite the important theoretical differences between the two models, there are distinct similarities in the technical tools used to estimate the models - ultimately they both depend on the singular value decomposition (or eigen decomposition when the input data is a correlation matrix). First, let’s consider PCA. One tool we can use as a sort of diagnostic is a biplot. This allows us to visualize the underling structure in the data, at least in one or two (or sometimes three) dimensions. We can use the ggbiplot() function in the package of the same name to make a biplot. # remotes::install_github(&quot;vqv/ggbiplot&quot;) library(ggbiplot) pcafit &lt;- princomp(PoliticalDemocracy[,c(&quot;y5&quot;, &quot;y6&quot;, &quot;y7&quot;, &quot;y8&quot;)], cor=TRUE) ggbiplot(pcafit) + theme_bw() + theme(aspect.ratio=1) The way that we can interpret the biplot is in terms of the angles between the vectors and the \\(x\\)- and \\(y\\)-axes. Geometrically, the correlation is the cosine of the angle between two vectors. FYI: angles in R are measured in radians, not degrees. To convert your degrees to radians, use the equation \\[\\text{raidans} = \\text{degrees}\\times \\frac{\\pi}{180}\\] Right, back to the plot. All of the vectors for the four variables have relative close angles with each other and with the main (horizontal) axis. They are less well closely related to the second axis. This, to me, would argue that there is really one dimension here. Looking at the results of the principal components fit above, we see a vector of standard deviations. These tell us how much of the underlying variation is captured by each component. When cor=TRUE, each variable has a indicator contributes a variance of 1 to the variance to be explained. If we square the standard deviations (to make them variances) and divide each one by the total, that gives us the proportion of variance explained by each component. Here, the first component captures about \\(74\\%\\) of the total variance. The next component captures only about \\(12\\%\\) of the variance to be explained. This is less than a single variable’s worth of variance (which would be \\(25\\%\\) because there are four indicators). Again, this corroborates the unidimensionality. summary(pcafit) ## Importance of components: ## Comp.1 Comp.2 Comp.3 Comp.4 ## Standard deviation 1.7250783 0.6946489 0.56639263 0.46985874 ## Proportion of Variance 0.7439738 0.1206343 0.08020015 0.05519181 ## Cumulative Proportion 0.7439738 0.8646080 0.94480819 1.00000000 If we look at the loadings element of the pcafit obeject, we will see the transformation applied to each variable to arrive at each component. pcafit$loadings ## ## Loadings: ## Comp.1 Comp.2 Comp.3 Comp.4 ## y5 0.480 0.645 0.593 ## y6 0.492 -0.624 0.326 -0.512 ## y7 0.505 0.315 -0.720 -0.357 ## y8 0.522 -0.309 -0.156 0.779 ## ## Comp.1 Comp.2 Comp.3 Comp.4 ## SS loadings 1.00 1.00 1.00 1.00 ## Proportion Var 0.25 0.25 0.25 0.25 ## Cumulative Var 0.25 0.50 0.75 1.00 Above, we see that the first component has all variables “loading” on the component with approximately the same coefficient. It is worth noting that the summated rating model assumes, at least in the construction of the index, that all indicators are equally reliable. Here, there is not much evidence that runs counter to that assumption. If we wanted to save the variable, we could do that with the scores element of the output. It will always have as many columns as there are original variables. In this case, we just want the first one. We could do the same for the other two models as well. We see pretty similar things with the other two models pcafit2 &lt;- princomp(PoliticalDemocracy[,c(&quot;y1&quot;, &quot;y2&quot;, &quot;y3&quot;, &quot;y4&quot;)], cor=TRUE) pcafit2$loadings ## ## Loadings: ## Comp.1 Comp.2 Comp.3 Comp.4 ## y1 0.518 0.221 0.826 ## y2 0.481 -0.656 -0.121 -0.569 ## y3 0.472 0.684 -0.476 -0.289 ## y4 0.527 -0.231 -0.276 0.770 ## ## Comp.1 Comp.2 Comp.3 Comp.4 ## SS loadings 1.00 1.00 1.00 1.00 ## Proportion Var 0.25 0.25 0.25 0.25 ## Cumulative Var 0.25 0.50 0.75 1.00 pcafit3 &lt;- princomp(PoliticalDemocracy[,c(&quot;x1&quot;, &quot;x2&quot;, &quot;x3&quot;)], cor=TRUE) pcafit3$loadings ## ## Loadings: ## Comp.1 Comp.2 Comp.3 ## x1 0.577 0.585 0.569 ## x2 0.588 0.186 -0.787 ## x3 0.567 -0.789 0.237 ## ## Comp.1 Comp.2 Comp.3 ## SS loadings 1.000 1.000 1.000 ## Proportion Var 0.333 0.333 0.333 ## Cumulative Var 0.333 0.667 1.000 So, we could save those scores: PoliticalDemocracy$pcd65 &lt;- pcafit$scores[,1] PoliticalDemocracy$pcd60 &lt;- pcafit2$scores[,1] PoliticalDemocracy$pci60 &lt;- pcafit3$scores[,1] And we could look at the correlations between the SRM variables and the PCA variables: cor(PoliticalDemocracy[,c(&quot;i60&quot;, &quot;d60&quot;, &quot;d65&quot;)], PoliticalDemocracy[,c(&quot;pci60&quot;, &quot;pcd60&quot;, &quot;pcd65&quot;)]) ## pci60 pcd60 pcd65 ## i60 0.9999937 0.3958996 0.5069449 ## d60 0.3935625 0.9998969 0.8606081 ## d65 0.5084075 0.8628187 0.9999486 The diagonals of the matrix above are the correlations between the SRM variable and its corresponding PCA variable. As you can see, these correlations are more than .999, meaning that they are nearly perfectly related. Next, we could look at the exploratory factor analysis (EFA) also sometimes referred to as the common factor model. The PCA model doesn’t inherently reduce dimensionality - if there are \\(k\\) observed variables, it will always return four components, but it might be that the variables could be reproduced with relatively little error using only \\(m&lt;k\\) components. In exploratory factor analysis, the model does (usually) reduce dimensionality, because it is hypothesized that there is a small number of dimensions that account for the inter-relationships among the observed indicators. It also assumes (or can assume) that some of each indicator’s variance is unique to itself (that is, cannot be explained by the common factors). There are several ways to do factor analysis in R, but I like the one in the psych package, called fa(). Here’s how we could estimate the model. f1 &lt;- fa(PoliticalDemocracy[,c(&quot;y5&quot;, &quot;y6&quot;, &quot;y7&quot;, &quot;y8&quot;)], nfactors = 1, SMC = TRUE, fm = &quot;pa&quot;) f1 ## Factor Analysis using method = pa ## Call: fa(r = PoliticalDemocracy[, c(&quot;y5&quot;, &quot;y6&quot;, &quot;y7&quot;, &quot;y8&quot;)], nfactors = 1, ## SMC = TRUE, fm = &quot;pa&quot;) ## Standardized loadings (pattern matrix) based upon correlation matrix ## PA1 h2 u2 com ## y5 0.75 0.56 0.44 1 ## y6 0.79 0.62 0.38 1 ## y7 0.82 0.68 0.32 1 ## y8 0.89 0.79 0.21 1 ## ## PA1 ## SS loadings 2.64 ## Proportion Var 0.66 ## ## Mean item complexity = 1 ## Test of the hypothesis that 1 factor is sufficient. ## ## df null model = 6 with the objective function = 2.29 with Chi Square = 164.19 ## df of the model are 2 and the objective function was 0.09 ## ## The root mean square of the residuals (RMSR) is 0.04 ## The df corrected root mean square of the residuals is 0.07 ## ## The harmonic n.obs is 75 with the empirical chi square 1.57 with prob &lt; 0.46 ## The total n.obs was 75 with Likelihood Chi Square = 6.33 with prob &lt; 0.042 ## ## Tucker Lewis Index of factoring reliability = 0.917 ## RMSEA index = 0.169 and the 90 % confidence intervals are 0.028 0.329 ## BIC = -2.31 ## Fit based upon off diagonal values = 1 ## Measures of factor score adequacy ## PA1 ## Correlation of (regression) scores with factors 0.95 ## Multiple R square of scores with factors 0.90 ## Minimum correlation of possible factor scores 0.79 There is a lot of output above. The main thing to look at is the loadings and the proportion of variance explained. The columns labeled PA1 (PA stands for principal axis) are the loadings. These show that there is some difference in the reliability of the indicators with y5 being a fair bit lower than y8. The column labeled h2 is the “communality” - this is the proportion of the variable’s variance that is explained by the factor. The column labeled u2 is the variable’s uniqueness. This is the proportion of the variable’s variance that cannot be explained by the latent factor. The higher this number, the more reliable the indicator. Above, I chose a single factor. One of the ways that we tend to choose the number of factors is with a scree plot. This plots te factor number on the \\(x\\)-axis and the eigen-value (amount of variance explained by the factor) on the \\(y\\)-axis. You’re looking for an “elbow” in the plot and you choose the solution indicated directly to the left of the elbow. Here’s the scree plot for our data: scree(PoliticalDemocracy[,c(&quot;y5&quot;, &quot;y6&quot;, &quot;y7&quot;, &quot;y8&quot;)]) Notice, that the elbow here happens at 2 factors, so we choose the one directly to the left - a one-factor solution. We could look at the scree plots for the other two sets of variables, too: scree(PoliticalDemocracy[,c(&quot;y1&quot;, &quot;y2&quot;, &quot;y3&quot;, &quot;y4&quot;)]) scree(PoliticalDemocracy[,c(&quot;x1&quot;, &quot;x2&quot;, &quot;x3&quot;)]) We see the same thing in both of those plots. So we could estimate the other two one-factor solutions. f2 &lt;- fa(PoliticalDemocracy[,c(&quot;y1&quot;, &quot;y2&quot;, &quot;y3&quot;, &quot;y4&quot;)], nfactors = 1, SMC = TRUE, fm = &quot;pa&quot;) f3 &lt;- fa(PoliticalDemocracy[,c(&quot;x1&quot;, &quot;x2&quot;, &quot;x3&quot;)], nfactors = 1, SMC = TRUE, fm = &quot;pa&quot;) Now, we could put all of those in the data as well. PoliticalDemocracy$fad60 &lt;- f2$scores PoliticalDemocracy$fad65 &lt;- f1$scores PoliticalDemocracy$fai60 &lt;- f3$scores Now, let’s look at the correlation of all of the 1965 latent measures. cor(PoliticalDemocracy[,c(&quot;d65&quot;, &quot;pcd65&quot;, &quot;fad65&quot;)]) ## d65 pcd65 fad65 ## d65 1.0000000 0.9999486 0.9936469 ## pcd65 0.9999486 1.0000000 0.9946834 ## fad65 0.9936469 0.9946834 1.0000000 These are all really highly inter-correlated. library(stargazer) m1 &lt;- lm(d65 ~ i60 + d60, data=PoliticalDemocracy) m2 &lt;- PoliticalDemocracy %&gt;% select(&quot;pcd65&quot;, &quot;pcd60&quot;, &quot;pci60&quot;) %&gt;% dplyr::rename(&quot;d65&quot; = &quot;pcd65&quot;, &quot;d60&quot; = &quot;pcd60&quot;, &quot;i60&quot; = &quot;pci60&quot;) %&gt;% lm(d65 ~ i60 + d60, data=.) m3 &lt;- PoliticalDemocracy %&gt;% select(&quot;fad65&quot;, &quot;fad60&quot;, &quot;fai60&quot;) %&gt;% dplyr::rename(&quot;d65&quot; = &quot;fad65&quot;, &quot;d60&quot; = &quot;fad60&quot;, &quot;i60&quot; = &quot;fai60&quot;) %&gt;% lm(d65 ~ i60 + d60, data=.) stargazer(m1, m2, m3, type=&quot;text&quot;) ## ## ============================================================== ## Dependent variable: ## -------------------------------- ## d65 ## (1) (2) (3) ## -------------------------------------------------------------- ## i60 0.182*** 0.206*** 0.165*** ## (0.055) (0.064) (0.061) ## ## d60 0.795*** 0.797*** 0.788*** ## (0.062) (0.062) (0.064) ## ## Constant -0.000 0.000 -0.000 ## (0.048) (0.096) (0.054) ## ## -------------------------------------------------------------- ## Observations 75 75 75 ## R2 0.776 0.776 0.762 ## Adjusted R2 0.769 0.770 0.755 ## Residual Std. Error (df = 72) 0.414 0.833 0.468 ## F Statistic (df = 2; 72) 124.385*** 124.761*** 115.312*** ## ============================================================== ## Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 All three of these methods produce very similar results. One of the potential problems with the above is that there the models are estimated assuming that our estimated latent variables are observed. We might rather have all of these things - the latent structure and the model parameters - estimated simultaneously to prevent that from happening. This is what a full structural equation model does. You Try It! Estimate the EFA for the three scales each independently. 9.2.1 Rotations EFA solutions are only identified up to a rotation - that is any rotation of the factors in the factor space fits the data equally well. Each rotation gives a slightly different view of the same configuration of points - it’s kind of like walking around a statue. Each time you move you’re still looking at the statue, but some things reveal themselves more plainly from some views than others. One isn’t “right” and the others “wrong” - some views might be more useful or interesting than others, though. We need to estimate at least two factors for rotations to make sense. So, let’s estimate the democracy and development factors for 1960 simultaneously. fa60 &lt;- fa(PoliticalDemocracy[,c(&quot;y1&quot;, &quot;y2&quot;, &quot;y3&quot;, &quot;y4&quot;, &quot;x1&quot;, &quot;x2&quot;, &quot;x3&quot;)], nfactors = 2, SMC=TRUE, fm=&quot;pa&quot;) fa60$loadings ## ## Loadings: ## PA1 PA2 ## y1 0.858 ## y2 0.769 ## y3 0.706 ## y4 0.104 0.840 ## x1 0.900 ## x2 0.969 ## x3 0.893 ## ## PA1 PA2 ## SS loadings 2.566 2.537 ## Proportion Var 0.367 0.362 ## Cumulative Var 0.367 0.729 We can visualize the factor structure like below. This is the un-rotated solution. Here, you see that the first dimension tries to capture the main parts of both factors even though both factors are clearly distinct. load60 &lt;- fa60$loadings class(load60) &lt;- &quot;matrix&quot; loads &lt;- as_tibble(load60, rownames=&quot;var&quot;) ggplot(loads) + geom_segment(aes(x=0, y=0, xend=PA1, yend=PA2), arrow = arrow(length = unit(0.1, &quot;inches&quot;))) + geom_text(aes(x=PA1, y=PA2, label=var), nudge_x=.05) + xlim(-1,1) + ylim(-1,1) + geom_vline(xintercept=0, col=&quot;gray50&quot;, lty=2) + geom_hline(yintercept=0, col=&quot;gray50&quot;, lty=2) + theme_bw() + theme(aspect.ratio=1) We can use a “varimax” rotation to solve this problem. The varimax rotation is an orthogonal rotation meaning that it maintains the uncorrelated nature of the factors. It tries to maximize variance in the columns of the factor loadings. This happens when some entries are really big and some are really small (rather than all being somewhere in the middle). You can implement this as an argument to the fa() function. fa60v &lt;- fa(PoliticalDemocracy[,c(&quot;y1&quot;, &quot;y2&quot;, &quot;y3&quot;, &quot;y4&quot;, &quot;x1&quot;, &quot;x2&quot;, &quot;x3&quot;)], nfactors = 2, SMC=TRUE, fm=&quot;pa&quot;, rotate=&quot;varimax&quot;) fa60v$loadings ## ## Loadings: ## PA1 PA2 ## y1 0.160 0.831 ## y2 0.731 ## y3 0.163 0.691 ## y4 0.286 0.843 ## x1 0.889 0.251 ## x2 0.945 0.216 ## x3 0.860 0.154 ## ## PA1 PA2 ## SS loadings 2.566 2.545 ## Proportion Var 0.367 0.364 ## Cumulative Var 0.367 0.730 Above, you can see that the first factor seems to capture the development variables now and the second factor the democracy variables. We can see what that structure looks like, too. load60v &lt;- fa60v$loadings class(load60v) &lt;- &quot;matrix&quot; loads &lt;- as_tibble(load60v, rownames=&quot;var&quot;) ggplot(loads) + geom_segment(aes(x=0, y=0, xend=PA1, yend=PA2), arrow = arrow(length = unit(0.1, &quot;inches&quot;))) + geom_text(aes(x=PA1, y=PA2, label=var), nudge_x=.05) + xlim(-1,1) + ylim(-1,1) + geom_vline(xintercept=0, col=&quot;gray50&quot;, lty=2) + geom_hline(yintercept=0, col=&quot;gray50&quot;, lty=2) + theme_bw() + theme(aspect.ratio=1) fa60p &lt;- fa(PoliticalDemocracy[,c(&quot;y1&quot;, &quot;y2&quot;, &quot;y3&quot;, &quot;y4&quot;, &quot;x1&quot;, &quot;x2&quot;, &quot;x3&quot;)], nfactors = 2, SMC=TRUE, fm=&quot;pa&quot;, rotate=&quot;promax&quot;) fa60p$loadings ## ## Loadings: ## PA1 PA2 ## y1 0.854 ## y2 0.765 ## y3 0.702 ## y4 0.115 0.836 ## x1 0.901 ## x2 0.970 ## x3 0.893 ## ## PA1 PA2 ## SS loadings 2.569 2.511 ## Proportion Var 0.367 0.359 ## Cumulative Var 0.367 0.726 These are a bit more difficult to visualize because they are in a non-orthogonal coordinate space. We can do it with the following though. Note that the fac.rotate2() function is in the source code for the chapter, but is not printed as output you can see in the book. r &lt;- fac.rotate2(rotproc=&quot;promax&quot;, data=PoliticalDemocracy[,c(&quot;y1&quot;, &quot;y2&quot;, &quot;y3&quot;, &quot;y4&quot;, &quot;x1&quot;, &quot;x2&quot;, &quot;x3&quot;)]) newax &lt;- as_tibble(r$new.hlvl) loads &lt;- as_tibble(r$loads, rownames=&quot;var&quot;) ggplot() + geom_segment(data=newax, aes(x=0, y=0, xend = V1, yend=V2), col=&quot;gray50&quot;, lty=2) + geom_segment(data=loads, aes(x=0, y=0, xend=PA1, yend=PA2), arrow = arrow(length=unit(.1, &quot;inches&quot;))) + geom_text(data=loads, aes(x=PA1, y=PA2, label=var), nudge_x=.05) + xlim(-1,1) + ylim(-1,1) + theme_bw() + theme(aspect.ratio=1) You Try It! Estimate the EFA of all of the indicators for the three latent variables and evaluate the dimensionality of the solution. 9.3 CFA and SEM The idea behind exploratory factor analysis is that all factors are related to all variables. This may not be a reasonable assumption and, in fact you may want to impose a certain structure on the data. The way to do this is with confirmatory factor analysis and structural equation models. CFA is a method of estimate factor models where force certain paths to be zero to impose simple structure on the model. SEM includes CFA, but allows regression parameters between latent variables and observed variables to be estimated. This is a huge area of research, but I thought I’d just give you a sense of how to fit these models in R. The lavaan package is in active development and hopes to become a sort of mirror of MPlus in R. It is the most robust and feature-rich way of estimating SEMs in R. The main parts of a lavaan model are the =~ which identifies the latent variables (LHS) and their indicators (RHS), the ~~ which frees variances or covariances to be estimates and ~, which defines the predictive part of the models that include the latent variables. Here’s an example from Bollen’s book. The default method for identifying these models is to set the coefficient (factor loading) for the first indicator to 1. This is a common method. Another common method is to set the latent variable variances to 1. Both do the job in that they provide identification for the model and the substantive inferences following either will be identical. library(lavaan) model &lt;- &#39; # measurement model ind60 =~ x1 + x2 + x3 dem60 =~ y1 + y2 + y3 + y4 dem65 =~ y5 + y6 + y7 + y8 # regressions dem60 ~ a*ind60 dem65 ~ b*ind60 + c*dem60 # indirect effect of ind60 ac := a*c total := a*c + b # residual correlations # freedom of the press y1 ~~ y5 # freedom of political opposition y2 ~~ y6 # fairness of election y3 ~~ y7 # effectiveness of legislature y4 ~~ y8 &#39; fit &lt;- sem(model, data=PoliticalDemocracy, mimic=&quot;MPlus&quot;) summary(fit) ## lavaan 0.6-19 ended normally after 58 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 40 ## ## Number of observations 75 ## Number of missing patterns 1 ## ## Model Test User Model: ## ## Test statistic 50.835 ## Degrees of freedom 37 ## P-value (Chi-square) 0.064 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Observed ## Observed information based on Hessian ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) ## ind60 =~ ## x1 1.000 ## x2 2.181 0.139 15.671 0.000 ## x3 1.819 0.152 11.949 0.000 ## dem60 =~ ## y1 1.000 ## y2 1.388 0.195 7.134 0.000 ## y3 1.053 0.158 6.645 0.000 ## y4 1.368 0.164 8.337 0.000 ## dem65 =~ ## y5 1.000 ## y6 1.317 0.188 7.007 0.000 ## y7 1.326 0.174 7.603 0.000 ## y8 1.391 0.182 7.634 0.000 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## dem60 ~ ## ind60 (a) 1.435 0.383 3.741 0.000 ## dem65 ~ ## ind60 (b) 0.507 0.217 2.333 0.020 ## dem60 (c) 0.816 0.100 8.188 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## .y1 ~~ ## .y5 0.892 0.395 2.261 0.024 ## .y2 ~~ ## .y6 1.893 0.764 2.477 0.013 ## .y3 ~~ ## .y7 1.268 0.652 1.944 0.052 ## .y4 ~~ ## .y8 0.141 0.500 0.281 0.778 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .x1 5.054 0.084 60.127 0.000 ## .x2 4.792 0.173 27.657 0.000 ## .x3 3.558 0.161 22.066 0.000 ## .y1 5.465 0.301 18.172 0.000 ## .y2 4.256 0.452 9.409 0.000 ## .y3 6.563 0.376 17.464 0.000 ## .y4 4.453 0.384 11.587 0.000 ## .y5 5.136 0.299 17.160 0.000 ## .y6 2.978 0.388 7.676 0.000 ## .y7 6.196 0.378 16.406 0.000 ## .y8 4.043 0.372 10.865 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .x1 0.082 0.020 4.135 0.000 ## .x2 0.120 0.070 1.704 0.088 ## .x3 0.467 0.089 5.227 0.000 ## .y1 2.181 0.496 4.396 0.000 ## .y2 6.490 1.240 5.235 0.000 ## .y3 5.490 1.020 5.383 0.000 ## .y4 2.470 0.712 3.469 0.001 ## .y5 2.662 0.531 5.009 0.000 ## .y6 4.249 0.835 5.091 0.000 ## .y7 3.560 0.724 4.918 0.000 ## .y8 2.531 0.650 3.891 0.000 ## ind60 0.448 0.087 5.169 0.000 ## .dem60 3.678 0.915 4.021 0.000 ## .dem65 0.350 0.189 1.848 0.065 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) ## ac 1.170 0.335 3.493 0.000 ## total 1.677 0.372 4.505 0.000 You can get a bunch of fit measures with the fitMeasures function. There are lots of things to look at here. The CFI and TLI are both over .95, which is what Hu and Bentler suggested. The Hu and Bentler criteria apply here since we’re using normal-theory ML estimation with continuous indicators just as in their simulation. The \\(\\chi^2\\) statistic is insigificant suggesting no strong evidence for lack of fit. This suggests that the model does a reasonably good job of recovering the correlation structure of the variables. fitMeasures(fit) ## npar fmin chisq ## 40.000 0.339 50.835 ## df pvalue baseline.chisq ## 37.000 0.064 730.654 ## baseline.df baseline.pvalue cfi ## 55.000 0.000 0.980 ## tli cfi.robust tli.robust ## 0.970 0.980 0.970 ## nnfi rfi nfi ## 0.970 0.897 0.930 ## pnfi ifi rni ## 0.626 0.980 0.980 ## nnfi.robust rni.robust logl ## 0.970 0.980 -1554.146 ## unrestricted.logl aic bic ## -1528.728 3188.292 3280.991 ## ntotal bic2 rmsea ## 75.000 3154.922 0.071 ## rmsea.ci.lower rmsea.ci.upper rmsea.ci.level ## 0.000 0.115 0.900 ## rmsea.pvalue rmsea.close.h0 rmsea.notclose.pvalue ## 0.234 0.050 0.396 ## rmsea.notclose.h0 rmsea.robust rmsea.ci.lower.robust ## 0.080 0.071 0.000 ## rmsea.ci.upper.robust rmsea.pvalue.robust rmsea.notclose.pvalue.robust ## 0.115 0.234 0.396 ## rmr rmr_nomean srmr ## 0.297 0.321 0.046 ## srmr_bentler srmr_bentler_nomean crmr ## 0.046 0.050 0.050 ## crmr_nomean srmr_mplus srmr_mplus_nomean ## 0.055 0.046 0.050 ## cn_05 cn_01 gfi ## 78.002 89.363 0.994 ## agfi pgfi mfi ## 0.988 0.478 0.912 ## ecvi ## 1.744 There are also commands that will produce modification indices (modificationIndices()). You Try It! Estimate the SEM for the model represented above. - How does it fit? "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
